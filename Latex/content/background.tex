\chapter{Background}\label{ch:background}

This chapter provides the necessary background knowledge to understand how Ghidra works, in particular the decompilation process,
with his architecture, main components, and the decompilation pipeline. The Decompiler of Ghidra is enormous and complex software, (over 200k lines of C++)
so we focus only on the parts that are relevant to this thesis.
Then we will introduce the main concepts behind \ac{LLM}s, their architecture, sampling, and metrics for evaluation.

\section{Ghidra}\label{sec:ghidra}

Ghidra, released by the \ac{NSA} in 2019, employs a bifurcated design
that separates the user-facing interaction layer from the core analysis engine. This separation is
not merely an implementation detail but a fundamental architectural constraint that dictates how data 
flows during the reverse engineering process.

The framework operates across two distinct memory spaces: a frontend implemented in Java and a backend
analysis engine written in C++. The Java frontend is responsible for the \ac{GUI},
project database management, and plugin orchestration. It provides the high-level API exposed to users
and scripts (e.g., Python or Java scripts via the GhidraScript framework). However, the computationally
intensive tasks of data-flow analysis, variable inference, and control flow structuring are offloaded to
a native C++ executable, typically named \texttt{decomp} or \texttt{decomp\_dbg} (for debugging).
These executables and the code are located at \texttt{Ghidra/Features/Decompiler/src/decompile/cpp}.

Communication is mediated by the \texttt{ghidra.app.decompiler.DecompInterface}. 
This interface manages a dedicated input/output stream to the native process, utilizing an XML-based protocol to exchange data.
When a function decompilation is requested, the Java client does not simply invoke a library function; it serializes the request into
an XML command (e.g., \texttt{<decompile\_at>}) and transmits it to the backend. The C++ process, holding its own representation
of the function's data flow in \texttt{Funcdata} objects, performs the analysis and returns the results as a serialized XML stream
describing the high-level code structure and syntax tokens.

\section{SLEIGH and P-code}\label{sec:slap}

As written in the documentation created by running \texttt{<make doc>}~\cite{DOC:shareGhidraDecompiler}, the 
decompiler provides its own \ac{RTL}, referred internally as p-code; you can see some examples in~\Cref{tab:pcode_ops}. The disassembly of processor
specific machine-code languages, and subsequent translation into p-code, forms a major sub-system 
of the decompiler. There is a processor specification language, referred to as SLEIGH, which is 
dedicated to this translation task, this piece of the code can be built as a standalone binary 
translation library, for use by other applications.

\subsection{P-code Semantics and Varnodes}
Unlike intermediate languages in compilers, P-code is designed specifically for reverse engineering, prioritizing the explicit representation of memory and register modifications.

The fundamental unit of data in P-code is the \emph{Varnode}. A Varnode is defined by the triple \emph{(Space, Offset, Size)}, representing a contiguous sequence of bytes in a specific address space.

\begin{table}[ht]
    \centering
    \caption{Some P-code Operations and Semantics \texttt{opcodes.hh}~\cite{DOC:shareGhidraDecompiler, DOC:spinselPCodeReference}}\label{tab:pcode_ops}
    \begin{tabular}{l p{0.25\textwidth} p{0.45\textwidth}}
        \toprule
        \textbf{Opcode} & \textbf{Operands} & \textbf{Semantics} \\
        \midrule
        \texttt{CPUI\_COPY } & $in_0 \rightarrow out$ & Copy one operand to another. \\
        \texttt{CPUI\_LOAD } & $space, ptr \rightarrow out$ & Load from a pointer into a specific address. \\
        \texttt{CPUI\_STORE} & $space, ptr, val$ & Store at a pointer into a specified address space. \\
        \texttt{CPUI\_INT\_ADD} & $in_0, in_1 \rightarrow out$ & Integer addition, signed or unsigned. \\
        \texttt{CPUI\_CBRANCH} & $dest, cond$ & Conditional jump to $dest$ if $cond$ is non-zero. \\
        \bottomrule
    \end{tabular}
\end{table}

We must distinguish between two forms of P-code used during analysis:
\begin{enumerate}
    \item \textbf{Raw P-code:} The direct, unoptimized output of the SLEIGH translation. 
    It is represented by the class \emph{PcodeOpRaw} (or by unprocessed PcodeOp), 
    and contains the bare essentials: an opcode, a sequence number (address), and 
    the input/output Varnodes.
    \item \textbf{High P-code:} The result of the analysis pipeline. In this form, 
    the code has been converted to \ac{SSA} form (a form where every varnode is defined 
    exactly once for each function, if a variable is assigned multiple times, each assignment 
    is given a new instance called low-level variable), dead code has been eliminated, 
    and high-level concepts like function calls (replacing jump-and-link semantics) 
    have been recovered. It is represented by the class \emph{HighVariable}; 
    this is an abstraction that groups multiple low-level Varnodes (which may reside 
    in different registers or stack locations during execution) into a single logical 
    variable, similar to a variable in C code.
\end{enumerate}

The transformation from Raw to High P-code is where the majority of the decompilation logic resides.
It is an inference process that attempts to raise the abstraction level of the code,
often relying on heuristics that may fail in the presence of obfuscation
or aggressive compiler optimizations.

\section{The Decompilation Pipeline}\label{sec:pipeline}

The C++ decompiler engine processes a function at a time through a series of iterative passes. 
The architecture organizes these passes into \emph{Actions} and \emph{Rules}, 
managed by the \texttt{ActionDatabase}.
Inside the \texttt{ActionDatabase::universalAction} we have two main types of objects:
\begin{itemize}
    \item \texttt{ActionGroup}: Represents a list of Actions that are applied sequentially. The group's properties (eg., rule\_repeatapply) influence how the contained actions are executed.
    \item \texttt{ActionPool}: It is a pool of Rules that are applied simultaneously to every PcodeOp. Each Rule triggers on a specific localized data-flow configuration. The Rules are applied repeatedly until no Rule can make any additional transformations.
\end{itemize}

\subsection{Actions and Rules}\label{sec:ActeRul}
Actions represent large-scale transformations applied to the graph of varnodes and operations. They are the base class for objects that make modifications to a function's (Funcdata) syntax tree. Their purpose is to manage complex stages of the workflow, such as recovering the control-flow structure or generating \ac{SSA} form.

Rules, on the other hand, are a class designed to perform a single specific transformation on a PcodeOp or a Varnode. A Rule triggers when it recognizes a particular local configuration in the data flow and specifies a sequence of modification operations to transform it.

\subsection{DefaultGroups}\label{sec:dgroups}

Actions and Rules are selected and activated according to the type of \emph{DefaultGroup} they belong to.
These groups represent standardized workflows for different analysis phases and are built by the method \texttt{ActionDatabase::buildDefaultGroups}. The main groups are:

\begin{itemize}
    \item \textbf{decompile}: the standard workflow for full decompilation, composed of all of the phases.
    \item \textbf{jumptable}: optimized for analyzing jump tables.
    \item \textbf{normalize}: used for code normalization.
    \item \textbf{paramid}: for parameter identification.
    \item \textbf{register}: for register analysis.
    \item \textbf{firstpass}: a first fast analysis pass.
\end{itemize}

Each DefaultGroup is a list of names that refer to specific \texttt{ActionGroup}, \texttt{ActionPool} or individual \texttt{Action} to execute in that configuration. These lists define subsets of all the Actions.

The decompiler can be customized by selecting different DefaultGroups in Java with the method \texttt{setSimplificationStyle} of the decompiler interface but
only the group named \emph{decompile} return C code to Ghidra, since in \texttt{ghidra\_process.cc} we have:

\begin{lstlisting}[language=C++, caption={ghidra\_process.cc}]
    [...]
      fd->encode(encoder,0,ghidra->getSendSyntaxTree());
      if (ghidra->getSendCCode()&&
	  (ghidra->allacts.getCurrentName() == "decompile"))  //HERE WE HAVE THE CHECK
        ghidra->print->docFunction(fd);
    [...]
\end{lstlisting}

\section{Logic of Control Flow Structuring}\label{sec:cfg_structuring}

Recovering high-level control structures (loops, conditionals) from the unstructured \ac{CFG}
is arguably the most challenging phase of decompilation. It is effectively a pattern-matching
problem on a directed graph, aimed at finding subgraphs that correspond to structured programming constructs.

\subsection{Basic Block Formulation}
The decompiler first aggregates P-code operations into \emph{BasicBlocks}, that is, sequences of instructions with a single entry point 
and a single exit point (excluding internal calls). The \ac{CFG} is formed by the edges representing jumps and branches between these blocks. 
Ghidra normalizes this graph to ensure a unique entry block, often inserting empty placeholder blocks to handle re-entrant loops or complex function entries.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{img/cfg1.png}
    \caption{Control Flow Graph of a function~\cite{Paper:11129273}}\label{img:cfg1}
\end{figure}

In \Cref{img:cfg1} we can see an example of a \ac{CFG}, we used it to create a C function and then we have extracted its corresponding P-code representation in the table below\footnote{
    P-codes varies during all phases of the decompilation process; due to optimization rules, dead code elimination, and other transformations, the P-code shown here are taken from the \texttt{collapseInternal} method using \texttt{printRaw} of the FlowBlock class.
    The BasicBlock order may not correspond directly to the original source code order
}.

\begin{longtable}{|p{0.30\linewidth}|p{0.65\linewidth}|}
    \hline
    \textbf{Source Code (C)} & \textbf{P-Code / Basic Blocks} \\ 
    \hline
    \endhead
    % --- ROW 1 (Block 0) ---
    \begin{minipage}[t]{\linewidth}
        \vspace{1mm}
        \begin{lstlisting}[style=CStyle]
int a2_local;
int a1_local;
putchar(L'1');
if ((a1 == 1)
        \end{lstlisting}
        \vspace{1mm}
    \end{minipage} 
    & 
    \begin{minipage}[t]{\linewidth}
        \vspace{1mm}
        \textbf{\scriptsize Basic Block 0}
        \begin{lstlisting}[style=PCodeStyle]
0x0010118d:1:	RSP(0x0010118d:1) = RSP(i) + #0xfffffffffffffff8
0x0010118d:2:	*(ram,RSP(0x0010118d:1)) = RBP(i)
0x00101195:d:	u0x00004780(0x00101195:d) = RSP(i) + #0xfffffffffffffff4
0x00101195:f:	*(ram,u0x00004780(0x00101195:d)) = EDI(i)
0x00101198:10:	u0x00004780(0x00101198:10) = RSP(i) + #0xfffffffffffffff0
0x00101198:12:	*(ram,u0x00004780(0x00101198:10)) = ESI(i)
0x001011a0:14:	RSP(0x001011a0:14) = RSP(i) + #0xffffffffffffffe0
0x001011a0:15:	*(ram,RSP(0x001011a0:14)) = #0x1011a5
0x001011a0:67:	u0x10000008:1(0x001011a0:67) = *(ram,RSP(0x001011a0:14))
0x001011a0:16:	call jputchar(free)(#0x31:4,u0x10000008:1(0x001011a0:67))
0x001011a5:17:	u0x00004780(0x001011a5:17) = RSP(i) + #0xfffffffffffffff4
0x001011a5:18:	u0x00011e80:4(0x001011a5:18) = *(ram,u0x00004780(0x001011a5:17))
0x001011a5:1e:	ZF(0x001011a5:1e) = u0x00011e80:4(0x001011a5:18) == #0x1:4
0x001011a9:23:	goto Block_2:0x001011bd if (ZF(0x001011a5:1e) != 0) else Block_1:0x001011ab
        \end{lstlisting}
        \vspace{1mm}
    \end{minipage} 
    \\ \hline

    % --- ROW 2 (Block 1) ---
    \begin{minipage}[t]{\linewidth}
        \vspace{1mm}
        \begin{lstlisting}[style=CStyle]
|| (a2 != 2)){
        \end{lstlisting}
        \vspace{1mm}
    \end{minipage}
    & 
    \begin{minipage}[t]{\linewidth}
        \vspace{1mm}
        \textbf{\scriptsize Basic Block 1}
        \begin{lstlisting}[style=PCodeStyle]
0x001011ab:24:	u0x00004780(0x001011ab:24) = RSP(i) + #0xfffffffffffffff0
0x001011ab:25:	u0x00011e80:4(0x001011ab:25) = *(ram,u0x00004780(0x001011ab:24))
0x001011ab:2b:	ZF(0x001011ab:2b) = u0x00011e80:4(0x001011ab:25) != #0x2:4
0x001011af:31:	goto Block_2:0x001011bd if (ZF(0x001011ab:2b) != 0) else Block_4:0x001011b1
        \end{lstlisting}
        \vspace{1mm}
    \end{minipage}
    \\ \hline

    % --- ROW 3 (Block 2) ---
    \begin{minipage}[t]{\linewidth}
        \vspace{1mm}
        \begin{lstlisting}[style=CStyle]
putchar(L'2');
if (a1 != a2) {
        \end{lstlisting}
        \vspace{1mm}
    \end{minipage}
    & 
    \begin{minipage}[t]{\linewidth}
        \vspace{1mm}
        \textbf{\scriptsize Basic Block 2}
        \begin{lstlisting}[style=PCodeStyle]
0x001011c2:46:	RSP(0x001011c2:46) = RSP(i) + #0xffffffffffffffe0
0x001011c2:47:	*(ram,RSP(0x001011c2:46)) = #0x1011c7
0x001011c2:69:	u0x10000011:1(0x001011c2:69) = *(ram,RSP(0x001011c2:46))
0x001011c2:48:	call jputchar(free)(#0x32:4,u0x10000011:1(0x001011c2:69))
0x001011c7:49:	u0x00004780(0x001011c7:49) = RSP(i) + #0xfffffffffffffff4
0x001011c7:4a:	u0x00011e80:4(0x001011c7:4a) = *(ram,u0x00004780(0x001011c7:49))
0x001011ca:4d:	u0x00004780(0x001011ca:4d) = RSP(i) + #0xfffffffffffffff0
0x001011ca:4e:	u0x00006a00:4(0x001011ca:4e) = *(ram,u0x00004780(0x001011ca:4d))
0x001011ca:54:	ZF(0x001011ca:54) = u0x00011e80:4(0x001011c7:4a) == u0x00006a00:4(0x001011ca:4e)
0x001011cd:59:	goto Block_3:0x001011cf if (ZF(0x001011ca:54) == 0) else Block_5:0x001011db
        \end{lstlisting}
        \vspace{1mm}
    \end{minipage}
    \\ \hline

    % --- ROW 4 (Block 3) ---
    \begin{minipage}[t]{\linewidth}
        \vspace{1mm}
        \begin{lstlisting}[style=CStyle]
putchar(L'4');
goto LAB_001011e5;
        \end{lstlisting}
        \vspace{1mm}
    \end{minipage}
    & 
    \begin{minipage}[t]{\linewidth}
        \vspace{1mm}
        \textbf{\scriptsize Basic Block 3}
        \begin{lstlisting}[style=PCodeStyle]
0x001011d4:5b:	RSP(0x001011d4:5b) = RSP(i) + #0xffffffffffffffe0
0x001011d4:5c:	*(ram,RSP(0x001011d4:5b)) = #0x1011d9
0x001011d4:6b:	u0x1000001a:1(0x001011d4:6b) = *(ram,RSP(0x001011d4:5b))
0x001011d4:5d:	call jputchar(free)(#0x34:4,u0x1000001a:1(0x001011d4:6b))
0x001011d9:5e:	goto Block_6:0x001011e5
        \end{lstlisting}
        \vspace{1mm}
    \end{minipage}
    \\ \hline

    % --- ROW 5 (Block 4) ---
    \begin{minipage}[t]{\linewidth}
        \vspace{1mm}
        \begin{lstlisting}[style=CStyle]
} else {
  putchar(L'3');
}
        \end{lstlisting}
        \vspace{1mm}
    \end{minipage}
    & 
    \begin{minipage}[t]{\linewidth}
        \vspace{1mm}
        \textbf{\scriptsize Basic Block 4}
        \begin{lstlisting}[style=PCodeStyle]
0x001011b6:33:	RSP(0x001011b6:33) = RSP(i) + #0xffffffffffffffe0
0x001011b6:34:	*(ram,RSP(0x001011b6:33)) = #0x1011bb
0x001011b6:6d:	u0x10000023:1(0x001011b6:6d) = *(ram,RSP(0x001011b6:33))
0x001011b6:35:	call jputchar(free)(#0x33:4,u0x10000023:1(0x001011b6:6d))
0x001011bb:36:	goto Block_5:0x001011db
        \end{lstlisting}
        \vspace{1mm}
    \end{minipage}
    \\ \hline

    % --- ROW 6 (Block 5) ---
    \begin{minipage}[t]{\linewidth}
        \vspace{1mm}
        \begin{lstlisting}[style=CStyle]
putchar(L'5');
}
        \end{lstlisting}
        \vspace{1mm}
    \end{minipage}
    & 
    \begin{minipage}[t]{\linewidth}
        \vspace{1mm}
        \textbf{\scriptsize Basic Block 5}
        \begin{lstlisting}[style=PCodeStyle]
0x001011e0:38:	RSP(0x001011e0:38) = RSP(i) + #0xffffffffffffffe0
0x001011e0:39:	*(ram,RSP(0x001011e0:38)) = #0x1011e5
0x001011e0:6f:	u0x1000002c:1(0x001011e0:6f) = *(ram,RSP(0x001011e0:38))
0x001011e0:3a:	call jputchar(free)(#0x35:4,u0x1000002c:1(0x001011e0:6f))
        \end{lstlisting}
        \vspace{1mm}
    \end{minipage}
    \\ \hline

    % --- ROW 7 (Block 6) ---
    \begin{minipage}[t]{\linewidth}
        \vspace{1mm}
        \begin{lstlisting}[style=CStyle]
LAB_001011e5:
putchar(L'6');
return;
}
        \end{lstlisting}
        \vspace{1mm}
    \end{minipage}
    & 
    \begin{minipage}[t]{\linewidth}
        \vspace{1mm}
        \textbf{\scriptsize Basic Block 6}
        \begin{lstlisting}[style=PCodeStyle]
0x001011ea:3c:	RSP(0x001011ea:3c) = RSP(i) + #0xffffffffffffffe0
0x001011ea:3d:	*(ram,RSP(0x001011ea:3c)) = #0x1011ef
0x001011ea:71:	u0x10000035:1(0x001011ea:71) = *(ram,RSP(0x001011ea:3c))
0x001011ea:3e:	call jputchar(free)(#0x36:4,u0x10000035:1(0x001011ea:71))
0x001011f1:44:	return(#0x0)
        \end{lstlisting}
        \vspace{1mm}
    \end{minipage}
    \\ \hline

\end{longtable}

Basicblocks are created in \texttt{flow.cc} by the method \texttt{FlowInfo::splitBasic}. 
The routine partitions the P-code instruction stream at control-flow boundaries: conditional and unconditional jumps, call sites that alter control flow, and return instructions.
Each such instruction ends the current block and/or starts a new one (targets of jumps also begin blocks).

The \ac{CFG} with the BasicBlocks can also be seen in ghidra by entering the \texttt{Display Function Graph} 
window and enabling the P-code field in the layout of Instruction/Data (These Pcode are the final high-level Pcode). See \Cref{img:cfg_ghidra}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{img/cfg_ghidra.png}
    \caption{Control Flow Graph with P-code in Ghidra}\label{img:cfg_ghidra}
\end{figure}

\subsection{The Structuring Algorithm}\label{sec:stralg}

To transform the \ac{CFG} into C statements, Ghidra employs a structuring algorithm implemented in the 
\texttt{ActionBlockStructure} class (an Action~\ref{sec:ActeRul}). The process involves identifying regions of the graph that match known schemas (or patterns) of control flow:

inside the \texttt{apply} method of \texttt{ActionBlockStructure} we have a call to \texttt{collapseAll} that is the main loop of the algorithm:

\begin{lstlisting}
void CollapseStructure::collapseAll(void)
{
  int4 isolated_count;

  finaltrace = false;
  graph.clearVisitCount();
  orderLoopBodies();

  collapseConditions();

  isolated_count = collapseInternal((FlowBlock *)0);
  while(isolated_count < graph.getSize()) {
    FlowBlock *targetbl = selectGoto();
    isolated_count = collapseInternal(targetbl);
  }
}
\end{lstlisting}

The method implements a deterministic sequence of passes that progressively transform 
the BasicBlocks into structured FlowBlocks and performs the following steps:

\begin{enumerate}
    \item \textbf{Preparation}\\
        The algorithm first clears previous visitation state (\texttt{graph.clearVisitCount}) 
        and invokes \texttt{orderLoopBodies}. This pass discovers loop headers and 
        back-edges, establishing a partial ordering among loop bodies. 
        Detecting loops early is essential to prevent later structuring passes 
        from erroneously breaking loop semantics.

    \item \textbf{Conditional simplification}\\
        Next, \texttt{collapseConditions} attempts to simplify complex boolean logic 
        and fold adjacent blocks that form logical AND/OR patterns (for example, 
        transforming sequences that represent \texttt{if (A \&\& B)} or \texttt{if (A || B)} 
        into single conditional constructs). This phase applies local rules such as \texttt{ruleBlockOr} 
        to reduce predicate complexity before higher-level structuring.

    \item \textbf{Initial collapse}\\
        The engine then calls \texttt{collapseInternal ((FlowBlock *)0)}, which scans the graph 
        and applies standard structuring rules (e.g. \texttt{ruleBlockIfElse}, \texttt{ruleBlockWhileDo}, 
        \texttt{ruleBlockSwitch}) to collapse perfectly structured regions. 
        The routine returns an \texttt{isolated\_count} indicating how many blocks have been 
        fully resolved without introducing gotos.

    \item \textbf{Unstructured flow handling}\\
        If the graph is not fully collapsed (\texttt{isolated\_count < graph.getSize}), 
        the method iterates: it selects a problematic edge with \texttt{selectGoto} and 
        marks that edge as unstructured (to be emitted as a \texttt{goto}/\texttt{break}/\texttt{continue} 
        in the final code). The selection is driven by heuristics  
        to minimize disruption to surrounding structure. After marking the edge, 
        \texttt{collapseInternal (targetbl)} is invoked again (often passing the target block of the newly created goto) 
        so the structuring engine can resume collapsing other regions. This loop repeats until every block 
        is resolved.
\end{enumerate}

in the \texttt{collapseInternal} method we have the main pattern recognition method, 
some patterns have precedence over others, since it may occur that a region matches 
multiple schemas. For example, a \texttt{switch} may also match an \texttt{if-else} pattern. 

These are the preferred patterns, in order:
\begin{itemize}
    \item \texttt{goto}
    \item \texttt{cat} (block concatenation)
    \item \texttt{proper if} (if without else)
    \item \texttt{if-else}
    \item \texttt{while-do}
    \item \texttt{do-while}
    \item \texttt{infinite loop}
    \item \texttt{switch}
\end{itemize}
These ``rules'' are implemented inside a loop that tries every pattern till no more changes are possible.

in the \texttt{ruleBlockWhileDo} method we can see how the pattern matching is done:

\begin{lstlisting}

bool CollapseStructure::ruleBlockWhileDo(FlowBlock *bl)

{
  FlowBlock *clauseblock;
  int4 i;

  if (bl->sizeOut() != 2) return false; // Must be binary condition
  if (bl->isSwitchOut()) return false;
  if (bl->getOut(0) == bl) return false; // No loops at this point
  if (bl->getOut(1) == bl) return false;
  if (bl->isInteriorGotoTarget()) return false;
  if (bl->isGotoOut(0)) return false;
  if (bl->isGotoOut(1)) return false;
  for(i=0;i<2;++i) {
    clauseblock = bl->getOut(i);
    if (clauseblock->sizeIn() != 1) continue; // Nothing else must hit clause
    if (clauseblock->sizeOut() != 1) continue; // Only one way out of clause
    if (clauseblock->isSwitchOut()) continue;
    if (clauseblock->getOut(0) != bl) continue; // Clause must loop back to bl

    bool overflow = bl->isComplex(); // Check if we need to use overflow syntax
    if ((i==0)!=overflow) {			// clause must be true out of bl unless we use overflow syntax
      if (bl->negateCondition(true))
	dataflow_changecount += 1;
    }
    BlockWhileDo *newbl = graph.newBlockWhileDo(bl,clauseblock);
    if (overflow)
      newbl->setOverflowSyntax();
    return true;
  }
  return false;
}
\end{lstlisting}

Firstly it checked that the block has exactly two outgoing edges (a binary condition) and is not already part of a switch or a loop. 
Then, for each outgoing edge, it checks if the clauseblock (the potential loop body) has exactly one incoming edge 
(from the condition block) and one outgoing edge (back to the condition block). 
If these conditions are met, it confirms the presence of a while-do loop structure.

A condition is considered complex when the basic block that computes it contains too many instructions to be cleanly represented within a single conditional expression.
The method \texttt{BlockBasic::isComplex} performs this check.

Criteria: the algorithm counts the number of \emph{statements} in the block:
\begin{itemize}
    \item A conditional jump (branch) counts as 1 statement.
    \item \texttt{CALL} instructions count as 1.
    \item Operations that produce outputs used only inside the block or marker instructions do not count, 
    but if a variable is used many times or is tied to memory, it contributes to the count.
\end{itemize}
If the total number of statements in the block exceeds 2, the block is considered complex.

The overflow syntax (\texttt{f\_whiledo\_overflow}) is a specific state assigned to a \texttt{BlockWhileDo} when its loop control condition is determined to be complex.
It indicates that, although a logical \texttt{while} structure exists, the conditional block is too long or complicated to be emitted as a single boolean expression \texttt{while(condition)\{\}}.
Instead of printing \texttt{while(<complex condition>)\{\}}, the decompiler emits an alternative form, typically an infinite loop with an internal \texttt{break} to preserve semantics.

After identifying a structure in the next iteration of the main loop in \texttt{collapseInternal}, a single FlowBlock representing the high-level construct (e.g., a \texttt{BlockWhileDo} for a while loop) is created. 
This new block encapsulates the original matched block, maintaining their internal P-code operations while providing a structured interface for further processing and eventual emission.

\subsection{The \emph{for} special case}
As can be seen in \Cref{sec:stralg}, the Ghidra decompiler does not have an explicit rule to recognize \texttt{for} loops. 
Indeed, \texttt{for} loops in Ghidra are treated as special cases of \texttt{while-do} loops\footnote{The transformation is triggered only if the architecture option \texttt{analyze\_for\_loops} is enabled.}:
The check is performed in the method \texttt{BlockWhileDo::finalTransform}, this method proceeds only if the block is not marked with overflow syntax.
\begin{enumerate}
    \item \textbf{Loop variable identification:} \texttt{findLoopVariable} is called to search for a variable controlling the iteration (e.g., \texttt{i} in \texttt{i < 10}). 
    This variable must appear in the exit condition and be modified within the loop body.
    \item \textbf{Initializer identification:} \texttt{findInitializer} searches for the instruction that initializes the variable (e.g., \texttt{i = 0}) 
    in the block immediately preceding the loop.
    \item \textbf{Relocation:} If both an iterator (\texttt{iterateOp}) and an initializer (\texttt{initializeOp}) are found, 
    the decompiler physically moves the P-code operations (using \texttt{opUninsert} / \texttt{opInsertAfter}) so they lie adjacent to the loop boundaries, 
    preparing them for syntactic emission.
    \item \textbf{Non-printing marking:} In \texttt{finalizePrinting} these operations are marked with \texttt{opMarkNonPrinting}. 
    This instructs the emitter not to print them as separate statements inside the body or before the loop, but to include them in the \texttt{for (\ldots)} header.
\end{enumerate}


\begin{lstlisting}

void BlockWhileDo::finalTransform(Funcdata &data)
{
  // Simplification style
  BlockGraph::finalTransform(data);
  if (!data.getArch()->analyze_for_loops) return;
  if (hasOverflowSyntax()) 
      return; // Still too complex
  FlowBlock *copyBl = getFrontLeaf();
  if (copyBl == (FlowBlock *)0) return;
  BlockBasic *head = (BlockBasic *)copyBl->subBlock(0);
  if (head->getType() != t_basic) return;
  PcodeOp *lastOp = getBlock(1)->lastOp();	// There must be a last op in body, for there to be an iterator statement
  if (lastOp == (PcodeOp *)0) return;
  BlockBasic *tail = lastOp->getParent();
  if (tail->sizeOut() != 1) return;
  if (tail->getOut(0) != head) return;
  PcodeOp *cbranch = getBlock(0)->lastOp();
  if (cbranch == (PcodeOp *)0 || cbranch->code() != CPUI_CBRANCH) return;
  if (lastOp->isBranch()) {			// Convert lastOp to -point- iterateOp must appear after
    lastOp = lastOp->previousOp();
    if (lastOp == (PcodeOp *)0) return;
  }

  findLoopVariable(cbranch, head, tail, lastOp);
  if (iterateOp == (PcodeOp *)0) return;

  if (iterateOp != lastOp) {
    data.opUninsert(iterateOp);
    data.opInsertAfter(iterateOp, lastOp);
  }

  // Try to set up initializer statement
  lastOp = findInitializer(head, tail->getOutRevIndex(0));
  if (lastOp == (PcodeOp *)0) return;
  if (!initializeOp->isMoveable(lastOp)) {
    initializeOp = (PcodeOp *)0;		// Turn it off
    return;
  }
  if (initializeOp != lastOp) {
    data.opUninsert(initializeOp);
    data.opInsertAfter(initializeOp, lastOp);
  }
}
\end{lstlisting}

If all conditions are met, the decompiler effectively transforms the \texttt{while-do} structure into a \texttt{for} loop by relocating and marking the relevant P-code operations.

\subsection{The \emph{Goto} Problem}
A significant limitation of this approach arises when the \ac{CFG} contains irreducible control flow 
that does not match any predefined schema. (This is common in binaries optimized with aggressive compiler 
techniques or those containing manual assembly optimizations).

When \texttt{ActionBlockStructure} fails to find a matching pattern, the jump inside the FlowBlock remains and it 
will be represented as a \emph{goto}\footnote{Or a \texttt{break}/\texttt{continue} if it jumps out of/into a loop structure.}
statement to preserve semantic correctness, this phenomenon significantly degrades the readability of the output.

\section{Code Emission}\label{sec:c_emission}

The final phase of the pipeline is the translation of the structured High P-code into C syntax. 
This is not a simple text dump but a structured generation of an \ac{AST} represented 
by \texttt{ClangToken} objects.

Before emission, the \emph{ActionNameVars} pass attempts to assign meaningful names to the 
recovered \texttt{HighVariable} objects. If debug symbols (DWARF, PDB) are available, 
they are utilized. In their absence, Ghidra relies on heuristics based on variable usage 
(e.g., loop counters named \texttt{i}, \texttt{j}) or storage location 
(e.g., \texttt{iVar1}, \texttt{uVar2}). This process is highly stochastic and often results in generic, 
non-descriptive identifiers.

The C++ backend generates a stream of \texttt{ClangToken} objects representing the code structure. This tokenized representation is 
sent to the Java frontend via the XML protocol. This structured data allows the Ghidra 
\ac{GUI} to provide interactive features—such as cross-referencing and dynamic renaming—since the UI 
elements remain linked to the underlying \texttt{Varnode} and \texttt{HighVariable} objects.

\section{Large Language Models}\label{sec:llm}
The advent of \ac{LLM} marks a fundamental discontinuity in the history of artificial intelligence and \ac{NLP}. It is not merely an increase in computational capacity, 
but an ontological redefinition of how machines process, represent, and generate semantic information. 
At the heart of this revolution lies the Transformer architecture, introduced in 2017 by Vaswani~\cite{Paper:vaswani2023attentionneed}, which enabled overcoming the sequential 
limitations of previous \ac{RNN} and \ac{LSTM} architectures.

\subsection{Transformer}
The shift from recurrent architectures to the Transformer was motivated by the need for parallelization 
and the handling of long-range dependencies. Whereas \ac{RNN}s processed tokens sequentially $(t_1, t_2, \dots, t_n)$, 
accumulating error and dispersing the gradient over long sequences, the Transformer processes the entire sequence simultaneously, 
relying entirely on the attention mechanism.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{img/ModalNet-21.png}
    \caption{The Transformer model architecture}\label{img:transformer}
\end{figure}

As shown in figure~\ref{img:transformer}, the Transformer architecture consists of an encoder-decoder structure, 
where both components are composed of multiple layers of self-attention mechanisms and feed-forward neural networks.

The \textbf{encoder} (left) ingests a token sequence $x=(x_1,\dots,x_n)$ and produces continuous representations
$z=(z_1,\dots,z_n)$. The \textbf{decoder} (right) conditions on $z$ and generates the output tokens
$y=(y_1,\dots,y_m)$ autoregressively, emitting one token per step. Both sides are built by stacking
identical blocks composed of Multi-Head Attention and position-wise Feed-Forward layers, typically
wrapped with residual connections and normalization. Inputs and targets are first embedded into a
n-dimensional space, and a positional encoding is added to each embedding to encode token order~\cite{Paper:vaswani2023attentionneed}.

At the core there is Multi-Head Attention, which allows the model to jointly attend to information from different representation
subspaces at different positions. Each attention head computes scaled dot-product attention, enabling the model to
focus on relevant parts of the input sequence when generating each output token.

Most of the state of art \ac{LLM}s use an architecture with only the decoder part,
omitting the encoder entirely~\cite{site:MostLLMs}. This design choice is preferred for its simplicity,
its good zero-shot generalization, and cheaper training cost to attain a reasonable performance.

\subsection{Tokens}

In \ac{LLM}s, text is processed in chunks called \textbf{tokens}. A token can represent a word, a subword, or even a single character, depending on the tokenization scheme used.
The choice of tokenization method significantly impacts the model's performance, as it affects how the model interprets and generates text.~\cite{Paper:Mullen2018}

We can see an example using \myhref{https://tiktokenizer.vercel.app/}{tiktokenizer}, a webtool for visualizing tokenization for differents models, to tokenize a sentence:

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{img/tiktokenizer.png}
    \caption{Tokenization example using tiktokenizer}\label{img:tokenization}
\end{figure}

As shown in figure~\ref{img:tokenization}, the sentence ``Hi i am Timossi Luigi'' is tokenized into a sequence of tokens.
Each token corresponds to a specific integer ID in the model's vocabulary.

For this example, for the model \texttt{gpt-4o} the token `` i'' corresponds to the ID 575.
The tokenization process is crucial for \ac{LLM}s, as it transforms raw text into a format that the model can process.
Different models use different tokenization values or schemes like \ac{BPE}.

In the tokens we count also the special tokens like \texttt{$<|im\_start|>$} that indicates boundaries for roles messages like system, assistant, or user.
in this case the token \texttt{$<|im\_start|>$} corresponds to ``input message start'' after that we have the role then the token ``input message separator'' the message and finally the token ``input message end''.
Different models use different special tokens or does not use them at all.

\subsection{Softmax}
The \texttt{Softmax} function is used as an output function in the last layer of a neural networks to transform a vector of real values into a probability distribution. (see figure~\ref{img:transformer})
This function for each component of the vector it computes the exponential normalized by the sum of the exponentials of all components, 
producing in output a vector of the same dimension with values in the interval [0,1] whose sum is 1.~\cite{site:baeldungWhatTemperature}

\[ \mathrm{softmax}(y_{i}) = \frac{e^{y_{i}}}{\sum_{j=1}^{n}e^{y_{j}}} \]

where $y = (y_{1}, y_{2}, .., y_{n})$ is an input vector and values $y_{i}, (i=\overline{1,n})$ are in range from $-\infty$ to $+\infty$.

\subsection{Quantization}
Quantization is a technique used to reduce the memory footprint and computational requirements of neural networks by representing weights and activations with lower precision.
In the context of \ac{LLM}s, quantization can be applied to the model's parameters (weights) and activations (intermediate outputs) to enable faster inference and reduce memory usage, especially on resource-constrained devices.~\cite{site:WhatQuantization,site:huggingfaceQuantization}
There are different quantization schemes, such as:
\begin{itemize}
    \item \textbf{Post-training quantization}: This method quantizes a pre-trained model without requiring additional training. It can be applied to both weights and activations, but it may lead to a drop in model accuracy if not done carefully.
    \item \textbf{Quantization-aware training}: This method incorporates quantization into the training process, allowing the model to learn to compensate for the reduced precision. This approach typically results in better accuracy compared to post-training quantization.    
    \end{itemize}
Quantization can significantly reduce the computational requirements of \ac{LLM}s, enabling faster inference and making it feasible to deploy large models on edge devices or in real-time applications. However, it is important to carefully evaluate the impact of quantization on model performance, as aggressive quantization can lead to a significant drop in accuracy.

\section{Decoding}
When the model generates text, it produces a vector of raw scores, called \emph{logits}, for each
token in the vocabulary at each timestep. These logits represent the unnormalized likeli-hood of each token being the next token in the sequence.
By applying the \texttt{Softmax} function to these logits, the model obtains a probability distribution over the vocabulary.
Once the probability distribution is computed, the model must select the next token. 
This process, called decoding, can be influenced by different strategies such as:\cite{site:ibmFoundationModel}

\begin{itemize}
    \item \textbf{Greedy decoding}: always select the token with the highest probability, produces output that closely matches 
    the most common language in the model's pretraining data and in your prompt text, which is desirable in less creative or fact-based use cases. 
    This can cause the model to produce repetitive or generic output.
    \item \textbf{Sampling decoding}: the model chooses a subset of tokens, and then one token is chosen randomly from this subset 
    to be added to the output text. Sampling adds variability and randomness to the decoding process, 
    which can be desirable in creative use cases. This can cause the model to produce unexpected or incorrect output.
\end{itemize}

\subsection{Temperature}

Temperature ($T$) is a hyperparameter that acts directly on the \texttt{Softmax} function. 
The \texttt{softmax} function with temperature becomes:

\[ \mathrm{softmax}(y_{i}) = \frac{e^{ (\frac{y_{i}}{T}) }}{\sum_{j=1}^{n}e^{ (\frac{y_{j}}{T}) }} \]

where $T > 0$ is the temperature parameter. The temperature modifies the distribution of probabilities over the tokens:

\begin{itemize}
    \item $T < 1$ (Cooling): Differences between logits are amplified. The token with the highest logit receives a probability close to 1. 
    The distribution becomes ``peaked'', reducing variety and increasing determinism. Useful for logical or mathematical tasks.
    \item $T > 1$ (Heating): Differences are flattened. The distribution tends toward uniformity. Tokens with lower logits gain probability mass, 
    increasing ``creativity'' but also the risk of incoherence (hallucinations).
    \item $T \to 0$: Equivalent to the \texttt{Greedy decoding}, where always the single most probable token is chosen.
\end{itemize}

Miklos and Rebeka (both Junior Research)~\cite{site:poltextlabParametersExplained}, in their study on the impact of temperature on text generation have tested different temperature settings
using OpenAI GPT-4.1 with the prompt \texttt{Why do researchers use control groups in experiments?}; They send two times the same prompt with different temperature settings and analyzed the outputs:

\begin{itemize}
    \item At $T=0.1$, the output was identical for both runs and both answers offered a clear, textbook-style explanation
    \item At $T=1.4$, the outputs varied between runs, providing more expressive, creative answers with illustrative examples.
    \item At $T=2$, the outputs became incoherent and nonsensical with grammatical errors, illogical statements, and gibberish characters.
\end{itemize}

These results highlight how much temperature settings influence the balance between coherence and creativity in \ac{LLM} outputs.

Even with an optimal temperature, the long tail of the distribution (thousands of tokens with infinitesimal but nonzero probability) can introduce errors if sampled.
To mitigate this, techniques like \texttt{Top-k} and \texttt{Top-p} sampling are employed.

\subsection{Top-p and Top-k}
Top-p (nucleus) sampling is a stochastic decoding strategy that at each generation step restricts sampling to the smallest
subset of tokens whose cumulative probability is at least a threshold $p$.~\cite{site:wikipediaToppSampling}.

Formally, given vocabulary $V$ and context $x_{<t}$, the nucleus $V^{(p)}$ is the minimal subset satisfying

\[ V^{(p)} \subseteq V,\qquad \sum_{x\in V^{(p)}} P(x\mid x_{<t}) \ge p . \]

Tokens outside \(V^{(p)}\) are assigned zero probability; probabilities inside the nucleus are renormalized

Top-k sampling limits the candidate tokens to the k most probable ones at each step, nucleus sampling dynamically adjusts 
the candidate set based on the cumulative probability threshold \(p\).

The combined use of Temperature (to model the shape of the curve) and Top-p (to intelligently truncate the tail) represents the current 
industry standard for high-quality text generation.

\section{Perplexity}\label{sec:perplexity}

Evaluating the quality of an \ac{LLM} is intrinsically challenging because language judgments are often subjective. 
Nonetheless, there are rigorous quantitative metrics. \emph{Perplexity} is the standard measure used during \ac{LLM} pre-training; 
it stems from information theory and quantifies the model's uncertainty when predicting the next token.~\cite{site:wikipediaPerplexityWikipedia}

Mathematically, perplexity is the exponential of the average negative log-likelihood (i.e., the exponentiated cross-entropy) 
of the predicted tokens. Exponentiating the cross-entropy restores the measure to probability-like units, yielding an intuitive 
``effective branching factor'' the average number of plausible next-token choices the model considers.~\cite{site:cometPerplexityEvaluation}

\[
\mathrm{PPL}(X) = \exp\!\left( -\frac{1}{N} \sum_{i=1}^N \log P(x_i \mid x_{<i}) \right)
\]

A perplexity of K indicates that, on average, the model behaves as if it were choosing among K equally likely alternatives at each prediction step.~\cite{site:UnderstandingPerplexity}
\begin{itemize}
    \item Relation to entropy: \(\mathrm{PPL} = 2^{H(P)}\), where \(H(P)\) denotes the Shannon entropy of the distribution.
    \item Interpretation: Lower perplexity means the model assigns higher probability mass to the true tokens from the test set.
\end{itemize}
Note that a low perplexity reflects statistical predictability relative to the training corpus and does not guarantee factual accuracy or correctness.

\section{Human-like}\label{sec:human-like}
\ac{LLM} are designed to generate text that closely mimics human language, capturing nuances, context, and stylistic elements.
This capability is achieved through extensive training on vast corpora of text, enabling the models to learn patterns and structures inherent in human communication.
This mimic of human-like text generation has profound implications across various domains, including customer service, content creation, and education.
since the generated text is often indistinguishable from that written by humans, \ac{LLM} we can use them to measure how ``human-like'' is a piece of text.
If we have two C functions that perform the same task but generated with different decompilers (or different settings/version of the same decompiler), 
we can probably assume that using an \ac{LLM} to measure how ``human-like'' is the generated code can be a good proxy for code quality/readability.

Here we have two main path to measure the human-likeness of a piece of code:
\begin{itemize}
    \item Ask the \ac{LLM} directly via prompt to rate the human-likeness of the code. This approach can lead to subjective and inconsistent results, 
    as the model's responses may vary based on the prompt phrasing, the context, and hallucinations.
    \item Use perplexity as a quantitative metric to evaluate the human-likeness of the code. This approach leverages the statistical 
    properties of the language model to assess how well the generated code aligns with patterns learned from human-written code.
\end{itemize}
As shown in \Cref{sec:perplexity}, perplexity measures how well a language model predicts a sequence of tokens. A lower perplexity indicates that the model finds the sequence more predictable, 
which often correlates with human-like text. By calculating the perplexity of code snippets generated by different decompilers, we can objectively compare their human-likeness.
For this reason, perplexity is used only to evaluate the Human-like quality of the decompiled code, it \textbf{does not} evaluate its functional correctness.
