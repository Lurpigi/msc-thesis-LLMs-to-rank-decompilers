\chapter{Methodology}\label{ch:method}

The framework of our work is based on a client-server architecture, where the server hosts the \ac{LLM} and provides an \ac{API} for interacting with it, 
while the client is responsible for building specific Ghidra version, preparing the code samples and prompts, invoking the server, and collecting results in \ac{JSON} format.
Every service is designed to be modular, allowing for the integration of different \ac{LLM} models and evaluation metrics, for
reproducibility we used \emph{Docker Compose} to containerize both the server and client components, ensuring consistent environments across different machines and operating systems.
The dataset creation is also a containerized process, and the result are mounted as volumes to the client container, allowing for easy access and manipulation of the data without the need for complex data transfer mechanisms.

\section{Dataset Maker}\label{sec:datmak}
As written on related works in~\Cref{sec:llmbench}, we decided to use a subsect of the complete OSS Fuzz dataset used by DecompileBench~\cite{Paper:gao2025decompilebenchcomprehensivebenchmarkevaluating} for our evaluation,
specifically the four Open Source projects: \texttt{file, libxls, readstat, xz}, which are written in C and have a rich history of commits and pull requests on GitHub.
These choice was motivated by the need to have a manageable dataset size for local evaluation, while still covering a set of real world code and functions to evaluate our approach.
for every project the dataset maker extracts all the functions and recompiles them  into standalone binaries; this process create different optimization levels of the binary, specifically \texttt{-O0} and \texttt{-O2}, and \texttt{-O3} 
which are the most common optimization levels used in real world scenarios, and which can have a significant impact on the decompilation output and its readability.

For obtaining this we had to fork the original dataset maker script and modify it to fit our edits; such as the specifically optimization levels (in the original repo they were using all the optimization levels) and some bug fix as pointed out by one pull request on the original repo~\cite{site:githubVarietyProblems}.
so we clone our fork into a container (wich will also run docker inside for building the projects), edited with the patches as shown in the \texttt{README} file of DecompileBench, selected just our four projects and then run the dataset maker script.

\subsection{Dataset Collection}
The result of the dataset maker is a folder named \texttt{Dataset} wich contains other three subfolders:
\begin{itemize}
    \item \textbf{binary}: contains the compiled binary of the functions, every file is named with the format \texttt{task\_project\_functionName-OX.so}, and can be used for decompilation and evaluation.
    \item \textbf{compiled\_ds}: contains a file structure of the dataset format used by the \texttt{Datasets} library~\cite{site:pypiDatasets}, which is a Python library for handling large datasets in a efficient way, and which we use for loading the dataset in our client code. 
        This structure have a file ``.arrow'' that store data and two \ac{JSON} files for the metadata such as field names and types. In our case we are interested only in three fields \texttt{file}, which contain the name of the function, \texttt{path} wich contains the path ``binary/namefile'', and \texttt{func} wich contains the source code of the function.
    \item \textbf{eval}: contains also a dataset structure, but we will not use it since is used for recompile success and other metrics that we are not interested in, since we want to focus on the evaluation of the decompilation output rather than the compilation process.
\end{itemize}

\section{LLM Server}

The server is responsible for hosting the \ac{LLM} and providing an \ac{API} for interacting with it, specifically for receiving code samples and prompts from the client, processing them with the \ac{LLM}, and returning the results.
The server is designed to be modular, allowing for the integration of different \ac{LLM} models and evaluation metrics, and it is containerized using Docker for reproducibility and ease of deployment.

It uses Gnunicorn as the WSGI HTTP server for handling incoming requests, and it is built on top of a Python web framework (Flask) to define the \ac{API} endpoints and handle the logic for processing requests and interacting with the \ac{LLM}.

\subsection{Models}
The heavy part of the framework is without doubt the server, and the models that runs on it.
In our case the local enviroment is a single \ac{GPU} machine with 16 GB of \ac{VRAM}, so we had to select models that can run on this hardware, and that can provide a good performance for our evaluation.
We also used the \myhref{https://apxml.com/tools/VRAM-calculator}{\ac{VRAM} Calculator} to estimate the memory requirements of different models and ensure they fit within our hardware constraints, inside the \ac{VRAM} have to cohesist different areas, such as:
\begin{itemize}
    \item \textbf{Base Model Weights}: The trained parameters of the model, the ``weights'' with their precision (could be quantizated for reduced memory usage).
    \item \textbf{Activations}: Intermediate computation results during forward passes through the layers. This grows with batch size and input length, and is critical for stability during inference.
    \item \textbf{KV Cache}: Key-Value cache used to avoid recomputing attention for previously processed tokens. Given the lengthy decompilation prompts containing source code, this cache grows proportionally with input length.
    \item \textbf{Framework Overhead}: Fixed memory cost from PyTorch, CUDA drivers, and buffer management. This overhead exists regardless of model size.
\end{itemize}


\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{img/old_models_distribution.png}
    \caption{Distribution of winner bias, the TIE is when a model select always one choice (BASE, PR) even after swapping the input.}\label{fig:bias_winner}
\end{figure}

Initially we wanted to use modles in the range of 8B parameters, the canditates where Meta Llama 3.1 (8B), Qwen2.5-Coder-Instruct (7B), DeepSeek-R1-Distill-Qwen (7B) and Google Gemma 2 (9B)
for various reasons, such as the availability of the model, the performance on code-related tasks, and the memory requirements, but after a lot of testing and variation of prompts, we decided to going up to use models in range of 14B parameters since 
the firsts were not able to provide consistent and meaningful scores for our evaluation, falling into the Position Bias problem, which is a common issue in \ac{LLM}-based evaluation where the model's scoring is heavily influenced by the position of the input text rather than its content, leading to unreliable assessments (even when we tried to mitigate it with different prompt engineering techniques, such as randomizing the order of the inputs, or using different templates for the prompts).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img/little_change.png}
    \caption{Example of a function with only one difference between the BASE and the PR.}\label{fig:little_change}
\end{figure}

As example of this problem in~\Cref{fig:bias_winner} when the majority of the scores are in the ``TIE'' category, meaning that the model is always selecting the same choice (either BASE or PR) as the winner, even after swapping the input order, which indicates a strong position bias and a lack of meaningful differentiation between the two inputs based on their content.
Even after checking the non TIE cases, we found that the motivation for the choice was not based on the content of the decompilation and the difference beetween the two inputs.
In this case whe have a function with only one difference between the BASE and the PR, that can be seen in~\Cref{fig:little_change}, and the corrisponding responses in~\Cref{fig:Biases} from the model, 
Among the TIEs we have two cases where the model consistently selected a winner, in the case of PR the motivation was consistent with the content of the decompilation and the difference between the two inputs, while in the case of BASE the motivation was on some superficial aspect of the input ignoring the context.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img/biases.png}
    \caption{Example of the motivation for the choice of the model, where the motivation is not based on the content of the decompilation and the difference beetween the two inputs, but rather on some superficial aspect of the input.}\label{fig:Biases}
\end{figure}

So for these reasons we decided to use bigger models, since with these results we were not able to draw any meaningful conclusion from the evaluation, and we wanted to have a more reliable and consistent evaluation for our study.

\subsubsection{Model Selection}
Based on these considerations, we wanted to select 4 models for our evaluation, the range of 14B parameters is the last range that can be run on our hardware with 4-bit quantization, and that can provide a good performance for our evaluation.
Unfortunately, due to the scarcity of models in this range, and the tendency of relasing only very big or very small models, we did not have much choice in the selection (Exluding models done by Google, OpenAI or Meta), we found these state of art models:

\begin{itemize}
    \item Qwen 3 (14B)
    \item DeepSeek-R1 Distill Qwen (14B)
    \item Phi 4 (14B)
    \item Mistral Nemo instruct 2407 (14B)
\end{itemize}

Unfortunatly after some testing we found out that Phi 4 and Mistral Nemo were not able to provide consistent and meaningful scores for our evaluation, falling into the Position Bias problem as can be seen in~\Cref{fig:new_models_distribution}, we also tried to replace them with Qwen2.5 Coder and Starcoder2 (both 15B) but even with those models we were not able to get consistent results, so we decided to use only Qwen3 and DeepSeek-R1 Distill Qwen, since they were able to provide more reliable and consistent scores.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img/new_models_distribution.png}
    \caption{Distribution of winner bias for the new models, where we can see that Phi 4 and Mistral Nemo are still falling into the Position Bias problem, while Qwen3 and DeepSeek-R1 Distill Qwen are providing more reliable and consistent scores for our evaluation.}\label{fig:new_models_distribution}
\end{figure}

\begin{itemize}

\item \textbf{Qwen 3 (14B)}: is the latest generation in the Qwen family and comes as a suite of both dense and mixture-of-experts (MoE) models. A distinctive feature of the series is the ability to switch within the same model between a \emph{thinking} mode (aimed at complex reasoning, mathematics, and coding) and a \emph{non-thinking} mode (optimized for efficient, general-purpose dialogue), allowing the same backbone to adapt to different interaction styles and difficulty levels~\cite{site:huggingfaceQwenQwen314BHugging}
\item \textbf{DeepSeek-R1 Distill Qwen (14B/15B\footnote{In the name and the description it shown that is 14B parameters but in the tag of huggingface site its tagged as 15B})}: Part of the DeepSeek-R1 family, which introduces a reasoning-focused post-training pipeline combining cold-start supervised data and large-scale reinforcement learning to improve reasoning quality and readability over purely RL-trained variants (e.g., DeepSeek-R1-Zero). The \emph{Distill} checkpoints transfer (distill) the reasoning patterns learned by the larger DeepSeek-R1 model into smaller dense backbones (including Qwen-based models), providing strong math/code/reasoning capability in a size that is practical to run locally on our hardware~\cite{site:deepseekai2025deepseekr1incentivizingreasoningcapability}.
%\item \textbf{Qwen 2.5 Coder Instruct (14B/15B\footnote{same as above})}: An earlier version of the Qwen series, specifically fine-tuned for code-related tasks. It serves as a useful benchmark to compare against the newer Qwen 3 and the distilled version, allowing us to assess the impact of model size and training on evaluation performance~\cite{site:qwen2, site:hui2024qwen2}.

\end{itemize}

We wanted to include different types of models but in the end we had to select models that are all based on the same architecture (Qwen), since they were the only ones that can provide consistent and meaningful scores 
for our evaluation, and that can fit in our hardware constraints, but we acknowledge that this is a limitation of our study and that it would be interesting to include models with different architectures and training paradigms 
in future work, to assess the generality of our findings across a wider range of \ac{LLM} designs.

\subsection{Configuration}\label{sec:configLLM}
The server supports multiple local \acp{LLM} through a simple configuration layer that maps a short, client-facing identifier to the corresponding Hugging Face repository name. 
Concretely, a dictionary (\texttt{MODELS\_CONFIG}) defines the available models and is the single source of truth for both the \texttt{/models} endpoint and for request-time model switching.

For ensure lightness, all models are loaded using 4-bit quantization via \texttt{bitsandbytes}.

\begin{verbatim}
bnb_config = BitsAndBytesConfig(
  load_in_4bit=True,
  bnb_4bit_quant_type="nf4",
  bnb_4bit_use_double_quant=True,
  bnb_4bit_compute_dtype=torch.bfloat16
)
\end{verbatim}

On startup, the server checks the availability of the \ac{GPU}, CUDA and, to reduce cold-start delays, checks and optionally downloads all model snapshots before accepting requests, ensuring that evaluation runs are not affected by network variability.
The greatest bottleneck in our setup is the model loading time and context switch (changing from one model to another), which can take various minutes per model due to the size and quantization overhead. We have organized the request pipeline in the Client to minimize the number of context switches but to mitigate further, 
the server implements a simple caching mechanism: At the start, when checking if the weights are in Huggingface cache folder we load the model into the \ac{GPU} for quantization,
after that we move the quantized weights and his tokenizer to CPU memory (RAM) and unload the model from \ac{VRAM}. When every model have been loaded the server opens up requests maintaining the quantized weights in RAM, so that every context switch can be fulfilled by quickly moving the quantized weights from RAM to \ac{VRAM}, which is much faster than loading from disk and quantizing on the fly.

\subsection{Decoding strategy (temperature and top-p)}
For the \texttt{/generate} endpoint, we configured the decoding parameters through a dedicated function that returns a dictionary of \texttt{transformers} generation arguments. 
In our experiments we rely on \emph{nucleus sampling} (\texttt{top\_p}) with a low \texttt{temperature}, to balance determinism (useful for fair comparisons across decompilers) 
and the ability to escape repetitive or low-quality completions.

Concretely, the default configuration is:
\begin{itemize}
    \item \texttt{temperature=0.4}: reduces randomness by sharpening the token distribution. Lower values make outputs more stable across runs, which is desirable for evaluation.
    \item \texttt{top\_p=0.9}: nucleus sampling, i.e., tokens are sampled only from the smallest set whose cumulative probability mass is $p$. This prevents the model from selecting very unlikely tokens while still allowing variation.
    \item \texttt{max\_new\_tokens=4096}: upper bound on completion length, used as a safety and latency-control measure.
\end{itemize}

\subsection{Routes}
The server exposes a minimal \ac{REST} \ac{API}, All endpoints exchange \ac{JSON} payloads and are intentionally kept coarse-grained (a lot of work in a single request) to decouple the client implementation from model-specific details. The available routes are:

\begin{itemize}
    \item \textbf{GET /}: health check endpoint. It returns the server readiness status, whether CUDA is available, and the currently loaded model identifier (if any). This is used by docker compose to ensure healthcheck status for required services.
    \item \textbf{GET /models}: returns the list of supported model keys (the abstract identifiers used by the client), mapped server-side to Hugging Face repository IDs.
    \item \textbf{POST /generate}: main inference endpoint. The request body includes \texttt{model\_id} and a \texttt{prompt}. The server loads (or switches to) the requested model, wraps the prompt into a chat-style template via the tokenizer, runs text generation, and returns the generated completion.
    \item \textbf{POST /score}: scoring endpoint used to compute a language-model based score for a given text. The request body includes \texttt{model\_id} and \texttt{text}. The server computes the token-level negative log-likelihood and returns the derived perplexity.
    \item \textbf{POST /free}: explicit cleanup endpoint to unload the currently resident model and aggressively release GPU memory.
\end{itemize}

Since different models cannot fit simultaneously in \ac{GPU} memory, model switching is handled server-side: each request triggers a check on the currently loaded model and, if needed, a full unload/load cycle. To avoid concurrent access to \ac{GPU} state, all inference and scoring operations are protected by a global lock, enforcing sequential execution.

\subsection{Metrics}
To make the evaluation reproducible and to quantify server-side overhead, the server logs per-request performance metrics to a CSV file (\texttt{llm\_metrics.csv}). Each entry includes:

\begin{itemize}
    \item \textbf{Model and operation}: \texttt{model\_id} and \texttt{operation} (\texttt{generate} or \texttt{score}).
    \item \textbf{Latency}: wall-clock duration (seconds) measured around the full operation, including tokenization and \ac{GPU} synchronization.
    \item \textbf{Peak GPU memory}: peak \ac{VRAM} allocated during the operation, obtained via CUDA peak memory statistics.
    \item \textbf{Tokens}: number of prompt/input tokens and number of generated output tokens; these are also used to derive an approximate throughput (tokens per second).
\end{itemize}

Metric collection is implemented via a dedicated monitoring context manager that resets CUDA peak counters before execution and synchronizes the device before reading final statistics. 
This design provides a uniform measurement procedure across both generation and perplexity scoring, and enables later analysis of the impact of model switching, 
prompt length, and decoding configuration on runtime and memory usage.

\section{Client}

The client is responsible for orchestrating the entire evaluation workflow, including building specific Ghidra versions, preparing code samples and prompts, invoking the server for decompilation and scoring, and collecting results in \ac{JSON} format for analysis.
It is designed to be modular and flexible, allowing for easy integration of different evaluation strategies and metrics, and it is containerized using Docker for reproducibility and ease of deployment.

The evaluation stage is an end-to-end pipeline over a set of target Ghidra \acp{PR}. At a high level, the client:
\begin{enumerate}
    \item Ensures that the \emph{base} version is built and has produced decompilations for all dataset binaries
    \item Iterates over the selected \ac{PR}, building each corresponding Ghidra revision and extracting the related decompilations
    \item Selects a limited subset of binaries that actually exhibit decompilation differences, to focus the evaluation on meaningful cases and reduce noise
    \item For each model, queries the \ac{LLM} server to score and compare the outputs, producing per-\ac{PR} and aggregate \ac{JSON} reports for later analysis.
\end{enumerate}

\subsection{Building Ghidra}
The build process is automated via Python scripts that interact with Git and Gradle (we use an ubuntu image for the container).
Firstly we clone and build the Ghidra repository from GitHub, this version is used as the base for all our evaluations.
after building base and extracted the functions from binary, we get the \ac{PR}s number that we want to evaluate against base from a function that calls github \ac{API} and returns the list of all \ac{PR}s of Ghidra.
Then for each \ac{PR} we have to checkout the specific version of Ghidra, for doing this we have a script that takes as input the \ac{PR} number, and then it fetches the specific head reference from the GitHub repository (\texttt{pull/ID/head:pr-ID}) and checks it out.

For building Ghidra are necessary two prerequisites: \texttt{Java 11} and \texttt{Gradle} (optionally), the first one is required for running the build scripts and the second one is used for managing dependencies and building the project, 
but since Ghidra in newer versions includes a wrapper for Gradle (\texttt{gradlew}), you can use it without installing Gradle globally.

One problem is that every version of Ghidra need a specific version of Java, so we have to check the \texttt{application.properties} file inside the repo for the required minimal Java version, and then install it in the container before building Ghidra.
So inside the container we manage more than one version of Java, and we switch between them based on the requirements of the Ghidra version we are building.
Another problem is that some \ac{PR}s are based on older versions of Ghidra wich does not have the gradle wrapper, so for building those versions we have to do the same thing we have done with Java, but for Gradle, 
we have to install more than one version of Gradle and switch between them based on the requirements of the Ghidra version we are building; This only if \texttt{gradlew} is not available since is more preferible running that instead.
This is the main reason for using an Ubuntu image for the container, since it allows us to easily manage multiple versions of Java and Gradle using the package manager and environment variables.

After building a specific version of Ghidra (Base or \ac{PR}), for every binary found in the dataset folder, we check if it is not already decompiled by that specific version of Ghidra (i.e., if the corresponding \ac{JSON} file with the decompilation output does not exist), 
after creating the list of the files not yet decompiled, we start the decompilation process. This incremental strategy prevents re-running expensive steps and makes the workflow resumable.

\subsection{Ghidra Headless}

For decompilation we use the headless mode of Ghidra, which allows us to run Ghidra in a command-line environment without the need for a \ac{GUI}~\cite{site:githubGhidraGhidraRuntimeScriptsCommonsupportanalyzeHeadlessREADMEmdMaster}.
This is particularly useful for automating the decompilation process and integrating it into our evaluation workflow.
The headless mode is invoked via a command-line script; 
the entry point is \texttt{support/pyghidraRun} (preferred, when available) executed in headless mode (\texttt{--headless}), creating a temporary per-binary Ghidra project, importing the binary, and finally running a post-script (\texttt{extract.py}) that performs the actual decompilation and exports the results to \ac{JSON}.
We use parallel execution to speed up the decompilation of multiple binaries; To avoid race conditions and project-file locks during parallel execution, we create an isolated project directory for each binary and delete it at the end of the run.

The headless invocation follows this template:

\begin{verbatim}
$GHIDRA_HOME/support/pyghidraRun --headless <proj_dir> <proj_name> \
    -deleteProject \
    -import <binary_path> -overwrite \
    -scriptPath <scripts_dir> \
    -postScript extract.py
\end{verbatim}

For older Ghidra versions where \texttt{pyghidraRun} is not present, we fall back to the standard headless launcher \texttt{support/analyzeHeadless} with the same arguments.
The client passes configuration to the post-script via environment variables: the output directory, the evaluated Ghidra version tag, and the comma-separated list of target function names to decompile 
(if not specified, all functions are decompiled, in our case we pass only the function name present in the database for that binary because in the compilation process some other functions are added in 
the binary for requirements). This allows the same \texttt{extract.py} script to be reused across runs and versions without hardcoding paths or dataset-specific information.



\subsection{Evaluation}

Not every decompilation difference is relevant for our study: superficial variations (e.g., whitespace, renaming, minor formatting) would introduce noise. For this reason, before invoking the \ac{LLM}, the client performs a structural comparison between the baseline and \ac{PR} outputs:
\begin{itemize}
    \item it loads the \ac{JSON} produced by the decompiler for both \texttt{base} and \texttt{pr\_\#ID},
    \item it compares the abstracted representation (via our \texttt{tree-sitter}-based abstraction, see~\Cref{sec:anonim}) of the two outputs,
    \item it retains only binaries for which the abstracted forms differ, i.e., where control-flow or structure plausibly changed.
\end{itemize}
For each retained binary we compute a complexity proxy (\emph{cyclomatic complexity}) based on the control-flow graph of the decompilation, which is defined as:

\[
M = E - N + 2P,
\]
where:
\begin{itemize}
    \item $E$ is the number of edges in the control-flow graph,
    \item $N$ is the number of nodes in the control-flow graph,
    \item $P$ is the number of connected components (one function is considered a single connected component, since we decompile at function level this is always 1 in our case).
\end{itemize}

The cyclomatic complexity quantifies code complexity based on decision points (e.g., if, while, case), where higher scores indicate harder-to-test, maintain, and error-prone code~\cite{site:sonarsourceCyclomaticComplexity}.
Then for sampling reasons, we sort candidates by decreasing complexity and keep only the top \texttt{MAX\_SAMPLES}. This strategy ensures that our evaluation focuses on the most challenging and relevant cases, where improvements in decompilation quality are most impactful.
\subsubsection{LLM-based scoring}
For each selected binary and for each model exposed by the server, we score:
\begin{itemize}
    \item the baseline decompilation,
    \item the \ac{PR} decompilation,
    \item the original source function (dataset ground truth),
\end{itemize}
using the server \texttt{/score} endpoint, which returns token-level negative log-likelihood aggregated into a \emph{perplexity} value. 
In addition, we compute the perplexity of the abstracted \ac{AST} forms (base/\ac{PR}/source), enabling an evaluation that is less sensitive to naming and formatting. 
To reduce redundant server calls across repeated evaluations, the client maintains a cache keyed by \texttt{(func\_name, model\_id, test\_binary\_name, ppl\_type)} for perplexity of base, source and their abstracted forms.

\subsubsection{Comparison metric}
For each sample we derive a simple \emph{relative} metric:
\[
\Delta\mathrm{PPL} = \mathrm{PPL}_{PR} - \mathrm{PPL}_{base},
\]
(and analogously for the abstracted representation). A negative $\Delta\mathrm{PPL}$ indicates that the \ac{PR} output is more \emph{likely} under the model than the baseline output, 
which we interpret as a proxy for improved fluency/regularity. Conversely, a positive $\Delta\mathrm{PPL}$ suggests a regression in perceived readability.

\subsection{Abstraction and Anonymization}\label{sec:anonim}
To evaluate the structural quality of the decompilation independently of variable naming and formatting, we implemented an abstraction mechanism using \texttt{tree-sitter} a 
Parser used by \texttt{ATOM}~\cite{site:wikipediaAtomtext}, specifically the language for C with \texttt{tree-sitter-c}.
The Python client parses the decompiled C code into an \ac{AST} and traverses it to generate a ``skeletal'' representation of the code.

In this representation, specific identifiers, literals, and types are replaced with generic placeholders (e.g., \texttt{id}, \texttt{num}, \texttt{type}), while control flow keywords (\texttt{if}, \texttt{while}, \texttt{for}, \texttt{switch}, \texttt{goto}) and block structures are preserved.
This process effectively anonymizes the code and create a filter/standard for identation and formatting, cleaning out the possible noise and forcing the \ac{LLM} to focus purely on the control flow logic and structural complexity (e.g., the presence of ``goto'' statements vs\. structured loops)
rather than being biased by variable names, comments or formatting.

We can see from this example how the original code (left) is transformed into the abstracted version (right), where all identifiers and literals are replaced with placeholders, while the control flow and structure are maintained.

\begin{longtable}{|p{0.50\linewidth}|p{0.50\linewidth}|}
    \hline
    \textbf{Source Code} & \textbf{Abstracted Code} \\ 
    \hline
    \endhead
    \begin{minipage}[t]{\linewidth}
       
        \begin{lstlisting}[style=CStyle]
//This function does random stuff dont try to understand it
void complex(int a, char *b) {
    long *f;
    int h[10];
    if (a > 0) {
        while (a < 10) {
            printf("Value: %d\n", a);
            a++;
        }  
    } else {
        goto end;//random comment
    }
    h[0] = 42;
    end:    
    char c = b[0];
    f->g(h[i]);
    (*(int *)(p + 4)) = 5;
}   
        \end{lstlisting}
        
    \end{minipage} 
    & 
    \begin{minipage}[t]{\linewidth}

        \begin{lstlisting}[style=CStyle]
type id(type id, type *id){
    type *id;
    type id[10];
    if(id > 0){
        while(id < 10){
        call(str, id);
        id++;
        }
    }else{
        goto lbl;
    }
    id[0] = 42;
    lbl:
    type id = id[0];
    call(id[id]);
    (*(type)(id + 4)) = 5;
}


        \end{lstlisting}
    \end{minipage} 
    \\ \hline
\end{longtable}

This allows us to evaluate the ``humanness'' of the decompilation output based on its logical structure and flow, 
rather than being influenced by specific naming choices or formatting styles that may vary widely across different decompilers and Ghidra versions.


\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/ast_vs_ppl.png}
    \caption{Pearson correlation between the abstracted \ac{AST} perplexity and source code perplexity for the same decompilation output, across all evaluated samples.}\label{fig:ast_ppl_vs}
\end{figure}

This anonimation also allows us to compute a separate perplexity score for the abstracted \ac{AST} representation, which can be compared against the perplexity of the original source code decompilation.
By analyzing the correlation between these two scores, we can gain insights into how well the \ac{LLM} is capturing the structural and logical aspects of the code, as opposed to just surface-level token patterns.

As shown in~\Cref{fig:ast_ppl_vs}, we observed a positive correlation (Pearson's $r > 0.7$, $p < 0.001$) between the perplexity of the fully abstracted \ac{AST} and that of the original source code across all evaluated models.
In other words, samples that were rated as more ``surprising'' or ``incoherent'' in their original form also tended to have higher perplexity in their abstracted \ac{AST} representation, and vice versa. 

\subsection{Prompting}

One key element of our evaluation framework is the design of the prompts used to query the \ac{LLM} for scoring the decompilation outputs.
We divide our analisys in two main types of prompts:
\begin{itemize}
    \item \textbf{Human evaluation prompt}: This type of prompt is designed to elicit a qualitative assessment from the \ac{LLM} about which version of the decompilation output is better in terms of human readability.
    \item \textbf{Closer to source prompt}: This type of prompt is designed to elicit a qualitative assessment from the \ac{LLM} about which version of the decompilation output is closer to the original source code.
\end{itemize}

for each type of prompt we have two version, the one with the code in the original form and the one with the code in the abstracted form, this is to check if the model prefers one version over the other and if the abstraction process is effective in making the model focus on the structural aspects of the code rather than being influenced by specific naming choices or formatting styles.

\subsubsection{Biases and prompt design}

We had to design the prompts carefully to avoid introducing bias and help the model to focus on the relevant aspects of the decompilation outputs.
For avoid ``update version'' bias, we used a template that does not explicitly mention the concept of ``base'' and ``\ac{PR}'', but rather presents the two decompilation outputs as ``Version A'' and ``Version B'', and then asks the model to compare them based on specific criteria.
To help the model from possible hallucinations it is important to make it generate the motivation for the choice \emph{before} revealing the correct answer, so we ask the model to first provide an explanation of which version it thinks is better and why, since it will use the token generated to fill his context window.

For avoid ``position bias'' we perform a consistency check by swapping the inputs (the one referred to as Version A and the one as Version B) and re-querying the model to see if it still prefers the same version, which can help us to identify and mitigate cases where the model's preference is influenced by the position of the input rather than its content. If the model's choice changes after swapping, it suggests that the original preference may have been due to position bias rather than a genuine assessment of the decompilation quality, so is flagged as \emph{TIE}.
To optimize the context window for the input, we designed the prompt to put version A and version B in a Diff format, where we include all the code of the first version but only the lines that differ in the second version (in the same position using the diff standard with ++ and --), this way we can reduce the amount of tokens in the input and make it easier for the model to focus on the relevant differences between the two versions.

\section{Pull requests}

Ghidra's development is organized around \acp{PR} on GitHub, where contributors propose changes to the codebase that can include bug fixes, new features, or improvements to existing functionality. Each \ac{PR} represents a specific set of changes that can affect the decompilation output in different ways.
Fortunatly in the GitHub repository the \acp{PR} have tags (in our case label:``Feature: Decompiler''), since a lot of them does not modify decompiler logic, so we can easily select only those that are relevant for our evaluation, and that can provide meaningful insights into the impact of specific changes on decompilation quality.

For our evaluation, we select a subset of PRs:
\begin{itemize}
\item \textbf{\ac{PR}~\#8628}: improves the cleanup rule \texttt{Rule2Comp2Sub} so that it also handles \emph{constant} subtractions that are rewritten during decompilation as additions with negative immediates (e.g., \texttt{x + -0x1a} $\rightarrow$ \texttt{x - 0x1a}), improving readability and bringing the output closer to typical C source style.
\item \textbf{\ac{PR}~\#8587}: extends the \texttt{constantptr} rule to automatically detect and correct \emph{one-based} indexing patterns for spacebase constants (e.g., *(undefined *) ((long)i * 0x30 + 0xaddr + (long)j * 4) $\rightarrow$ globalArray[(long)i + -1].field[j]). 
\item \textbf{\ac{PR}~\#8161}: fixes an issue in \texttt{BlockWhileDo} where the decision to use overflow syntax could be made using unoptimized pcode and then treated as permanent: after pcode optimization the exit-condition block may no longer be ``complex,'' but the stale overflow flag would still block for-loop recovery; the patch re-checks that overflow-marked \texttt{BlockWhileDo} blocks are still complex at the time of for-loop recovery and clears the overflow decision when it is no longer justified.
\item \textbf{\ac{PR}~\#7253}: sorts \texttt{switch} \texttt{case} entries by their target address (i.e., the destination basic-block address) rather than leaving them in an arbitrary/disassembler-dependent order.
\item \textbf{\ac{PR}~\#6722}: fixes cases where the decompiler fails to recover submember array access due to \texttt{RulePtrArith} distributing an addition through an \texttt{INT\_MULT} (e.g., \texttt{(idx + -0x30) * 4 + 4} $\rightarrow$ \texttt{idx * 4 + (-0x30 * 4 + 4)}), which prevents matching the struct member; the rule now undoes this distribution and retries, allowing clean output such as \texttt{PTR\_0041a1b8->ar[local\_18]} instead of raw pointer casts and offsets.
\end{itemize}

Ghidra is a large and complex codebase, \acp{PR} are modifications that can affect some specific aspect of the decompilation output, it will never be the case that a \ac{PR} will improve all the decompilations or change the decompilation core aspects in a drastic way, but rather it will improve some specific cases and make them more readable, while in other cases it can make them worse, so we want to evaluate the impact of these specific changes on the decompilation quality and readability, and see if the \ac{LLM}-based evaluation can capture these improvements or regressions in a meaningful way.

\section{Reporting}
For each \ac{PR} the client writes a dedicated report file (\texttt{reports/\#PR.json}) containing aggregate statistics such as mean $\Delta\mathrm{PPL}$ across samples, mean baseline/\ac{PR}/source perplexities, and their abstracted counterparts, and then a list of results for each model:
\begin{inparaenum}[\it (i)]
    \item the list of evaluated samples with their individual perplexities and $\Delta\mathrm{PPL}$ values, and
    \item results of qualitative analysis from the \ac{LLM}.
\end{inparaenum}
At the end of the full run, a \texttt{final\_report.json} aggregates all per-\ac{PR} summaries. If a report already exists, the client loads it and skips re-evaluation, enabling robust recovery after interruptions.

Finally, to avoid leaving a large model resident in \ac{GPU} memory, the client calls the server cleanup endpoint at the end of the run, explicitly unloading the active model.


\section{Dogbolt}
Our analysis is focused on evaluating the impact of specific \acp{PR} on decompilation readability and humanness for Ghidra, but it is also interesting to see how the decompilation quality has evolved over time across different decompilers, and to have a more general overview of the trends and patterns in the decompilation output horizzontally (not vertical on only Ghidra).
For this reason, we also created a framework of three different decompilers (Ghidra 12.0.1, Binary Ninja 5.2, and Hex-Rays 9.2) using \myhref{https://dogbolt.org/}{Dogbolt}, an interactive online \emph{Decompiler Explorer} that displays outputs from multiple decompilers for the same input binary, as can be seen in \Cref{fig:dogbolt}. 
Dogbolt is community-maintained and open source but intentionally does not provide a public automation API, we used it in a restricted version of our Dataset (only the project ``file''), while all large-scale and reproducible measurements in this thesis were executed in our own local pipeline.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img/dogbolt.png}
    \caption{Example of the Dogbolt interface, where we can see the decompilation outputs from three different decompilers}\label{fig:dogbolt}
\end{figure}

Other differences with our main evaluation framework are the fact that the outputs of decompilers are not directly comparable using Diff since they generate different code, names, and formatting, so we rely, in this case, on giving all the code to the \ac{LLM}.
For the evaluation we use the same prompting strategy described in~\Cref{sec:configLLM}, but we used a pipeline when for every function we create three different prompts, one for each pair of decompilers (Binary Ninja vs Ghidra, Ghidra vs Hex-Rays, Binary Ninja vs Hex-Rays), and then we aggregate the results to have a more general overview of the trends and patterns in the decompilation output across different decompilers.
We also had to parse the results for extracting the functions from the dogbolt results since this is external to the Dataset maker framework in \Cref{sec:datmak}