\chapter{Methodology}\label{ch:method}

The framework of our work is based on a client-server architecture, where the server hosts the \ac{LLM} and provides an \ac{API} for interacting with it, 
while the client is responsible for building specific Ghidra version, preparing the code samples and prompts, invoking the server, and collecting results in \ac{JSON} format.
Every service is designed to be modular, allowing for the integration of different \ac{LLM} models and evaluation metrics, for
reproducibility we used \emph{Docker Compose} to containerize both the server and client components, ensuring consistent environments across different machines and operating systems.
The dataset creation is also a containerized process, and the result are mounted as volumes to the client container, allowing for easy access and manipulation of the data without the need for complex data transfer mechanisms.

\section{Dataset Maker}
As written on related works~\ref{sec:llmbench}, we decided to use a subsect of the complete OSS Fuzz dataset used by DecompileBench~\cite{Paper:gao2025decompilebenchcomprehensivebenchmarkevaluating} for our evaluation,
specifically the four Open Source projects: \texttt{file, libxls, readstat, xz}, which are written in C and have a rich history of commits and pull requests on GitHub.
These choice was motivated by the need to have a manageable dataset size for local evaluation, while still covering a set of real world code and functions to evaluate our approach.
for every project the dataset maker extracts all the functions and recompiles them  into standalone binaries; this process create different optimization levels of the binary, specifically \texttt{-O0} and \texttt{-O2}, and \texttt{-O3} 
which are the most common optimization levels used in real world scenarios, and which can have a significant impact on the decompilation output and its readability.

For obtaining this we had to fork the original dataset maker script and modify it to fit our edits; such as the specifically optimization levels (in the original repo they were using all the optimization levels) and some bug fix as pointed out by one pull request on the original repo~\cite{site:githubVarietyProblems}.
so we clone our fork into a container (wich will also run docker inside for building the projects), edited with the patches as shown in the \texttt{README} file of DecompileBench, selected just our four projects and then run the dataset maker script.

\subsection{Dataset Collection}
The result of the dataset maker is a folder named \texttt{Dataset} wich contains other three subfolders:
\begin{itemize}
    \item \textbf{binary}: contains the compiled binary of the functions, every file is named with the format \texttt{task\_project\_functionName-OX.so}, and can be used for decompilation and evaluation.
    \item \textbf{compiled\_ds}: contains a file structure of the dataset format used by the \texttt{Datasets} library~\cite{site:pypiDatasets}, which is a Python library for handling large datasets in a efficient way, and which we use for loading the dataset in our client code. 
        This structure have a file `.arrow' that store data and two \ac{JSON} files for the metadata such as field names and types. In our case we are interested only in three fields \texttt{file}, which contain the name of the function, \texttt{path} wich contains the path `binary/namefile', and \texttt{func} wich contains the source code of the function.
    \item \textbf{eval}: contains also a dataset structure, but we will not use it since is used for recompile success and other metrics that we are not interested in, since we want to focus on the evaluation of the decompilation output rather than the compilation process.
\end{itemize}

\section{LLM Server}

The server is responsible for hosting the \ac{LLM} and providing an \ac{API} for interacting with it, specifically for receiving code samples and prompts from the client, processing them with the \ac{LLM}, and returning the results.
The server is designed to be modular, allowing for the integration of different \ac{LLM} models and evaluation metrics, and it is containerized using Docker for reproducibility and ease of deployment.

It uses Gnunicorn as the WSGI HTTP server for handling incoming requests, and it is built on top of a Python web framework (Flask) to define the \ac{API} endpoints and handle the logic for processing requests and interacting with the \ac{LLM}.
The server is configured with a single worker and thread to manage sequential requests with an extended timeout of 800 seconds to accommodate longer inference times (but more importantly the model loading and the context switch when changing models).
On startup, it performs \ac{GPU} availability checks and pre-downloads all required model weights to the container's cache directory using the Hugging Face libraries~\ref{sec:configLLM}.

\subsection{Models}
The heavy part of the framework is without doubt the server, and the models that runs on it.
In our case the local enviroment is a single \ac{GPU} machine with 16 GB of \ac{VRAM}, so we had to select models that can run on this hardware, and that can provide a good performance for our evaluation.
We also used the \myhref{https://apxml.com/tools/VRAM-calculator}{\ac{VRAM} Calculator} to estimate the memory requirements of different models and ensure they fit within our hardware constraints, inside the \ac{VRAM} have to cohesist different areas, such as:
\begin{itemize}
    \item \textbf{Base Model Weights}: The trained parameters of the model, the `weights' with their precision (could be quantizated for reduced memory usage).
    \item \textbf{Activations}: Intermediate computation results during forward passes through the layers. This grows with batch size and input length, and is critical for stability during inference.
    \item \textbf{KV Cache}: Key-Value cache used to avoid recomputing attention for previously processed tokens. Given the lengthy decompilation prompts containing source code, this cache grows proportionally with input length.
    \item \textbf{Framework Overhead}: Fixed memory cost from PyTorch, CUDA drivers, and buffer management. This overhead exists regardless of model size.
\end{itemize}

Based on these considerations, we selected the following models for our evaluation:
\begin{itemize}

\item \textbf{Meta Llama 3.1 (8B)}: 
A general-purpose instruction-tuned model from the Llama family, used as a strong baseline for multilingual dialogue and reasoning. We selected the \emph{8B} variant because it fits our single 16\,GB \ac{VRAM} setup with 4-bit quantization, while still providing competitive performance on code-related prompts. The model supports long-context inference (up to 128K tokens), which is particularly useful in our setting where prompts may include lengthy decompilations, structured evaluation instructions, and reference code. From an architectural standpoint, Llama~3.1 is an auto-regressive Transformer with \ac{GQA}, enabling more efficient inference at larger context lengths.~\cite{site:huggingfaceMetallamaLlama318BInstructHugging}

\item \textbf{Qwen2.5-Coder-Instruct (7B)}: 
Is a code-specialized instructed family of Qwen models, spanning multiple sizes to address different deployment constraints. In our experiments we use the \emph{7B} variant, since it fits comfortably on our \ac{VRAM} setup with 4-bit quantization while remaining competitive for program understanding tasks. Compared to earlier CodeQwen releases, Qwen2.5-Coder is trained on a substantially larger mixture of code and text-code data, improving code generation, code reasoning, and bug-fixing capabilities. It also supports long-context inference (up to 128K tokens).~\cite{Paper:hui2024qwen2}

\item \textbf{DeepSeek-R1-Distill-Qwen (7B)}: 
A distilled, reasoning-focused model derived from DeepSeek-R1 by fine-tuning a Qwen2.5-based 7B dense checkpoint on reasoning trajectories generated by the larger R1 model. This distillation transfers behaviors such as multi-step deliberation and self-verification into a smaller model that remains practical to run locally (our setup). We include it to evaluate whether a reasoning-specialized LLM provides more consistent and accurate assessments on code-understanding tasks compared to general instruction-tuned baselines.~\cite{Paper:deepseekai2025deepseekr1incentivizingreasoningcapability}

\item \textbf{Google Gemma 2 (9B)}: 
An instruction-tuned, decoder-only model from Googleâ€™s Gemma family of lightweight open-weight \acp{LLM}, derived from the same research and technology used for Gemini. We use the \emph{9B} variant, which offers a strong general-purpose baseline for text-to-text generation and reasoning while remaining feasible on our setup. Although primarily English-focused, it is trained on a diverse mixture of web documents, code, and mathematics, making it suitable for code understanding and structured evaluation prompts.~\cite{Paper:gemma_2024}
\end{itemize}

These models were selected to provide a diverse range of architectures, training data, and specialization levels, allowing us to evaluate the effectiveness of \ac{LLM}-based evaluation across different model types and capabilities.

\subsection{Configuration}\label{sec:configLLM}
The server supports multiple local \acp{LLM} through a simple configuration layer that maps a short, client-facing identifier to the corresponding Hugging Face repository ID. Concretely, a dictionary (\texttt{MODELS\_CONFIG}) defines the available models (e.g., \texttt{qwen-coder}, \texttt{deepseek-r1}, \texttt{llama3.1}, \texttt{gemma2}) and is the single source of truth for both the \texttt{/models} endpoint and for request-time model switching.

For ensure lightness, all models are loaded using 4-bit quantization via \texttt{bitsandbytes}.

\begin{verbatim}
bnb_config = BitsAndBytesConfig(
  load_in_4bit=True,
  bnb_4bit_quant_type="nf4",
  bnb_4bit_use_double_quant=True,
  bnb_4bit_compute_dtype=torch.bfloat16
)
\end{verbatim}

To reduce cold-start delays, the container optionally pre-downloads all model snapshots at startup using \texttt{snapshot\_download}, ensuring that evaluation runs are not affected by network variability. 
Finally, since only one model can reside in GPU memory at a time, the server unloads the currently active model (explicit \texttt{del} + garbage collection + CUDA cache cleanup) 
before loading a new one.

\subsection{Decoding strategy (temperature and top-p)}
For the \texttt{/generate} endpoint, we configured the decoding parameters through a dedicated function that returns a dictionary of \texttt{transformers} generation arguments. 
In our experiments we rely on \emph{nucleus sampling} (\texttt{top\_p}) with a low \texttt{temperature}, to balance determinism (useful for fair comparisons across decompilers) 
and the ability to escape repetitive or low-quality completions.

Concretely, the default configuration is:
\begin{itemize}
    \item \texttt{temperature=0.4}: reduces randomness by sharpening the token distribution. Lower values make outputs more stable across runs, which is desirable for evaluation.
    \item \texttt{top\_p=0.9}: nucleus sampling, i.e., tokens are sampled only from the smallest set whose cumulative probability mass is $p$. This prevents the model from selecting very unlikely tokens while still allowing variation.
    \item \texttt{max\_new\_tokens=2048}: upper bound on completion length, used as a safety and latency-control measure.
\end{itemize}

\subsection{Routes}
The server exposes a minimal \ac{REST} \ac{API}, All endpoints exchange \ac{JSON} payloads and are intentionally kept coarse-grained (a lot of work in a single request) to decouple the client implementation from model-specific details. The available routes are:

\begin{itemize}
    \item \textbf{GET /}: health check endpoint. It returns the server readiness status, whether CUDA is available, and the currently loaded model identifier (if any). This is used by docker compose to ensure healthcheck status for required services.
    \item \textbf{GET /models}: returns the list of supported model keys (the abstract identifiers used by the client), mapped server-side to Hugging Face repository IDs.
    \item \textbf{POST /generate}: main inference endpoint. The request body includes \texttt{model\_id} and a \texttt{prompt}. The server loads (or switches to) the requested model, wraps the prompt into a chat-style template via the tokenizer, runs text generation, and returns the generated completion.
    \item \textbf{POST /score}: scoring endpoint used to compute a language-model based score for a given text. The request body includes \texttt{model\_id} and \texttt{text}. The server computes the token-level negative log-likelihood and returns the derived perplexity.
    \item \textbf{POST /free}: explicit cleanup endpoint to unload the currently resident model and aggressively release GPU memory.
\end{itemize}

Since different models cannot fit simultaneously in \ac{GPU} memory, model switching is handled server-side: each request triggers a check on the currently loaded model and, if needed, a full unload/load cycle. To avoid concurrent access to \ac{GPU} state, all inference and scoring operations are protected by a global lock, enforcing sequential execution.

\subsection{Metrics}
To make the evaluation reproducible and to quantify server-side overhead, the server logs per-request performance metrics to a CSV file (\texttt{llm\_metrics.csv}). Each entry includes:

\begin{itemize}
    \item \textbf{Model and operation}: \texttt{model\_id} and \texttt{operation} (\texttt{generate} or \texttt{score}).
    \item \textbf{Latency}: wall-clock duration (seconds) measured around the full operation, including tokenization and \ac{GPU} synchronization.
    \item \textbf{Peak GPU memory}: peak \ac{VRAM} allocated during the operation, obtained via CUDA peak memory statistics.
    \item \textbf{Tokens}: number of prompt/input tokens and number of generated output tokens; these are also used to derive an approximate throughput (tokens per second).
\end{itemize}

Metric collection is implemented via a dedicated monitoring context manager that resets CUDA peak counters before execution and synchronizes the device before reading final statistics. 
This design provides a uniform measurement procedure across both generation and perplexity scoring, and enables later analysis of the impact of model switching, 
prompt length, and decoding configuration on runtime and memory usage.

\section{Client}

The client is responsible for orchestrating the entire evaluation workflow, including building specific Ghidra versions, preparing code samples and prompts, invoking the server for decompilation and scoring, and collecting results in \ac{JSON} format for analysis.
It is designed to be modular and flexible, allowing for easy integration of different evaluation strategies and metrics, and it is containerized using Docker for reproducibility and ease of deployment.

The evaluation stage is an end-to-end pipeline over a set of target Ghidra pull requests. At a high level, the client:
\begin{enumerate}
    \item Ensures that the \emph{base} version is built and has produced decompilations for all dataset binaries
    \item Iterates over the selected \ac{PR}, building each corresponding Ghidra revision and extracting the related decompilations
    \item Selects a limited subset of binaries that actually exhibit decompilation differences, to focus the evaluation on meaningful cases and reduce noise
    \item For each model, queries the \ac{LLM} server to score and compare the outputs, producing per-\ac{PR} and aggregate \ac{JSON} reports for later analysis.
\end{enumerate}

\subsection{Building Ghidra}
The build process is automated via Python scripts that interact with Git and Gradle (we use an ubuntu image for the container).
Firstly we clone and build the Ghidra repository from GitHub, this version is used as the base for all our evaluations.
after building base and extracted the functions from binary, we get the \ac{PR}s number that we want to evaluate against base from a function that calls github \ac{API} and returns the list of all \ac{PR}s of Ghidra.
Then for each \ac{PR} we have to checkout the specific version of Ghidra, for doing this we have a script that takes as input the \ac{PR} number, and then it fetches the specific head reference from the GitHub repository (\texttt{pull/ID/head:pr-ID}) and checks it out.

For building Ghidra are necessary two prerequisites: \texttt{Java 11} and \texttt{Gradle} (optionally), the first one is required for running the build scripts and the second one is used for managing dependencies and building the project, 
but since Ghidra in newer versions includes a wrapper for Gradle (\texttt{gradlew}), you can use it without installing Gradle globally.

One problem is that every version of Ghidra need a specific version of Java, so we have to check the \texttt{application.properties} file inside the repo for the required minimal Java version, and then install it in the container before building Ghidra.
So inside the container we manage more than one version of Java, and we switch between them based on the requirements of the Ghidra version we are building.
Another problem is that some \ac{PR}s are based on older versions of Ghidra wich does not have the gradle wrapper, so for building those versions we have to do the same thing we have done with Java, but for Gradle, 
we have to install more than one version of Gradle and switch between them based on the requirements of the Ghidra version we are building; This only if \texttt{gradlew} is not available since is more preferible running it instead.
This is the main reason for using an Ubuntu image for the container, since it allows us to easily manage multiple versions of Java and Gradle using the package manager and environment variables.

After building a specific version of Ghidra (Base or \ac{PR}), for every binary found in the dataset folder, we check if it is not already decompiled by that specific version of Ghidra (i.e., if the corresponding \ac{JSON} file with the decompilation output does not exist), 
after creating the list of the files not yet decompiled, we start the decompilation process. This incremental strategy prevents re-running expensive steps and makes the workflow resumable.

\subsection{Ghidra Headless}

For decompilation we use the headless mode of Ghidra, which allows us to run Ghidra in a command-line environment without the need for a graphical user interface~\cite{site:githubGhidraGhidraRuntimeScriptsCommonsupportanalyzeHeadlessREADMEmdMaster}.
This is particularly useful for automating the decompilation process and integrating it into our evaluation workflow.
The headless mode is invoked via a command-line script; 
the entry point is \texttt{support/pyghidraRun} (preferred, when available) executed in headless mode, creating a temporary per-binary Ghidra project, importing the binary, and finally running a post-script (\texttt{extract.py}) that performs the actual decompilation and exports the results to \ac{JSON}.
We use parallel execution to speed up the decompilation of multiple binaries; To avoid race conditions and project-file locks during parallel execution, we create an isolated project directory for each binary and delete it at the end of the run.

The headless invocation follows this template:

\begin{verbatim}
$GHIDRA_HOME/support/pyghidraRun --headless <proj_dir> <proj_name> \
    -deleteProject \
    -import <binary_path> -overwrite \
    -scriptPath <scripts_dir> \
    -postScript extract.py
\end{verbatim}

For older Ghidra versions where \texttt{pyghidraRun} is not present, we fall back to the standard headless launcher \texttt{support/analyzeHeadless} with the same arguments.
The client passes configuration to the post-script via environment variables: the output directory, the evaluated Ghidra version tag, and the comma-separated list of target function names to decompile 
(if not specified, all functions are decompiled, in our case we pass only the function name present in the database for that binary because in the compilation process some other functions are added in 
the binary for requirements). This allows the same \texttt{extract.py} script to be reused across runs and versions without hardcoding paths or dataset-specific information.



\subsection{Evaluation}

Not every decompilation difference is relevant for our study: superficial variations (e.g., whitespace, renaming, minor formatting) would introduce noise. For this reason, before invoking the \ac{LLM}, the client performs a structural comparison between the baseline and \ac{PR} outputs:
\begin{itemize}
    \item it loads the \ac{JSON} produced by the decompiler for both \texttt{base} and \texttt{pr\_\#ID},
    \item it compares the abstracted representation (via our \texttt{tree-sitter}-based abstraction, see Section~\ref{sec:anonim}) of the two outputs,
    \item it retains only binaries for which the abstracted forms differ, i.e., where control-flow or structure plausibly changed.
\end{itemize}
For each retained binary we compute a complexity proxy (\emph{cyclomatic complexity}\footnote{Cyclomatic complexity is a software metric developed by Thomas McCabe in 1976 that measures the number of linearly independent paths through a program's source code. It quantifies code complexity based on decision points (e.g., if, while, case), where higher scores indicate harder-to-test, maintain, and error-prone code~\cite{site:sonarsourceCyclomaticComplexity}}) 
and sort candidates by decreasing complexity; we then keep only the top \texttt{MAX\_SAMPLES}. This emphasizes samples where changes are more likely to be semantically meaningful and where readability improvements matter most.

\subsubsection{LLM-based scoring}
For each selected binary and for each model exposed by the server, we score:
\begin{itemize}
    \item the baseline decompilation,
    \item the \ac{PR} decompilation,
    \item the original source function (dataset ground truth),
\end{itemize}
using the server \texttt{/score} endpoint, which returns token-level negative log-likelihood aggregated into a perplexity value. 
In addition, we compute the perplexity of the abstracted \ac{AST} forms (base/\ac{PR}/source), enabling an evaluation that is less sensitive to naming and formatting. 
To reduce redundant server calls across repeated evaluations, the client maintains a cache keyed by \texttt{(func\_name, model\_id, test\_binary\_name, ppl\_type)} for perplexity of base, source and their abstracted forms.

\subsubsection{Comparison metric}
For each sample we derive a simple \emph{relative} metric:
\[
\Delta\mathrm{PPL} = \mathrm{PPL}_{PR} - \mathrm{PPL}_{base},
\]
(and analogously for the abstracted representation). A negative $\Delta\mathrm{PPL}$ indicates that the \ac{PR} output is more \emph{likely} under the model than the baseline output, 
which we interpret as a proxy for improved fluency/regularity. Conversely, a positive $\Delta\mathrm{PPL}$ suggests a regression in perceived readability.

\subsection{Abstraction and Anonymization}\label{sec:anonim}
To evaluate the structural quality of the decompilation independently of variable naming and formatting, we implemented an abstraction mechanism using \texttt{tree-sitter} a 
Parser used by \texttt{ATOM}~\cite{site:wikipediaAtomtext}, specifically the language for C with \texttt{tree-sitter-c}.
The Python client parses the decompiled C code into an \ac{AST} and traverses it to generate a `skeletal' representation of the code.

In this representation, specific identifiers, literals, and types are replaced with generic placeholders (e.g., \texttt{id}, \texttt{num}, \texttt{type}), while control flow keywords (\texttt{if}, \texttt{while}, \texttt{for}, \texttt{switch}, \texttt{goto}) and block structures are preserved.
This process effectively anonymizes the code and create a filter/standard for identation and formatting, cleaning out the possible noise and forcing the \ac{LLM} to focus purely on the control flow logic and structural complexity (e.g., the presence of `goto' statements vs\. structured loops)
rather than being biased by variable names, comments or formatting.

We can see from this example how the original code (left) is transformed into the abstracted version (right), where all identifiers and literals are replaced with placeholders, while the control flow and structure are maintained.

\begin{longtable}{|p{0.50\linewidth}|p{0.50\linewidth}|}
    \hline
    \textbf{Source Code} & \textbf{Abstracted Code} \\ 
    \hline
    \endhead
    \begin{minipage}[t]{\linewidth}
       
        \begin{lstlisting}[style=CStyle]
//This function does random stuff dont try to understand it
void complex(int a, char *b) {
    long *f;
    int h[10];
    if (a > 0) {
        while (a < 10) {
            printf("Value: %d\n", a);
            a++;
        }  
    } else {
        goto end;//random comment
    }
    h[0] = 42;
    end:    
    char c = b[0];
    f->g(h[i]);
    (*(int *)(p + 4)) = 5;
}   
        \end{lstlisting}
        
    \end{minipage} 
    & 
    \begin{minipage}[t]{\linewidth}

        \begin{lstlisting}[style=CStyle]
type id(typeid, type*id){
    type *id;
    type id[num];
    if(id > num){
        while(id < num){
            call(str, id);
            id upd;
        }
    }else{
        goto lbl;
    }
    id[num] = num;
    lbl:
    type id = id[num];
    call(id[id]);
    (*(type)(id op num)) = num;
}

        \end{lstlisting}
    \end{minipage} 
    \\ \hline
\end{longtable}

This allows us to evaluate the `humanness' of the decompilation output based on its logical structure and flow, 
rather than being influenced by specific naming choices or formatting styles that may vary widely across different decompilers and Ghidra versions.

\begin{figure}[H]\label{fig:ast_ppl_vs}
    \centering
    \includegraphics[width=1\textwidth]{img/ast_vs_ppl.png}
    \caption{Pearson correlation between the abstracted \ac{AST} perplexity and source code perplexity for the same decompilation output, across all evaluated samples.}
\end{figure}

This anonimation also allows us to compute a separate perplexity score for the abstracted \ac{AST} representation, which can be compared against the perplexity of the original source code decompilation.
By analyzing the correlation between these two scores, we can gain insights into how well the \ac{LLM} is capturing the structural and logical aspects of the code, as opposed to just surface-level token patterns.

As shown in the figure~\ref{fig:ast_ppl_vs}, we observed a positive correlation (Pearson's $r > 0.6$, $p < 0.001$) between the perplexity of the fully abstracted \ac{AST} and that of the original source code across all evaluated models.
In other words, samples that were rated as more `surprising' or `incoherent' in their original form also tended to have higher perplexity in their abstracted \ac{AST} representation, and vice versa. 
In most of the cases the \ac{AST} perplexity was higher than the source code perplexity, which is expected since the abstraction process removes a lot of the surface-level information and leaves only the structural aspects of the code, which are more challenging for the \ac{LLM} to predict accurately.

\subsection{Prompting}

\dots

\section{Pull requests}

\dots

\section{Reporting}
For each \ac{PR} the client writes a dedicated report file (\texttt{reports/\#PR.json}) containing aggregate statistics such as mean $\Delta\mathrm{PPL}$ across samples, mean baseline/\ac{PR}/source perplexities, and their abstracted counterparts, and then a list of results for each model:
\begin{inparaenum}[\it (i)]
    \item the list of evaluated samples with their individual perplexities and $\Delta\mathrm{PPL}$ values, and
    \item results of qualitative analysis from the \ac{LLM}
\end{inparaenum}
At the end of the full run, a \texttt{final\_report.json} aggregates all per-\ac{PR} summaries. If a report already exists, the client loads it and skips re-evaluation, enabling robust recovery after interruptions.

Finally, to avoid leaving a large model resident in \ac{GPU} memory, the client calls the server cleanup endpoint at the end of the run, explicitly unloading the active model.


%\section{Dogbolt}