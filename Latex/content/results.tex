\chapter{Results}\label{ch:results}



\section{LLM performance}

An empirical evaluation of the \emph{qwen-3} and \emph{deepseek-r1} models was conducted across two distinct tasks: evaluating the ``Humanity'' of decompiled code via generation (\ac{LLM} Judge) and calculating code perplexity (Score). The performance was measured in terms of execution time and peak \ac{VRAM} usage.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/llm_time.png}
    \caption{Execution time for each evaluated model.}\label{fig:llm_time}
\end{figure}

As shown in \Cref{fig:llm_time}, there is a stark dichotomy in execution times between the two operations. The \texttt{score} operation is inherently fast, with both models completing most passes in under 50 seconds. However, the \texttt{generate} operation exhibits significantly higher and more erratic execution times, particularly for \emph{qwen-3}, which frequently exceeds 200 seconds and reaches up to nearly 400 seconds. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/llm_vram.png}
    \caption{Peak VRAM for each evaluated model.}\label{fig:llm_vram}
\end{figure}

\Cref{fig:llm_vram} presents a counter-intuitive finding: calculating perplexity (\texttt{score}) requires a higher median and maximum Peak VRAM than generating text (\emph{generate}). While generation heavily utilizes the KV cache over time, perplexity calculations typically require processing the entire sequence in a single forward pass to compute logits, leading to massive activation memory spikes~\cite{atmer2025prefillvsdecodebottlenecks}.

\subsection{Generation}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/llm_gen.png}
    \caption{Generated tokens vs execution time and peak VRAM for each evaluated model.}\label{fig:llm_gen}
\end{figure}

A closer inspection of \Cref{fig:llm_gen} reveals the root cause of the discrepancy between the two models. The scatter plot for ``Generated Tokens vs Execution Time'' shows a near-perfect linear correlation, confirming that generation time is bottlenecked entirely by the length of the model's output rather than the input context. 
Critically, \emph{qwen-3} routinely generates between 2,000 and 4,000+ tokens during the evaluation task and often reaches our limit of 4096 new tokens. 

Given that the task is an \ac{LLM} Judge producing an evaluation of code ``humanity,'' a rationale exceeding 4,000 tokens is highly suspicious. This suggests that \emph{qwen-3} may be suffering from severe verbosity, repeating the input code, or failing to trigger stop tokens appropriately. 
In contrast, \emph{deepseek-r1} is significantly more concise, rarely exceeding 2,500 tokens, which translates directly to more predictable and efficient execution times.

\begin{wrapfigure}{r}{0.55\textwidth}
    \centering
    \includegraphics[width=0.45\textwidth]{img/qwen_stuck.png}
    \caption{Example of repetitions in \emph{qwen-3} output}\label{fig:stuck}
\end{wrapfigure}

In \Cref{fig:stuck} we can see an example of the output of \emph{qwen-3} for a specific function, we can see that the model is repeating the same pattern of thoughts, which is a common symptom of verbosity and lack of proper stop token triggering. 
This behavior not only leads to unnecessarily long outputs but also significantly increases execution time and VRAM usage, as the model continues to generate tokens without producing meaningful content.
This verbosity issue is a critical flaw for the \ac{LLM} Judge task, as it undermines the model's ability to provide concise and relevant evaluations of code ``humanity,'' and it also creates significant computational inefficiencies that could be prohibitive in larger-scale evaluations or with longer input contexts, 
maybe could be mitigated by implementing stricter stop conditions or by fine-tuning the model to better understand the task requirements and avoid unnecessary verbosity but it will need further investigation.

\subsection{Score}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/llm_score.png}
    \caption{Score vs execution time and peak VRAM for each evaluated model.}\label{fig:llm_score}
\end{figure}

The linear scaling of VRAM during the \emph{score} operation (\Cref{fig:llm_score}, right panel) is a major scalability risk. At roughly 3,800 input tokens, both models push past 12 GB of \ac{VRAM}. 
If the dataset contains an enormous larger input (e.g., 8k to 32k tokens), the current perplexity scoring methodology will inevitably result in \ac{OOM} failures on standard consumer GPUs.

Fortunately, the \emph{generate} operation does not exhibit this issue, as it processes tokens sequentially and can leverage the KV cache to manage memory more efficiently.

The duration is mostly flat (near 0--10 seconds), but spikes occur at highly specific input token lengths for both models. This implies either batch-processing artifacts or that the evaluation dataset contains many decompiled functions of the exact same token length that trigger specific internal computational bottlenecks.
A possible explanation is that runtime is influenced not only by input length, but also by the distribution of per-token loss values. This hypothesis requires additional targeted experiments, and further studies will be necessary to confirm it.


While both models are capable of performing the required tasks, \emph{deepseek-r1} is objectively better suited for the \ac{LLM} Judge (\emph{generate}) role due to its restraint and conciseness, avoiding the computationally expensive verbosity traps that plague \emph{qwen-3}. 
However, for the perplexity (\emph{score}) task, the architecture of the operation itself poses a severe hardware bottleneck that scales poorly with larger decompiler outputs.

\section{Perplexity as a Metric for ``Humanness''}\label{sec:perplexityres}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/ppl_distribution.png}
    \caption{On the left perplexity values across original source code, base code, and pr code. On the right perplexity values across abstracted representations of the same functions.}\label{fig:ppl_distribution}
\end{figure}

In \Cref{fig:ppl_distribution} we can see the distribution of perplexity values for the original source code, the base code, and the pr code, as well as their abstracted representations.
We can observe that the original source code has generally higher perplexity values compared to the decompiled versions,
which is unexpected since the original source code should be more ``natural'' and predictable than the decompiled output.
This suggests that the decompilation process may introduce certain patterns or structures that are more familiar to the language model, leading to lower perplexity scores, while the original source code may contain more variability and less predictable constructs that result in higher perplexity.
We can observe that the perplexity distribution calculated with \emph{deepseek-r1} is generally higher than the one calculated with \emph{qwen-3}, this is consistent with the previous observation that \emph{qwen-3}.

Another observation is that the abstracted representations of the code (right side of the figure) tend to have higher perplexity values compared to their original counterparts (left side of the figure).
This is likely because the abstraction process removes specific identifiers and literals, which can make the code less predictable and more ``surprising'' to the language model, 
but even in this case, the original source code still has higher perplexity than the decompiled versions, reinforcing the idea that the decompilation process may be introducing more predictable patterns into the code.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.99\textwidth]{img/global_loss.png}
    \caption{Density distribution of cross-entropy loss values for all tokens in the original source code and the decompiled versions.}\label{fig:global_loss}
\end{figure}

In \Cref{fig:global_loss} we can observe that the original code exhibits a wider, slightly flatter distribution with a higher mean loss compared to the decompiled output, which is characterized by a sharper peak shifted towards zero.
This visualization highlights a counter-intuitive phenomenon: despite the original source code being the ``human ground truth'', the language model finds the decompiled code significantly more predictable.

We attribute this behavior to Token Inflation and Loss Dilution:
As indicated by the token counts in the figure (e.g., $\sim$168k tokens for decompiled vs $\sim$120k for original source), the decompilation process introduces a substantial \emph{token inflation}.
Ghidra generate verbose, explicit code full of boilerplate structures (e.g., redundant casts, standard control flow patterns, explicit initializations, and restricted vocabulary). These pattern tokens are syntactically rigid and in a context where are used, they are easy to predict for the model, leading to a large number of tokens with very low loss values (close to zero). 
Their sheer volume effectively dilutes the mean loss, artificially lowering the overall perplexity score compared to the denser, more information-rich human code.
In contrast, the original source code reflects human authorship, which includes domain-specific naming conventions, creative syntactic choices, and stylistic variability. This ``human entropy'' flattens the density curve and shifts the mean loss to the right, as the model is more frequently ``surprised'' by the programmer's unique choices compared to the machine's standardized output.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.99\textwidth]{img/global_loss_ast.png}
    \caption{Density distribution of cross-entropy loss values for all tokens in the original source code and the decompiled versions.}\label{fig:global_loss_ast}
\end{figure}

In \Cref{fig:global_loss_ast} we can see the \ac{AST} version of the previous analysis, where we abstracted away variable names and literals to focus on the structural aspects of the code.
Comparing these results with the previous analysis on raw code tokens, we observe two critical phenomena:

\begin{enumerate}
    \item \textbf{Persistence of structural inflation:} Even in the anonymized form, the \textit{token inflation} remains significant. The decompiled AST contains $\sim$107k tokens compared to $\sim$70k for the original source ($\sim$+52\%). 
        This confirms that the verbosity of the decompiled code is not merely lexical (e.g., long variable names) but syntactical. The decompiler introduces explicit casts, redundant blocks, and verbose control flow structures that persist even after anonymization, continuing to dilute the mean loss with predictable tokens.
    \item \textbf{The closer entropy:}
    Unlike the raw code analysis, where the gap between the distributions was pronounced, the AST distributions for Source and Decompiled code are more similar in shape. The difference in Mean Loss has narrowed (e.g., for \emph{deepseek-r1}, the gap represents only $\sim 0.047$, compared to larger margins in the raw code).

    This convergence suggests that the \emph{lexical entropy} was a discriminator in the previous analysis.
    \begin{itemize}
        \item In the raw code, human-written names provided high variability (surprisal), while decompiled names were generic.
        \item In the AST version, the anonymization process effectively ``standardizes'' the two codes; Consequently, the source code becomes more predictable by the model.
    \end{itemize}
\end{enumerate}


\begin{wrapfigure}{r}{0.55\textwidth}
    \centering
    \includegraphics[width=0.45\textwidth]{img/example_loss_func.png}
    \caption{Loss values for \texttt{xls\_parseWorkBook} in the decompiled base code}\label{fig:example_loss_func}
\end{wrapfigure}

The token-level analisys allow us to verify the tokens that contribute the most to the loss, and consequently to the perplexity, in a specific function.
We can observe that the token itself is not the only factor that contributes to the loss, but especially the context in which it is used.
For example, in \Cref{fig:example_loss_func} we can see the loss values for the token ``\texttt{LAB}'', which is a common label used in the decompiled code to indicate jump targets.
This token has a low loss value (1.664) when it appears after the \texttt{do{}while loop}, but it has a much higher loss value (11.812) when it appears inside the \texttt{if} allowing a flow branch to ignore the condition and entering the scope without checks.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.99\textwidth]{img/example_loss.png}
    \caption{Example of loss values for a function in the decompiled code.}\label{fig:example_loss}
\end{figure}

Obviously every function has a different distribution of loss values, and some functions may be more ``natural'' than others.
In \Cref{fig:example_loss} we can see an example of the loss values for \texttt{xls\_parseWorkBook} function from the DeepSeek analisys, 
remembering that the perplexity is calculated as the exponential of the mean loss~\ref{sec:perplexity}, we can see that the original source code has a perplexity of $\sim 3.06$ (mean loss $\sim 1.12$), while the decompiled base version has a perplexity of $\sim 2.4$ (mean loss $\sim 0.89$).
We can see that the max values for the loss are higher for the decompiled version, in contrary to the global distribution where the decompiled code had a sharper peak towards zero, but then when we look at the anonymized version of the same function, the original source code became the one with a higher max loss value than the decompiled version.
However, the anonymization process manages to bring the two versions significantly closer, both in terms of loss values and, consequently, perplexity.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.99\textwidth]{img/std_dev_loss.png}
    \caption{Standard deviation of loss values across all functions in the normal version and anonymized version.}\label{fig:std_dev_loss}
\end{figure}

In \Cref{fig:std_dev_loss} we investigate the \emph{Entropy Variance} of the code by analyzing the standard deviation of the token loss.
While the mean loss indicates the average predictability, the standard deviation reveals the \emph{dynamic range} of the code complexity.

We can observe a clear trend across both models:
\begin{itemize}
    \item \textbf{Difference in Variance:} The original source code consistently exhibits higher standard deviation compared to the decompiled versions. This confirms that human-written code is characterized by \textit{burstiness}: it alternates between low-entropy boilerplate and high-entropy domain-specific logic. The language model struggles to predict this rhythm, leading to fluctuating loss values.
    The decompiled code shows significantly lower variance. This reflects the \textit{monotonicity} of machine-generated code. In our case Ghidra applies consistent transformation rules throughout the binary, resulting in more predictabile results.
    
    \item \textbf{The Lexical Factor:} Comparing the code panel with the \ac{AST} one, we notice that the gap between Source and Decompiled shrinks significantly in the AST representation. This implies that a substantial portion of the entropy variance in human code is driven by \emph{lexical choices} (variable naming and literals) as we predicted. Once these are removed, the structural variability of human code is only marginally higher than that of the decompiled code.
\end{itemize}

This result reinforces our conclusion: in our case (Ghidra vs Source) Human-Likeness is not defined by raw predictability (where the machine wins), but by the \textbf{variance of unpredictability}. A ``natural'' code signature is one that surprises the model in inconsistent, context-dependent bursts, rather than being uniformly predictable.
Meanwhile for the anonymized code, the gap in variance is smaller but it still exists, suggesting that even at the structural level, human code retains a degree of unpredictability that machine-generated code lacks.

\subsection{Other decompilers}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/ppl_distribution_dogbolt.png}
    \caption{Perplexity values for the original source code and the decompiled versions from other decompilers.}\label{fig:ppl_distribution_dogbolt}
\end{figure}

The dogbolt analysis allows us to compare the perplexity values of the decompiled versions from other decompilers (Hex-Rays and Binary Ninja) with the original source code and the Ghidra decompilation, with a subset of the original database (we cannot compare in an absolute way the plot in \Cref{fig:ppl_distribution} with these but only the relative ordering of the distributions).
We can see in \Cref{fig:ppl_distribution_dogbolt} that the perplexity values for the decompiled versions resulted by the dogbolt analysis, are generally in line with the Ghidra ones.
we can see that the original source code still has higher perplexity values compared to the decompiled versions, and the abstracted representations of the code still tend to have higher perplexity values compared to their original counterparts.
Another key observation is that the binary ninja has a perplexity distribution that is more similar to the original source code compared to the Ghidra decompilation, meanwhile Hex-Rays has a distribution in the middle between the other two decompilers, then we can observe that every distribution with \emph{deepseek-r1} has a higher mean perplexity than the same distribution with \emph{qwen-3} like we seen previously.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/dogbolt_loss.png}
    \caption{Loss values for the original source code and the decompiled versions from other decompilers. Code and Abstract}\label{fig:dogbolt_loss}
\end{figure}

The density plots in \Cref{fig:dogbolt_loss} confirm the phenomena observed in the previous sections across both LLM judges. Focusing on the raw \emph{Code Tokens} (top row), we can observe a distinct hierarchy in predictability:

\begin{itemize}
    \item \textbf{Ghidra} exhibits the sharpest peak near zero and the lowest mean loss ($\sim 1.09$ for DeepSeek-R1 and $\sim 0.90$ for Qwen-3). This reaffirms its tendency to generate highly rigid and predictable code.
    \item \textbf{Hex-Rays} follows closely, showing a similar sharp peak but with a slightly higher mean loss and a much larger token count (over 7,000 tokens in this sample), in particular with the \emph{deepseek-r1} while with \emph{qwen-3} we can see that the mean is the same as Ghidra but with a density at peak higher than the others (as the Max loss is much higher than Ghidra). The massive token inflation in Hex-Rays suggests extremely verbose syntactic and lexical choices that artificially dilute the cross-entropy loss.
    \item \textbf{Binary Ninja} stands out as the decompiler that most closely approximates the original source code. Its loss distribution is flatter, and its mean loss ($\sim 1.33$ for DeepSeek-R1) is significantly closer to the original source ($\sim 1.60$) than the other tools. It also exhibits less token inflation compared to Ghidra and Hex-Rays with a Max loss closer to the original source.
\end{itemize}

Meanwhile in the \emph{AST} plots, where identifiers and literals are abstracted, we observe the expected flattening of all distributions. By stripping away lexical entropy, the gap between human-written code and machine-generated code narrows. Notably, the massive token inflation seen in Hex-Rays raw code drops significantly during AST abstraction (from 7,034 down to 3,869 tokens), 
indicating that a vast majority of its predictability stems from highly repetitive lexical tokens, types, and literal declarations rather than purely structural blocks. Even in the abstracted form, Binary Ninja maintains the distribution shape most similar to the original source code and with Hex-Rays having less tokens, the density at peak now is less than the Ghidra one for \emph{qwen-3}. 
Finally, comparing the two models across all subplots, we note that while \emph{qwen-3} consistently produces lower absolute loss values (higher confidence/predictability) than \emph{deepseek-r1}, both models almost perfectly agree (except for the code tokens for \emph{qwen-3} where Hex-Rays have a higher peak density than Ghidra) on the relative ordering of the distributions. Original source code always retains the highest mean loss and widest variance, followed by Binary Ninja, Hex-Rays, and finally Ghidra. 

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/hex-ray_loss.png}
        \caption{Heatmap of loss values for Hex-Rays decompilation with model \emph{qwen-3}}\label{fig:hex-ray_loss}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/hex-ray_loss2.png}
        \caption{Heatmap of loss values for Hex-Rays decompilation with model \emph{deepseek-r1}}\label{fig:hex-ray_loss2}
    \end{minipage}
\end{figure}

A token-level analysis of the Hex-Rays decompilation (e.g., \Cref{fig:hex-ray_loss}) for the model \emph{qwen-3}, reveals that Hex-Rays generates comments and other repetitive tokens that are highly predictable. The most predictable tokens are often those associated with those artefacts of the decompilation process. 
These tokens create a dense cluster of low-loss values that significantly lower the overall perplexity score, despite the presence of more complex and less predictable tokens elsewhere in the code.

When we look at the same decompilation with the model \emph{deepseek-r1} (\Cref{fig:hex-ray_loss2}), we can see that the loss values are generally higher, but the distribution of low-loss tokens is still present, confirming that the anonimization process is a key factor in the analysis of the perplexity, as it standardizes the code through the removal of personalized identifiers, making the predictability of the code more dependent on its structural patterns rather than on specific lexical choices.

\section{LLM-as-a-Judge Evaluation}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/winner_distribution.png}
    \caption{Distribution of winners across all evaluated models.}\label{fig:winner_distribution}
\end{figure}

In \Cref{fig:winner_distribution} we can see the distribution of winners across all evaluated models, based on the qualitative judgments of the \ac{LLM} as a judge.
The two models, Qwen3 and DeepSeek, have a significant number of Ties, but they also show a balanced distribution of wins between the base code and the pr code.
We can see a bin for ``Error'' winners as well, this happens primarly when the model exceed the token limit of 4096 tokens (reasonable limit set by us), since they are reasoning models (they create a context with the generate tokens inside $</think>$ tags) sometimes the context becomes too large (often because they start to repeating thoughts) and they finish the limit without giving inside the response a clear winner (e.g., ``Winner'':``X'').

We previously said that a ``Tie'' is when the model judges always the same result regardless the switch of the base and pr code.
We have a significant number of Ties for all models, watching the result we can observe a ratio of $8.7$ ($618/71$) beetween the number of times that the \ac{LLM} prefers the \ac{PR} version regardless the content.
This suggests a strong bias towards the ``newer'' code, even without telling in the prompt that the Diff code is the newer one, this bias could be due to the fact that Diff code is often more recent and may contain improvements or bug fixes that make it more appealing to the model, or it could be a bias in the model itself towards preferring changes.
Unfortunatly for highlight changes through versions and save on the context window the Diff method is the most convenient, forcing us to Do not count tie-break results in our analysis, giving up almost half of the results.


\subsection{PR \#8628}

\subsection{PR \#8587}

\subsection{PR \#8161}

\subsection{PR \#7253}

\subsection{PR \#6722}

\section{Correlation Perplexity \& LLM}

\section{Vs Human Evaluation}


\section{Discussion}

