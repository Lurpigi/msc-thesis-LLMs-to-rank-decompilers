\chapter{Results}\label{ch:results}



\section{LLM performance}

An empirical evaluation of the \emph{qwen-3} and \emph{deepseek-r1} models was conducted across two distinct tasks: evaluating the ``Humanity'' of decompiled code via generation (\ac{LLM} Judge) and calculating code perplexity (Score). The performance was measured in terms of execution time and peak \ac{VRAM} usage.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/llm_time.png}
    \caption{Execution time for each evaluated model.}\label{fig:llm_time}
\end{figure}

As shown in \Cref{fig:llm_time}, there is a stark dichotomy in execution times between the two operations. The \texttt{score} operation is inherently fast, with both models completing most passes in under 50 seconds. However, the \texttt{generate} operation exhibits significantly higher and more erratic execution times, particularly for \emph{qwen-3}, which frequently exceeds 200 seconds and reaches up to nearly 400 seconds. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/llm_vram.png}
    \caption{Peak VRAM for each evaluated model.}\label{fig:llm_vram}
\end{figure}

\Cref{fig:llm_vram} presents a counter-intuitive finding: calculating perplexity (\texttt{score}) requires a higher median and maximum Peak VRAM than generating text (\emph{generate}). While generation heavily utilizes the KV cache over time, perplexity calculations typically require processing the entire sequence in a single forward pass to compute logits, leading to massive activation memory spikes~\cite{atmer2025prefillvsdecodebottlenecks}.

\subsection{Generation}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/llm_gen.png}
    \caption{Generated tokens vs execution time and peak VRAM for each evaluated model.}\label{fig:llm_gen}
\end{figure}

A closer inspection of \Cref{fig:llm_gen} reveals the root cause of the discrepancy between the two models. The scatter plot for ``Generated Tokens vs Execution Time'' shows a near-perfect linear correlation, confirming that generation time is bottlenecked entirely by the length of the model's output rather than the input context. 
Critically, \emph{qwen-3} routinely generates between 2,000 and 4,000+ tokens during the evaluation task and often reaches our limit of 4096 new tokens. 

Given that the task is an \ac{LLM} Judge producing an evaluation of code ``humanity,'' a rationale exceeding 4,000 tokens is highly suspicious. This suggests that \emph{qwen-3} may be suffering from severe verbosity, repeating the input code, or failing to trigger stop tokens appropriately. 
In contrast, \emph{deepseek-r1} is significantly more concise, rarely exceeding 2,500 tokens; it can still reach the 4096-token limit, but much less frequently, which translates directly to more predictable and efficient execution times.


\begin{wrapfigure}{r}{0.55\textwidth}
    \centering
    \includegraphics[width=0.45\textwidth]{img/qwen_stuck.png}
    \caption{Example of repetitions in \emph{qwen-3} output}\label{fig:stuck}
\end{wrapfigure}

In \Cref{fig:stuck} we can see an example of the output of \emph{qwen-3} for a specific function, we can see that the model is repeating the same pattern of thoughts, which is a common symptom of verbosity and lack of proper stop token triggering. 
This behavior not only leads to unnecessarily long outputs but also significantly increases execution time and VRAM usage, as the model continues to generate tokens without producing meaningful content.
This verbosity issue is a critical flaw for the \ac{LLM} Judge task, as it undermines the model's ability to provide concise and relevant evaluations of code ``humanity,'' and it also creates significant computational inefficiencies that could be prohibitive in larger-scale evaluations or with longer input contexts, 
maybe could be mitigated by implementing stricter stop conditions or by fine-tuning the model to better understand the task requirements and avoid unnecessary verbosity but it will need further investigation.

Furthermore, analyzing the memory consumption in the bottom row of \Cref{fig:llm_gen} reveals a distinct behavior regarding how \ac{VRAM} scales during the generation process. 
The ``Input Tokens vs Peak VRAM'' subplot demonstrates a strange phenomenon: Under 2500 tokens we can observe a cloud of data points but exceeding that number of token we observe an uninterrupted linear relationship, this line is also the minimum boundary of the data. In the line, \emph{deepseek-r1} exhibits a slightly higher memory scaling compared to \emph{qwen-3}, consuming marginally more \ac{VRAM} for the same input length. 

\subsection{Score}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/llm_score.png}
    \caption{Score vs execution time and peak VRAM for each evaluated model.}\label{fig:llm_score}
\end{figure}

The linear scaling of VRAM during the \emph{score} operation (\Cref{fig:llm_score}, right panel) is a major scalability risk. At roughly 3,800 input tokens, both models push past 12 GB of \ac{VRAM}. 
If the dataset contains an enormous larger input (e.g., 8k to 32k tokens), the current perplexity scoring methodology will inevitably result in \ac{OOM} failures on standard consumer GPUs.

Fortunately, the \emph{generate} operation does not exhibit this issue, as it processes tokens sequentially and can leverage the KV cache to manage memory more efficiently.

The duration is mostly flat (0--20 seconds), but spikes occurfor both models. This implies either batch-processing artifacts or that the evaluation dataset contains many decompiled functions of the exact same token length that trigger specific internal computational bottlenecks.
A possible explanation is that runtime is influenced not only by input length, but also by the distribution of per-token loss values. This hypothesis requires additional targeted experiments, and further studies will be necessary to confirm it.


While both models are capable of performing the required tasks, \emph{deepseek-r1} is objectively better suited for the \ac{LLM} Judge (\emph{generate}) role due to its restraint and conciseness, avoiding the computationally expensive verbosity traps that plague \emph{qwen-3}. 
However, for the perplexity (\emph{score}) task, the architecture of the operation itself poses a severe hardware bottleneck that scales poorly with larger decompiler outputs.

\section{Perplexity as a Metric for ``Humanness''}\label{sec:perplexityres}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/ppl_distribution.png}
    \caption{On the top, the perplexity values across original source code, base code, and pr code. On the bottom, perplexity values across abstracted representations of the same functions.}\label{fig:ppl_distribution}
\end{figure}

In \Cref{fig:ppl_distribution} we can see the distribution of perplexity values for the original source code, the base code, and the pr code, as well as their abstracted representations.
We can observe that the original source code has generally higher perplexity values compared to the decompiled versions,
which is unexpected since the original source code should be more ``natural'' and predictable than the decompiled output.
This suggests that the decompilation process may introduce certain patterns or structures that are more familiar to the language model, leading to lower perplexity scores, while the original source code may contain more variability and less predictable constructs that result in higher perplexity.
We can observe that the perplexity distribution calculated with \emph{deepseek-r1} is generally higher than the one calculated with \emph{qwen-3}, this is consistent with the previous observation that \emph{qwen-3}.

Another observation is that the abstracted representations of the code (right side of the figure) tend to have higher perplexity values compared to their original counterparts (left side of the figure).
This is likely because the abstraction process removes specific identifiers and literals, which can make the code less predictable and more ``surprising'' to the language model, 
but even in this case, the original source code still has higher perplexity than the decompiled versions, reinforcing the idea that the decompilation process may be introducing more predictable patterns into the code.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.99\textwidth]{img/global_loss.png}
    \caption{Density distribution of cross-entropy loss values for all tokens in the original source code and the decompiled versions.}\label{fig:global_loss}
\end{figure}

In \Cref{fig:global_loss} we can observe that the original code exhibits a wider, slightly flatter distribution with a higher mean loss compared to the decompiled output, which is characterized by a sharper peak shifted towards zero.
This visualization highlights a counter-intuitive phenomenon: despite the original source code being the ``human ground truth'', the language model finds the decompiled code significantly more predictable.

We attribute this behavior to Token Inflation and Loss Dilution:
As indicated by the token counts in the figure (e.g., $\sim$168k tokens for decompiled vs $\sim$120k for original source), the decompilation process introduces a substantial \emph{token inflation}.
Ghidra generate verbose, explicit code full of boilerplate structures (e.g., redundant casts, standard control flow patterns, explicit initializations, and restricted vocabulary). These pattern tokens are syntactically rigid and in a context where are used, they are easy to predict for the model, leading to a large number of tokens with very low loss values (close to zero). 
Their sheer volume effectively dilutes the mean loss, artificially lowering the overall perplexity score compared to the denser, more information-rich human code.
In contrast, the original source code reflects human authorship, which includes domain-specific naming conventions, creative syntactic choices, and stylistic variability. This ``human entropy'' flattens the density curve and shifts the mean loss to the right, as the model is more frequently ``surprised'' by the programmer's unique choices compared to the machine's standardized output.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.99\textwidth]{img/global_loss_ast.png}
    \caption{Density distribution of cross-entropy loss values for all tokens in the original source code and the decompiled versions.}\label{fig:global_loss_ast}
\end{figure}

In \Cref{fig:global_loss_ast} we can see the \ac{AST} version of the previous analysis, where we abstracted away variable names and literals to focus on the structural aspects of the code.
Comparing these results with the previous analysis on raw code tokens, we observe two critical phenomena:

\begin{enumerate}
    \item \textbf{Persistence of structural inflation:} Even in the anonymized form, the \textit{token inflation} remains significant. The decompiled AST contains $\sim$107k tokens compared to $\sim$70k for the original source ($\sim$+52\%). 
        This confirms that the verbosity of the decompiled code is not merely lexical (e.g., long variable names) but syntactical. The decompiler introduces explicit casts, redundant blocks, and verbose control flow structures that persist even after anonymization, continuing to dilute the mean loss with predictable tokens.
    \item \textbf{The closer entropy:}
    Unlike the raw code analysis, where the gap between the distributions was pronounced, the AST distributions for Source and Decompiled code are more similar in shape. The difference in Mean Loss has narrowed (e.g., for \emph{deepseek-r1}, the gap represents only $\sim 0.047$, compared to larger margins in the raw code).

    This convergence suggests that the \emph{lexical entropy} was a discriminator in the previous analysis.
    \begin{itemize}
        \item In the raw code, human-written names provided high variability (surprisal), while decompiled names were generic.
        \item In the AST version, the anonymization process effectively ``standardizes'' the two codes; Consequently, the source code becomes more predictable by the model.
    \end{itemize}
\end{enumerate}


\begin{wrapfigure}{r}{0.55\textwidth}
    \centering
    \includegraphics[width=0.45\textwidth]{img/example_loss_func.png}
    \caption{Loss values for \texttt{xls\_parseWorkBook} in the decompiled base code}\label{fig:example_loss_func}
\end{wrapfigure}

The token-level analisys allow us to verify the tokens that contribute the most to the loss, and consequently to the perplexity, in a specific function.
We can observe that the token itself is not the only factor that contributes to the loss, but especially the context in which it is used.
For example, in \Cref{fig:example_loss_func} we can see the loss values for the token ``\texttt{LAB}'', which is a common label used in the decompiled code to indicate jump targets.
This token has a low loss value (1.664) when it appears after the \texttt{do{}while loop}, but it has a much higher loss value (11.812) when it appears inside the \texttt{if} allowing a flow branch to ignore the condition and entering the scope without checks.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.99\textwidth]{img/example_loss.png}
    \caption{Example of loss values for a function in the decompiled code.}\label{fig:example_loss}
\end{figure}

Obviously every function has a different distribution of loss values, and some functions may be more ``natural'' than others.
In \Cref{fig:example_loss} we can see an example of the loss values for \texttt{xls\_parseWorkBook} function from the DeepSeek analisys, 
remembering that the perplexity is calculated as the exponential of the mean loss~\ref{sec:perplexity}, we can see that the original source code has a perplexity of $\sim 3.06$ (mean loss $\sim 1.12$), while the decompiled base version has a perplexity of $\sim 2.4$ (mean loss $\sim 0.89$).
We can see that the max values for the loss are higher for the decompiled version, in contrary to the global distribution where the decompiled code had a sharper peak towards zero, but then when we look at the anonymized version of the same function, the original source code became the one with a higher max loss value than the decompiled version.
However, the anonymization process manages to bring the two versions significantly closer, both in terms of loss values and, consequently, perplexity.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.99\textwidth]{img/std_dev_loss.png}
    \caption{Standard deviation of loss values across all functions in the normal version and anonymized version.}\label{fig:std_dev_loss}
\end{figure}

In \Cref{fig:std_dev_loss} we investigate the \emph{Entropy Variance} of the code by analyzing the standard deviation of the token loss.
While the mean loss indicates the average predictability, the standard deviation reveals the \emph{dynamic range} of the code complexity.

We can observe a clear trend across both models:
\begin{itemize}
    \item \textbf{Difference in Variance:} The original source code consistently exhibits higher standard deviation compared to the decompiled versions. This confirms that human-written code is characterized by \textit{burstiness}: it alternates between low-entropy boilerplate and high-entropy domain-specific logic. The language model struggles to predict this rhythm, leading to fluctuating loss values.
    The decompiled code shows significantly lower variance. This reflects the \textit{monotonicity} of machine-generated code. In our case Ghidra applies consistent transformation rules throughout the binary, resulting in more predictabile results.
    
    \item \textbf{The Lexical Factor:} Comparing the code panel with the \ac{AST} one, we notice that the gap between Source and Decompiled shrinks significantly in the AST representation. This implies that a substantial portion of the entropy variance in human code is driven by \emph{lexical choices} (variable naming and literals) as we predicted. Once these are removed, the structural variability of human code is only marginally higher than that of the decompiled code.
\end{itemize}

This result reinforces our conclusion: in our case (Ghidra vs Source) Human-Likeness is not defined by raw predictability (where the machine wins), but by the \textbf{variance of unpredictability}. A ``natural'' code signature is one that surprises the model in inconsistent, context-dependent bursts, rather than being uniformly predictable.
Meanwhile for the anonymized code, the gap in variance is smaller but it still exists, suggesting that even at the structural level, human code retains a degree of unpredictability that machine-generated code lacks.

\subsection{Other decompilers}\label{sec:perplexitydog}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/ppl_distribution_dogbolt.png}
    \caption{Perplexity values for the original source code and the decompiled versions from other decompilers.}\label{fig:ppl_distribution_dogbolt}
\end{figure}

The dogbolt analysis allows us to compare the perplexity values of the decompiled versions from other decompilers (Hex-Rays and Binary Ninja) with the original source code and the Ghidra decompilation, with a subset of the original database (we cannot compare in an absolute way the plot in \Cref{fig:ppl_distribution} with these but only the relative ordering of the distributions).
We can see in \Cref{fig:ppl_distribution_dogbolt} that the perplexity values for the decompiled versions resulted by the dogbolt analysis, are generally in line with the Ghidra ones.
we can see that the original source code still has higher perplexity values compared to the decompiled versions, and the abstracted representations of the code still tend to have higher perplexity values compared to their original counterparts.
Another key observation is that the binary ninja has a perplexity distribution that is more similar to the original source code compared to the Ghidra decompilation, meanwhile Hex-Rays has a distribution in the middle between the other two decompilers, then we can observe that every distribution with \emph{deepseek-r1} has a higher mean perplexity than the same distribution with \emph{qwen-3} like we seen previously.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/dogbolt_loss.png}
    \caption{Loss values for the original source code and the decompiled versions from other decompilers. Code}\label{fig:dogbolt_loss}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/dogbolt_loss_ast.png}
    \caption{Loss values for the original source code and the decompiled versions from other decompilers. Abstract}\label{fig:dogbolt_loss_ast}
\end{figure}

The density plots in \Cref{fig:dogbolt_loss} confirm the phenomena observed in the previous sections across both LLM judges. Focusing on the raw \emph{Code Tokens} (top row), we can observe a distinct hierarchy in predictability:

\begin{itemize}
    \item \textbf{Ghidra} exhibits the sharpest peak near zero and the lowest mean loss ($\sim 1.09$ for DeepSeek-R1 and $\sim 0.90$ for Qwen-3). This reaffirms its tendency to generate highly rigid and predictable code.
    \item \textbf{Hex-Rays} follows closely, showing a similar sharp peak but with a slightly higher mean loss and a much larger token count (over 7,000 tokens in this sample), in particular with the \emph{deepseek-r1} while with \emph{qwen-3} we can see that the mean is the same as Ghidra but with a density at peak higher than the others (as the Max loss is much higher than Ghidra). The massive token inflation in Hex-Rays suggests extremely verbose syntactic and lexical choices that artificially dilute the cross-entropy loss.
    \item \textbf{Binary Ninja} stands out as the decompiler that most closely approximates the original source code. Its loss distribution is flatter, and its mean loss ($\sim 1.33$ for DeepSeek-R1) is significantly closer to the original source ($\sim 1.60$) than the other tools. It also exhibits less token inflation compared to Ghidra and Hex-Rays with a Max loss closer to the original source.
\end{itemize}

Meanwhile in the \emph{AST} plots in \Cref{fig:dogbolt_loss_ast}, where identifiers and literals are abstracted, we observe the expected flattening of all distributions. By stripping away lexical entropy, the gap between human-written code and machine-generated code narrows. Notably, the massive token inflation seen in Hex-Rays raw code drops significantly during AST abstraction (from 7,034 down to 3,869 tokens), 
indicating that a vast majority of its predictability stems from highly repetitive lexical tokens, types, and literal declarations rather than purely structural blocks. Even in the abstracted form, Binary Ninja maintains the distribution shape most similar to the original source code and with Hex-Rays having less tokens, the density at peak now is less than the Ghidra one for \emph{qwen-3}. 
Finally, comparing the two models across all subplots, we note that while \emph{qwen-3} consistently produces lower absolute loss values (higher confidence/predictability) than \emph{deepseek-r1}, both models almost perfectly agree (except for the code tokens for \emph{qwen-3} where Hex-Rays have a higher peak density than Ghidra) on the relative ordering of the distributions. Original source code always retains the highest mean loss and widest variance, followed by Binary Ninja, Hex-Rays, and finally Ghidra. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{img/hex-ray_loss.png}
        \caption{Heatmap of loss values for Hex-Rays decompilation with model \emph{qwen-3}}\label{fig:hex-ray_loss}
\end{figure}

\begin{figure}
\centering
    \includegraphics[width=0.9\linewidth]{img/hex-ray_loss2.png}
        \caption{Heatmap of loss values for Hex-Rays decompilation with model \emph{deepseek-r1}}\label{fig:hex-ray_loss2}
\end{figure}

A token-level analysis of the Hex-Rays decompilation (e.g., \Cref{fig:hex-ray_loss}) for the model \emph{qwen-3}, reveals that Hex-Rays generates comments and other repetitive tokens that are highly predictable. The most predictable tokens are often those associated with those artefacts of the decompilation process. 
These tokens create a dense cluster of low-loss values that significantly lower the overall perplexity score, despite the presence of more complex and less predictable tokens elsewhere in the code.

When we look at the same decompilation with the model \emph{deepseek-r1} (\Cref{fig:hex-ray_loss2}), we can see that the loss values are generally higher, but the distribution of low-loss tokens is still present, confirming that the anonimization process is a key factor in the analysis of the perplexity, as it standardizes the code through the removal of personalized identifiers, making the predictability of the code more dependent on its structural patterns rather than on specific lexical choices.

\section{LLM-as-a-Judge Evaluation}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/winner_distribution.png}
    \caption{Distribution of winners across all evaluated models.}\label{fig:winner_distribution}
\end{figure}

In \Cref{fig:winner_distribution} we can see the distribution of winners across all evaluated models, based on the qualitative judgments of the \ac{LLM} as a judge.
The two models, Qwen3 and DeepSeek, have a significant number of Ties, but they also show a balanced distribution of wins between the base code and the pr code.
We can see a bin for ``Error'' winners as well, this happens primarly when the model exceed the token limit of 4096 tokens (reasonable limit set by us), since they are reasoning models (they create a context with the generate tokens inside $<think>$ tags) sometimes the context becomes too large (often because they repeat thoughts entering in a loop) and they finish the limit without giving inside the response a clear winner (e.g., ``Winner'':``X'').

We previously said that a ``Tie'' is when the model judges always the same result regardless the switch of the base and pr code.
We have a significant number of Ties for all models, watching the result we can observe a ratio of $\sim \frac{2}{3}$ ($204 / 308$) of times where the model prefers the \ac{PR} version (204) over the \texttt{BASE} version (104).
This suggests a position bias, even using our Diff format explained in \Cref{sec:prompts}, this bias could be caused by the model's tendency to favor the second option presented in a comparison, or it could be due to some subtle cues in the prompt formatting that inadvertently signal a preference for the \ac{PR} version. 


\subsection{PR \#8628}
This \ac{PR} improves handling of constant subtractions that are rewritten during decompilation as additions with negative immediates (e.g., \texttt{x + -0x1a} $\rightarrow$ \texttt{x - 0x1a}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8628/preference.png}
    \caption{Distribution of winners across the four prompts for PR \#8628}\label{fig:8628_winner_distribution}
\end{figure}

As shown in \Cref{fig:8628_winner_distribution}, the qualitative evaluation strongly favors the \ac{PR} branch. In the ``Code Blind (Human-Likeness)'' and ``AST Blind (Structural Humanity)'' assessments, the \ac{PR} version overwhelmingly wins against the \texttt{BASE} branch across both \emph{deepseek-r1} and \emph{qwen-3} models, often exceeding a 60\% win rate. This indicates that rewriting expressions to use explicit subtraction rather than adding a negative immediate is universally recognized by the models as a more natural, human-readable idiom.

However, when evaluating fidelity to the original source code (``Code Fidelity'' and ``AST Fidelity''), the margin of victory for the \ac{PR} branch shrinks, and the number of ``TIE'' outcomes increases significantly. This suggests that while the \ac{PR}'s output is structurally cleaner, the original source code may contain diverse expressions that do not perfectly align with either the \texttt{BASE} or the \ac{PR} decompilation, causing the models to frequently declare a tie.



\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8628/consistency.png}
    \caption{Consistency across the prompts for evaluating ``humanity'' for PR \#8628}\label{fig:8628_consistency}
\end{figure}

To further investigate the robustness of these judgments, we analyzed the consistency of the models' decisions between evaluating raw code and evaluating its abstracted \ac{AST} representation (\Cref{fig:8628_consistency}). \emph{Deepseek-r1} shows 100\% consistency, meaning it never changed its preference between the raw code and the \ac{AST}. In contrast, \emph{qwen-3} exhibits an inconsistency rate of approximately 14\% (two times).
These two inconsistencies occur in the files \texttt{task-readstat\_sav\_parse\_date} and \texttt{task-readstat\_readstat\_parse\_por}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8628/ex1.png}
    \caption{Inconsistency in file task-readstat\_sav\_parse\_date-O3}\label{fig:8628_ex1}
\end{figure}

In the first inconsistent case (\Cref{fig:8628_ex1}), the \emph{qwen-3} model shifts its preference based on the presence of parentheses. When evaluating the raw code, it praises the \ac{PR} for using parentheses to group arithmetic operations (``\texttt{((type)id * 2 - 2)}''), arguing it makes the intent clearer and aligns with human engineering practices, It does not talk about the sum of negative number but rely his judgment on parenthesis. However, when evaluating the \ac{AST}, the model flips its vote to the \texttt{BASE} version. It suddenly argues that the extra parentheses introduced in the \ac{PR} feel ``mechanically generated'' and that the \texttt{BASE} version is more idiomatic because it trusts operator precedence, againg ignoring the sum of negative number. 
We can see that in this case, the model put in priority the parentheses over the sum of negative number. But the fact that the model is not consistent in its judgment when the parentheses are removed in the \ac{AST} evaluation, suggests that it may be overfitting to superficial cues rather than truly understanding the underlying code structure and semantics.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8628/ex2.png}
    \caption{Inconsistency in file task-readstat\_readstat\_parse\_por-O0}\label{fig:8628_ex2}
\end{figure}

The second inconsistency (\Cref{fig:8628_ex2}) highlights a conflict regarding type casting and literal suffixes. In the raw code evaluation, \emph{qwen-3} prefers the \texttt{BASE} version because it uses a direct comparison with \texttt{-1}, calling the \ac{PR}'s use of an unsigned cast (\texttt{-1U}) an unnecessary complication that obscures intent. 
Yet, when evaluating the \ac{AST}, the model flips to favor the \ac{PR}. In this abstracted view, it argues that the explicit unsigned suffix (\texttt{U}) improves clarity and correctness, avoiding signed/unsigned ambiguity. 
This flip suggests that the model's judgment is heavily influenced by surface-level syntax rather than a deeper understanding of the code's semantics. The presence of the unsigned suffix in the \ac{PR} version may have triggered a bias in the model when evaluating the raw code, leading it to view the \texttt{BASE} version as more straightforward. However, when the syntax is abstracted away in the \ac{AST}, the model's preference shifts, indicating that it may be overfitting to specific tokens or patterns rather than consistently evaluating the underlying code quality and readability.

Despite these edge cases of inconsistency, the models generally demonstrate a strong ability to recognize the \ac{PR}'s structural improvements regardless of the lexical context. \Cref{fig:8628_ex3} provides a clear example of this robust evaluation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8628/ex3.png}
    \caption{Consistency in evaluating arithmetic expressions}\label{fig:8628_ex3}
\end{figure}

In this instance, \emph{qwen-3} correctly identifies the core structural improvement in both the raw code and the \ac{AST} representation. When evaluating the raw code, it notes that subtracting a positive integer (e.g., \texttt{iVar3 - 1}) is a standard decrement pattern and significantly more idiomatic than adding a negative value. When transitioning to the \ac{AST} evaluation, the model maintains this exact reasoning, explicitly favoring the abstracted \texttt{id - 1} over \texttt{id + -1}. 

This demonstrates that when a structural transformation is sufficiently distinct and maps clearly to standard human programming conventions (like direct mathematical notation), the model's judgment remains stable. The removal of specific variable names and literal values during the \ac{AST} anonymization does not disrupt its ability to identify the more human-like syntactic pattern.


\subsection{PR \#8587}

This \ac{PR} focuses on automatically detecting and correcting one-based indexing patterns for spacebase constants (e.g., \texttt{*(undefined *)((long)i * 0x30 + 0xaddr + (long)j * 4)} $\rightarrow$ \texttt{globalArray[(long)i + -1].field[j]}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8587/preference.png}
    \caption{Distribution of winners across the four prompts for PR \#8587}\label{fig:8587_winner_distribution}
\end{figure}

As seen in \Cref{fig:8587_winner_distribution}, the results for the raw code blind evaluation are exceptionally decisive: both \emph{deepseek-r1} and \emph{qwen-3} unanimously prefer the \ac{PR} output 100\% of the time. Transforming obscure, hardcoded pointer arithmetic into clean array indexing is universally recognized by the models as a massive improvement in human readability. 

However, a notable shift occurs when examining the \ac{AST} Fidelity evaluations (bottom right). When forced to compare the abstracted code against the abstracted original source, the ``TIE'' outcome becomes the dominant result for both models. This suggests that while the \ac{PR} produces much cleaner C code, the original source code's structure might still differ slightly from Ghidra's output, or the abstraction process obscures the specific array-indexing improvements, leading the models to find both versions equally distant from the original blueprint.

This \ac{PR} in particular only modifies 7 functions from the base branch.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8587/consistency.png}
    \caption{Consistency across the prompts for evaluating ``humanity'' for PR \#8587}\label{fig:8587_consistency}
\end{figure}

Because of this small sample size, \Cref{fig:8587_consistency} shows binary extremes. \emph{Deepseek-r1} is 100\% consistent in its evaluations across the 7 functions. \emph{Qwen-3}, however, shows an inconsistency rate of roughly 25\% (one function).
We encounter this inconsistency specifically in the \texttt{task-readstat\_sav\_parse\_date-O2} file.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8587/ex1.png}
    \caption{Inconsistency in file task-readstat\_sav\_parse\_date-O2}\label{fig:8587_ex1}
\end{figure}

In \Cref{fig:8587_ex1}, we can observe exactly why \emph{qwen-3} changes its verdict. During the raw code evaluation, it correctly praises the \ac{PR} for using a direct array lookup rather than the \texttt{BASE} branch's obscure pointer arithmetic with a hardcoded address (\texttt{0x102311}). It notes that this leverages named data structures, which is highly idiomatic.
However, when the code is stripped into an \ac{AST}, the array name is replaced by generic identifiers. In the anonimized version, \emph{qwen-3} suddenly perceives the \ac{PR}'s dynamic array indexing (e.g., \texttt{id + id + 1}) as ``unnecessary arithmetic complexity'' and reverts to favoring the \texttt{BASE} branch, arguing that a fixed offset (\texttt{0x102311 + id}) is simpler and more intentional. 
In this case the model's judgment hallucinates, since the \ac{PR} version is objectively more human-like in its use of array indexing, and the \texttt{BASE} version's pointer arithmetic is more convoluted.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8587/ex2.png}
    \caption{Example of consistency in evaluation by deepseek-r1}\label{fig:8587_ex2}
\end{figure}

Conversely, \Cref{fig:8587_ex2} shows that the majority of cases correctly identify the modifications of the \ac{PR}. In this case \emph{Deepseek-r1} identifies that the \ac{PR}'s output avoids raw pointer arithmetic in the raw code, and when evaluating the \ac{AST}, it continues to recognize the underlying structural pattern of ``direct array access'' as the superior, more human-like engineering choice, regardless of whether the specific variable names are visible or not.
\subsection{PR \#8161}

This \ac{PR} addresses a possible issue with the handling of the \texttt{final\_transform} method of the \texttt{while} BasicBlocks, which could lead to not recover a \texttt{for} loop where it could be possible instead.

Unfortunately, the difference between the BASE and \ac{PR} versions in our dataset is not inherent to the \ac{PR} itself, but rather a consequence of the fact that the BASE branch was generated with an newer version of Ghidra (post-PR) while the \ac{PR} branch was generated with a older version of Ghidra.
We can see that in the fork repo of the \ac{PR} branch, where it contains the message ``This branch is 2 commits ahead of and 1948 commits behind NationalSecurityAgency:master'', and consequently the evaluation is not really about the \ac{PR} but about the differences between two versions of Ghidra, which makes it difficult to draw conclusions about the specific impact of the \ac{PR}'s changes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8161/preference.png}
    \caption{Distribution of winners across the four prompts for PR \#8161}\label{fig:8161_winner_distribution}
\end{figure}

Looking at the aggregate data in \Cref{fig:8161_winner_distribution}, the impact of the \ac{AST} abstraction on this specific dataset is profound. In the ``Code Blind'' evaluation, the models are largely undecided (high ``TIE'' rate) or slightly favor the older \ac{PR} branch due to localized quirks like the one seen in \Cref{fig:8161_ex3}. However, in the anonimized evaluation, there is a massive shift in preference toward the BASE branch. This suggests that the structural improvements accumulated in the 1948 commits of the BASE branch become much more apparent to the models once distracting lexical artifacts are removed.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8161/consistency.png}
    \caption{Consistency across the prompts for evaluating ``humanity'' for PR \#8161}\label{fig:8161_consistency}
\end{figure}

Consequently, as shown in \Cref{fig:8161_consistency}, the inconsistency rate for this dataset is notable with more than 25\% for \emph{deepseek-r1} and over 30\% for \emph{qwen-3}. 

Nevertheless, we can still observe a few interesting examples:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8161/ex1.png}
    \caption{The BASE version is better at for loop recovery}\label{fig:8161_ex1}
\end{figure}

In \Cref{fig:8161_ex1}, we can clearly see a paradox: although this \ac{PR} was explicitly designed to improve \texttt{for}-loop recovery, the evaluated BASE output still appears more idiomatic. In our dataset, this is explained by the branch mismatch discussed above: the \texttt{BASE} branch was generated from a much newer Ghidra state, while the \ac{PR} branch is based on an older snapshot. 
As a result, the older \ac{PR} output falls back to a more convoluted \texttt{do-while} structure with manual counter management. Encouragingly, \emph{deepseek-r1} consistently identifies the \texttt{BASE} version as more human-readable in both raw-code and \ac{AST} evaluations, explicitly favoring the recovered \texttt{for} loop as the more natural control-flow pattern.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8161/ex2.png}
    \caption{Inconsistency in file task-libxls\_xls\_parseWorkBook-O3}\label{fig:8161_ex2}
\end{figure}


However, the differences in Ghidra versions across thousands of commits introduce other artifacts that cause evaluation inconsistencies. In \Cref{fig:8161_ex2}, we observe an inconsistency from \emph{deepseek-r1} regarding explicit type casting. When evaluating the raw code, the model praises the \ac{PR} branch for including an explicit cast, interpreting it as a ``human-like trait, showing attention to detail.'' Yet, when the lexical details are abstracted away in the \ac{AST} evaluation, the model's judgment flips completely. 
It penalizes the exact same cast, now viewing it as ``redundancy'' that makes the code appear ``more like a machine translation''.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8161/ex3.png}
    \caption{Older is better in file task-file\_file\_zmagic-O3}\label{fig:8161_ex3}
\end{figure}

Interestingly, despite the BASE branch generally representing a much newer and more capable decompiler state, there are isolated instances where the older \ac{PR} branch generated cleaner localized logic. 
\Cref{fig:8161_ex3} illustrates a scenario where both models \emph{consistently} prefer the \ac{PR} branch across both raw code and \ac{AST} evaluations. 

\subsection{PR \#7253}

This \ac{PR} sorts \texttt{switch} case entries by their target address

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/7253/preference.png}
    \caption{Distribution of winners across the four prompts for PR \#7253}\label{fig:7253_winner_distribution}
\end{figure}

As shown in \Cref{fig:7253_winner_distribution}, we can observe that in the ``AST Blind'' evaluation, the BASE branch is heavily preferred, with \emph{deepseek-r1} strongly favoring the BASE version (nearly 70\% win rate) and \emph{qwen-3} predominantly defaulting to TIE or BASE., while we can see in the other plots a prominent indecision with a lot of TIEs and a slight preference for the BASE branch.
This behavior can be explained by two main factors: 
\begin{inparaenum}[\it (i)]
    \item As the \ac{PR} \#8161 was generated with an older version of Ghidra, the BASE branch already contains improvements
    \item We will see that in this case of reordering switch cases, the models struggle to recognize the transformation as an improvement, and they may even hallucinate reasons to justify their bias
\end{inparaenum}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/7253/consistency.png}
    \caption{Consistency across the prompts for evaluating ``humanity'' for PR \#7253}\label{fig:7253_consistency}
\end{figure}

This is clearly reflected in \Cref{fig:7253_consistency}, where \emph{deepseek-r1} exhibits a substantial inconsistency rate of roughly 35\%, and \emph{qwen-3} proves wildly unstable, disagreeing with its own raw-code evaluations in over 60\% of the cases when presented with the \ac{AST}.

In the next two figures, the only modification is the order of the cases in the switch statement (No other changes are introduced).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/7253/ex1.png}
    \caption{Not recognized the modification in task-file\_file\_signextend-O0}\label{fig:7253_ex1}
\end{figure}

In \Cref{fig:7253_ex1}, we observe a severe instance of model hallucination by \emph{qwen-3}. Unable to find a structural or logical improvement in the simple reordering, the model fabricates reasons to justify its preference for the \ac{PR}. 
During the raw code evaluation, it falsely claims that the \ac{PR} ``introduces explicit 'break;' statements'' to prevent fall-through. During the \ac{AST} evaluation, it similarly hallucinates that the \ac{PR} ``reduces redundant case entries''. 
This exposes a significant blind spot in using \ac{LLM}s as judges: when forced to compare logically identical blocks that differ only in arrangement, the model may invent non-existent structural diffs rather than correctly identifying the transformation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/7253/ex2.png}
    \caption{Inconsistency in evaluation in task-file\_file\_signextend-O2}\label{fig:7253_ex2}
\end{figure}

\Cref{fig:7253_ex2} demonstrates a different vulnerability: contradictory reasoning driven by lexical abstraction. In this instance, \emph{deepseek-r1} evaluates the raw code and selects the BASE version, arguing that it ``groups similar cases together'', avoiding machine-like redundancy. 
However, when the specific \texttt{case} values and internal variables are abstracted into an \ac{AST}, the model flips its decision to the \ac{PR} version, ironically using the exact same claiming. 

\subsection{PR \#6722}

This \ac{PR} focuses on improving pointer-expression recovery, allowing clean output such as \texttt{PTR 0041a1b8->ar[local\_18]} instead of raw pointer casts and offset arithmetic.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/6722/preference.png}
    \caption{Distribution of winners across the four prompts for PR \#6722}\label{fig:6722_winner_distribution}
\end{figure}

This \ac{PR} suffers from the same branch mismatch issue as \ac{PR} \#8161, where the BASE branch is generated with a much newer Ghidra version than the \ac{PR} branch.
Because of this significant version mismatch, the overall results in \Cref{fig:6722_winner_distribution} are heavily skewed toward the \texttt{BASE} branch. While the \ac{PR} itself improves pointer-expression recovery, the thousands of newer commits in the \texttt{BASE} branch include numerous other control-flow and structural enhancements. Consequently, the \ac{LLM} judges, particularly in the ``AST Blind'' evaluation where structural integrity takes precedence over lexical details, strongly favor the \texttt{BASE} version. \emph{Deepseek-r1} chooses \texttt{BASE} in over 60\% of the AST cases, recognizing the overall maturity of the newer decompiler output over the isolated pointer fixes in the older \ac{PR} branch.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/6722/consistency.png}
    \caption{Consistency across the prompts for evaluating ``humanity'' for PR \#6722}\label{fig:6722_consistency}
\end{figure}

Despite this imbalance, the consistency rates for PR \#6722 (\Cref{fig:6722_consistency}) are relatively high compared to other PRs, with \emph{deepseek-r1} achieving over 85\% consistency and \emph{qwen-3} nearing 80\%. However, the inconsistencies that do occur highlight recurring vulnerabilities in the models' evaluation logic, particularly regarding type casting and complex control flow.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/6722/ex1.png}
    \caption{Inconsistency in evaluation in sav\_parse\_long\_variable\_names\_record-O0}\label{fig:6722_ex1}
\end{figure}

\Cref{fig:6722_ex1} illustrates a classic contradiction in how \ac{LLM}s perceive type casting, mirroring issues seen in previous PRs. In the raw code evaluation, \emph{deepseek-r1} prefers the \ac{PR} branch, praising the explicit cast of the \texttt{bsearch} result to \texttt{(char *)} as a ``human-like trait'' that enhances clarity and maintainability. Yet, when evaluating the abstracted \ac{AST} where specific types are hidden, the model completely reverses its logic. It awards the win to the \texttt{BASE} branch, penalizing the explicit cast as an ``unnecessary'' and ``redundant'' feature, suddenly arguing that trusting implicit conversions is the true idiomatic human practice.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/6722/ex2.png}
    \caption{Inconsistency in evaluation in task-xz\_lzma\_validate\_chain-O3}\label{fig:6722_ex2}
\end{figure}

More alarmingly, \Cref{fig:6722_ex2} exposes a severe case of double hallucination and confusion caused by the \ac{AST} abstraction process. In this complex function, both the \texttt{BASE} and \ac{PR} versions utilize \texttt{goto} statements to handle edge-case returns. 
\begin{itemize}
    \item During the raw code evaluation, \emph{qwen-3} favors the \ac{PR}, falsely claiming that it ``avoids using a \texttt{goto} statement'' to justify its preference for the \ac{PR}'s \texttt{do-while} loop structure. 
    \item When presented with the \ac{AST}, the model flips its preference to \texttt{BASE} and hallucinates again, this time falsely claiming that \texttt{BASE} is the one that ``avoids complex nested conditions and goto statements.'' 
\end{itemize}

%Furthermore, in the \ac{AST} evaluation, \emph{qwen-3} explicitly penalizes the \ac{PR} branch for containing ``potential typos (e.g., `idid')''. This ``typo'' is not a decompiler error, but rather an artifact of our \ac{AST} anonymization process, where adjacent abstracted identifiers or specific token concatenations end up rendered as \texttt{idid}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/6722/ex3.png}
    \caption{Example of majority consistency in evaluation across models}\label{fig:6722_ex3}
\end{figure}

Despite the before mentioned edge cases and hallucinations, the vast majority of evaluations for this dataset follow a highly consistent path that strongly favors the \texttt{BASE} branch. \Cref{fig:6722_ex3} serves as the quintessential example of this dominant trend, perfectly illustrating why the newer Ghidra version consistently wins both the raw code and \ac{AST} evaluations.

In this snippet, the older \ac{PR} version introduces a convoluted and non-idiomatic pattern for array access.
Both \emph{deepseek-r1} and \emph{qwen-3} correctly and consistently penalize the \ac{PR} branch for this regression. During the raw code evaluation, they easily identify the \ac{PR}'s approach as verbose, mechanical, and overly complex. 

Because the redundancy is purely structural and does not rely on specific variable names to appear unnatural, the models easily maintain their judgment without falling into the trap of lexical bias. This demonstrates that when a decompiler generates an objectively inferior syntactic construct, such as redundant address-taking or dereferencing operations, the \ac{LLM} judges can reliably and consistently detect the ``machine-like'' quality regardless of whether the code is raw or anonymized.

\section{Dogbolt Evaluation}\label{sec:dogbolteva}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/dogbolt_winners.png}
    \caption{Distribution of winners across the four prompts for the dogbolt evaluation}\label{fig:dogbolt_winner_distribution}
\end{figure}


Based on the data presented in \Cref{fig:dogbolt_winner_distribution} and detailed in \Cref{tab:dogbolt_winners_by_prompt}, several critical insights emerge regarding how the LLM judges perceive the outputs of the different decompilers.
There is a stark contrast between the ``Blind'' evaluations (where the LLM judges purely on abstract readability and perceived ``humanity'') and the ``+ Source'' evaluations (where the LLM judges fidelity to the original code) for \emph{binary-ninja}.
In the blind tests (\emph{Code Blind} and \emph{AST Blind}), \emph{binary-ninja} frequently emerges as winner, particularly according to \emph{deepseek-r1}, securing 16 and 21 wins respectively. The models evidently perceive its output as highly readable and natural. However, the moment the original source code is introduced as a baseline (\emph{Code + Source} and \emph{AST + Source}), its win rate collapses drastically. This heavily implies that while \emph{binary-ninja} applies aggressive transformations to make the code look clean and human-readable, these transformations structurally distance the output from the original human-written logic. 

Another key indicator of the massive differences between the versions is the exceptionally high number of ``Ties''. When the source code is provided, the tie rate spikes, peaking at 30 ties out of the dataset for \emph{deepseek-r1} in the \emph{Code + Source} test. 
This high rate of indecision suggests that none of the decompilers successfully reconstruct the original source in a recognizable way. When the LLM attempts to match the decompiler outputs to the human ground truth, it finds that all candidates have hallucinated distinct structural artifacts, applied different control-flow recovery heuristics, or inflated the code with boilerplate to such a degree that choosing a ``closest match'' becomes arbitrary. 
It could be position Bias, (since the ratio of wins after switch is 99/47 for Version B) but it could also be that the LLM judges are overwhelmed by the sheer unpredictability of two different decompiled code, leading to a default tie decision when no clear winner emerges.

\emph{Hex-Rays} manages to maintain the most stable performance across both blind and source-provided prompts, winning almost consistently across all prompts, suggesting a more balanced approach that preserves some of the original code's structure while still applying transformations for readability.

\emph{Ghidra}, on the other hand, consistently underperforms for \emph{deepseek-r1}, rarely winning and often being overshadowed by the other two decompilers, but it also perform slightly better when the raw code is provided. Furthermore the inconsistencies in the Ghidra results across the two models suggest that its output may be more sensitive to the specific evaluation criteria emphasized by each LLM, which further complicates the notion of a single ``best'' decompiler.

\begin{table}
\centering
\caption{Dogbolt evaluation winners by judge and prompt type.}\label{tab:dogbolt_winners_by_prompt}
\begin{tabular}{llrrrr}
\toprule
\textbf{Judge} & \textbf{Test Type} & \textbf{Tie} & \textbf{binary-ninja} & \textbf{ghidra} & \textbf{hex-rays} \\
\midrule
deepseek-r1 & AST + Source  & 25 & 11 &  6 & 14 \\
 & AST Blind     & 19 & 21 &  6 & 14 \\
 & Code + Source & 30 &  3 &  6 & 18 \\
 & Code Blind    & 18 & 16 &  9 & 17 \\
\midrule
qwen-3 & AST + Source  & 17 & 11 & 11 & 16 \\
 & AST Blind     & 14 & 18 & 11 & 17 \\
 & Code + Source & 23 &  5 & 13 & 15 \\
 & Code Blind    & 16 & 12 & 17 & 15 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Length Bias}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/dogbolt_length_bias.png}
    \caption{Distribution of winners across the four prompts for the dogbolt evaluation, showing length bias}\label{fig:dogbolt_length_bias}
\end{figure}

\Cref{fig:dogbolt_length_bias} further quantifies the drastic variations among the decompilers. The boxplots illustrate the delta length between the winning snippet and the losing alternatives. The presence of massive outlierssome exceeding a difference of $\pm 1000$ tokensproves that the tools generate code of vastly different lengths for the exact same binary function. 

While the medians hover near zero, we can observe a slight negative bias in the blind tests (\emph{winner} and \emph{winner\_ast}), meaning the LLM judges generally exhibit a slight preference for more concise code when evaluating purely for readability. However, in the source-provided tests (\emph{winner\_s} and \emph{winner\_ast\_s}), this distribution flattens or shifts slightly positive. 
When forced to prioritize fidelity, the models occasionally reward verbosity if the longer decompiler output happens to capture explicit logic or variable declarations that were present in the original source but optimized away by the more concise decompilers. 

\subsection{Dogbolt Results}

The evaluation of the Dogbolt dataset corroborates the core findings from the Ghidra \ac{PR} analysis, albeit on a much larger scale. While the Ghidra dataset compared minor, localized modifications between two commits of the same engine, the Dogbolt dataset tasks the \ac{LLM} judges with comparing entirely different decompilers (Binary Ninja, Hex-Rays, and Ghidra). Consequently, the judges have a significantly broader range of structural, lexical, and control-flow modifications to choose from. 

Despite this increased variance, the fundamental behaviors, biases, and inconsistencies of both \emph{qwen-3} and \emph{deepseek-r1} remain perfectly in line with our previous observations.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/dogbolt_ex1.png}
    \caption{Incoherent results from dogbolt with Qwen}\label{fig:dogbolt_ex1}
\end{figure}

In \Cref{fig:dogbolt_ex1} we can see a case of inconsistency driven by lexical abstraction, mirroring the vulnerabilities seen in the Ghidra evaluations. When \emph{qwen-3} evaluates the raw code (comparing Binary Ninja and Hex-Rays), it prefers Candidate A (Binary Ninja), praising its \texttt{do-while} loop as a ``semantic structure that aligns with high-level human thinking'' and criticizing Candidate B's \texttt{while(1)} with a \texttt{break} as less idiomatic. 
However, when the exact same logic is abstracted into an \ac{AST}, the model flips its decision to Candidate B. It suddenly contradicts its previous motivation, now claiming that the \texttt{while(1)} loop is a ``natural infinite loop'' that aligns with idiomatic C patterns, while dismissing Candidate A's \texttt{do-while} as ``tautological'' and ``assembly-like''.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/dogbolt_ex2.png}
    \caption{Coherent results from dogbolt with Qwen}\label{fig:dogbolt_ex2}
\end{figure}

Conversely, in \Cref{fig:dogbolt_ex2} the \ac{LLM} compares Ghidra against Hex-Rays. In this scenario, Hex-Rays generates a \texttt{while(1)} loop, whereas Ghidra produces a structure with a \texttt{do-while} loop nested inside multiple conditional checks and variable updates. 
\emph{Qwen-3} consistently rewards Hex-Rays across all four evaluation prompts (Code Blind, AST Blind, Code Fidelity, and AST Fidelity). It recognizes that avoiding artificial nesting and deep \texttt{if-else} cascades is a universal hallmark of human engineering, a judgment it can confidently make even when variable names are stripped away in the \ac{AST}.

Interestingly, the \ac{LLM} evaluates the same structural pattern as a negative in \Cref{fig:dogbolt_ex1} (where the \texttt{do-while} is penalized) but as a positive in \Cref{fig:dogbolt_ex2} (where the \texttt{do-while} is praised). This highlights the extreme sensitivity of the models to contextual cues.

\section{Correlation Perplexity \& LLM judge}

\subsection{Ghidra}

To determine whether the quantitative predictability of the decompiled code (measured via Perplexity) aligns with the qualitative ``human-likeness'' evaluations provided by our LLM judges, we conducted a Point-Biserial correlation analysis 
\footnote{The point-biserial correlation ($r_{pb}$) measures the strength and direction of the association between a continuous interval/ratio variable and a binary (dichotomous) variable, ranging from -1 to +1. As a special case of the Pearson correlation, it is commonly used to determine if a naturally occurring dichotomy (e.g., pass/fail, yes/no) relates to a continuous score~\cite{site:numiqoPointBiserialCorrelation}}. 
In this setup, the qualitative winner is treated as a binary variable (\texttt{PR} $= 1$, \texttt{BASE} $= 0$), and it is correlated against the $\Delta$ Perplexity ($\mathrm{PPL}_{\mathrm{PR}} - \mathrm{PPL}_{\mathrm{BASE}}$). 

Consequently, a \textit{negative} correlation coefficient ($r < 0$) would indicate the expected behavior: when the LLM judge prefers the \texttt{PR} branch (1), the $\Delta$ PPL is negative (meaning the \texttt{PR} code had a lower, ``better'' perplexity). 

\begin{table}[H]
\centering
\caption{Point-Biserial Correlation between LLM Judge Preference and $\Delta$Perplexity}\label{tab:ppl_correlation}
\begin{tabular}{lrrrrrr}
\toprule
\textbf{Model} & \textbf{Code $r$} & \textbf{Code $p$-value} & \textbf{N} & \textbf{AST $r$} & \textbf{AST $p$-value} & \textbf{N} \\
\midrule
qwen-3 & -0.0833 & 0.5232 & 61 & -0.1825 & 0.1147 & 76 \\
deepseek-r1 & -0.0625 & 0.5970 & 74 & -0.1329 & 0.2399 & 80 \\
\bottomrule
\end{tabular}
\end{table}

Looking at the raw code correlations (\emph{Code $r$}) in \Cref{tab:ppl_correlation}, we observe values extremely close to zero for both models ($-0.0833$ and $-0.0625$). Furthermore, the associated $p$-values ($> 0.5$) clearly indicate that these results are not statistically significant. This demonstrates a complete lack of correlation between raw code perplexity and qualitative \ac{LLM} judgment. 

This disconnect highlights a crucial insight: the features that make code ``predictable'' to an LLM's next-token-prediction engine (lower perplexity) are fundamentally different from the features that an LLM judge evaluates when prompted to look for ``human-like'' readability. As discussed in previous sections, a decompiler might generate highly repetitive, boilerplate-heavy code that drives perplexity down through token inflation, but an LLM judge will correctly penalize this exact same code for being verbose and mechanical.

When evaluating the abstracted \ac{AST} representations (\emph{AST $r$}), the negative correlation slightly strengthens for both \emph{qwen-3} ($-0.1825$) and \emph{deepseek-r1} ($-0.1329$). Additionally, the sample size (N) increases (as the number of TIEs decreases).

While the $p$-values ($0.1147$ and $0.2399$) still fall short of the threshold for statistical significance ($p < 0.05$ to confirm if the relationship exists in the population), the shift is notable. By removing the ``lexical noise'' and standardize the decompiler output artificially, the structural predictability of the \ac{AST} aligns slightly better with the structural judgment of the \ac{LLM}. However, the correlation remains weak overall.

Ultimately, this data confirms that \textbf{Perplexity is not a reliable proxy for human-likeness in decompiled code}. While perplexity effectively measures how well the code aligns with the statistical distribution of the model's training data, qualitative human-likeness is defined by higher-level engineering concepts, such as idiomatic control flow, logical grouping, and abstraction, which often introduce necessary structural ``surprises'' that inherently increase perplexity.

\subsection{Dogbolt}

We extended our correlation analysis to the Dogbolt dataset, where the \ac{LLM} judges compared outputs from different decompilers (e.g., Binary Ninja vs. Hex-Rays vs. Ghidra) in an A/B testing format. We binarized the qualitative winner (Candidate A $= 1$, Candidate B $= 0$) and correlated it against the $\Delta$ Perplexity ($\mathrm{PPL}_{\mathrm{A}} - \mathrm{PPL}_{\mathrm{B}}$). 

As established previously, if the LLM judge preferred the code that is ``most predictable'' to a language model, we would expect a \textit{negative} correlation (i.e., when A wins, A's perplexity is lower, making $\Delta$ PPL negative). 

\begin{table}[H]
\centering
\caption{Point-Biserial Correlation between LLM Judge Preference and $\Delta$Perplexity}\label{tab:dogbolt_ppl_correlation}
\begin{tabular}{lrrrrrr}
\toprule
\textbf{Model} & \textbf{Code $r$} & \textbf{Code $p$-value} & \textbf{N} & \textbf{AST $r$} & \textbf{AST $p$-value} & \textbf{N} \\
\midrule
qwen-3 & 0.1953 & 0.2040 & 44 & 0.2292 & 0.1255 & 46 \\
deepseek-r1 & 0.0352 & 0.8250 & 42 & 0.2283 & 0.1511 & 41 \\
\bottomrule
\end{tabular}
\end{table}

The data in~\vref{tab:dogbolt_ppl_correlation} reveals a striking phenomenon: the correlation coefficients (\textbf{Code $r$}) are \textit{positive}. A positive $r$ value ($0.1953$ for \emph{qwen-3} and $0.0352$ for \emph{deepseek-r1}) dictates that when the \ac{LLM} judge selects a candidate as the ``human-like'' winner, that candidate actually tends to have a \textit{higher} perplexity score than the loser. 

While the high $p$-values ($0.2040$ and $0.8250$) confirm that this is not a statistically significant linear rule, the inversion of the sign perfectly corroborates our findings in \Cref{sec:perplexitydog,sec:dogbolteva}. Decompilers that score high in qualitative human-likeness achieve this by utilizing diverse, domain-specific, and sometimes complex idioms that carry higher entropy. 
Conversely, decompilers with artificially low perplexity (by inflating the number of tokens) are penalized by the judges for being overly verbose and machine-like. 

When evaluating the \ac{AST} versions (AST $r$), the positive correlation strengthens noticeably to $\sim 0.229$ for both models, and the $p$-values improve ($0.1255$ and $0.1511$), though they still remain above the strict $0.05$ threshold for significance. 

This parallel strengthening across both \emph{qwen-3} and \emph{deepseek-r1} indicates that the preference for ``higher perplexity'' structures is not merely a lexical artifact. Even when the code is anonymized, the \ac{LLM} judges prefer abstracted syntactic structures that contain more variance and unpredictability over the repetitive, low-loss boilerplate generated by the most predictable decompilers.

The Dogbolt correlation data acts as a definitive counter-proof to the assumption that language models inherently prefer code that perfectly matches their own predictive distributions.
By analyzing the data, and the correlations, we saw that perplexity is not a sufficient measure of human-likeness;
Therefore, it should not be used as a standalone metric to measure the readability or human-likeness of a decompiler's output.

\section{Judge vs. Human Evaluation}

To validate the results, the biases, and inconsistencies observed in the \ac{LLM} judges, we conducted a human-subject experiment. We designed a blind survey using Google Forms, presenting real software developers with pairs of decompiled C code (labeled simply as ``Snippet A'' and ``Snippet B'') and asking them to identify which version appeared more natural, idiomatic, and ``human-like''.

The dataset for this quiz consisted of 21 carefully selected questions: 15 questions comparing Ghidra \emph{BASE} vs. \emph{PR} branches, and 6 questions comparing different decompilers from the Dogbolt dataset (Binary Ninja, Hex-Rays, and Ghidra). 
The snippets were strategically chosen to include a mix of scenarios: cases where the \ac{LLM} judges showed strong consensus, cases of severe inconsistency (flipping decisions between Raw Code and \ac{AST}), and cases where the models hallucinated structural differences (e.g., PR \#7253).

Responses were collected from 11 participants. We aggregated their votes to establish a human ``Ground Truth'' consensus for each question and compared it against the predictions made by \emph{qwen-3} and \emph{deepseek-r1}.

It's interesting to note that the human consensus was not always unanimous. In some cases, the votes were split, reflecting genuine ambiguity in the code's readability. However, in the majority of cases, a clear preference emerged, giving a mean alignment score of 78\% between the human choices (min 55\%, max 100\%).

\subsection{Quantitative Alignment}

To measure how well the \ac{LLM} judges proxy human intuition, we calculated an alignment score. A score of 1.0 indicates a perfect match between the model's winner and the human consensus, 0.0 indicates a complete mismatch, and 0.5 is awarded for partial matches (e.g., the model declared a \texttt{TIE}). 

\begin{table}[H]
\centering
\caption{Mean Alignment Score between LLM Judges and Human Consensus}\label{tab:human_alignment}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Raw Code Alignment} & \textbf{AST Alignment} \\
\midrule
qwen-3 & $\sim$64.28\% & $\sim$69.04\% \\
deepseek-r1 & $\sim$73.80\% & $\sim$73.80\% \\
\bottomrule
\end{tabular}
\end{table}

The results in \Cref{tab:human_alignment} provide two critical insights into the capabilities of \ac{LLM}s as code judges:

\begin{enumerate}
    \item \textbf{DeepSeek-R1 is superior for human judgment:} With a consistent $\sim$73.8\% alignment, \emph{deepseek-r1} demonstrates a much more robust understanding of human programming idioms than \emph{qwen-3}. It is important to note that the same results for raw code and \ac{AST} code are identical just in the final number as the intermedial results are different.
    \item \textbf{AST Abstraction improves Qwen-3:} Strikingly, \emph{qwen-3}'s alignment with human developers increases from $\sim$64.28\% to $\sim$69.04\% when evaluating the anonymized \ac{AST}. This means that probably \emph{qwen-3} suffers from a light lexical bias.
\end{enumerate}

Unlike the \ac{LLM} pipeline, the human study was conducted exclusively on \emph{raw code} snippets and did not include an \ac{AST}-anonymized condition. This was a deliberate design choice: we assumed human reviewers would be less affected by the lexical anchoring effects observed in the models, and we wanted to preserve a realistic reading experience close to practical reverse-engineering workflows. In addition, several diffs were extremely subtle and already challenging in raw form. In some questions, the only effective change was minimal (e.g., the addition of a single explicit cast on a variable), making any further abstraction likely to remove the remaining discriminative signal and turn the comparison into near-random guessing.

We intentionally included snippets from PR \#7253 (where only the vertical order of \texttt{switch} cases was altered). Previously, we noted that the \ac{LLM} judges, especially \emph{qwen-3}, often hallucinated non-existent structural improvements to justify picking a winner in these scenarios. 
Human developers, conversely, quickly recognized the logical equivalence. Human votes were often split, or respondents correctly noted that the differences were purely stylistic, heavily contrasting with the models' tendency to confidently invent false technical justifications.

The final 6 questions of the quiz pitted the outputs of Binary Ninja, Hex-Rays, and Ghidra against each other. The human consensus overwhelmingly favored Binary Ninja and Hex-Rays over Ghidra. 
This definitively validates the \ac{LLM} evaluations from the Dogbolt dataset. It is important to note that the human votes were not unanimous, and in some cases, Ghidra's output was preferred by a 40+\% of respondents. This reflects the inherent subjectivity in code readability and the fact that different developers may have varying preferences for certain coding styles or patterns. However, the overall trend clearly supports the LLM judges' conclusions about the relative quality of the decompilers' outputs.

\subsection{Perplexity correlation}

We also calculated the Point-Biserial correlation between the human consensus winners and the $\Delta$ Perplexity for the raw code snippets used in the quiz, for both the raw code and the \ac{AST} conditions. This analysis aimed to determine whether the human preferences align with the same perplexity trends observed in the LLM judges.
\begin{table}[H]
\centering
\caption{Point-Biserial correlation between Human Consensus and $\Delta$Perplexity (quiz subset).}\label{tab:human_ppl_correlation}
\begin{tabular}{lrrrrrr}
\toprule
\textbf{Evaluator} & \textbf{Code $r$} & \textbf{Code $p$-value} & \textbf{N} & \textbf{AST $r$} & \textbf{AST $p$-value} & \textbf{N} \\
\midrule
Human Consensus & 0.3278 & 0.2330 & 15 & 0.4609 & 0.0838 & 15 \\
\bottomrule
\end{tabular}
\end{table}

As shown in Table~\ref{tab:human_ppl_correlation}, the correlation with human consensus is positive in both settings, and stronger for the \ac{AST} condition ($r=0.4609$) than for raw code ($r=0.3278$). 
This direction is coherent with the previous analyses: snippets judged as more human-like are not associated with lower perplexity, but tend to show equal or higher perplexity. However, with $N=15$, neither result is statistically significant at $\alpha=0.05$ ($p=0.2330$ for code, $p=0.0838$ for \ac{AST}), so these values should be interpreted as an exploratory trend rather than conclusive evidence.

\section{Discussion}

The results presented in this study offer a critical perspective on the use of Large Language Models (LLMs) for evaluating the quality and readability of decompiled code.

First, the quantitative analysis clearly shows that perplexity is not a reliable proxy for code human-likeness. Contrary to the initial intuition, the original source code tends to have systematically higher perplexity values than decompiled versions. This phenomenon is rooted in the nature of human-written code, which is characterized by varied stylistic choices, domain-specific naming conventions, and general high-entropy burstiness. In contrast, decompilers such as Ghidra tend to produce rigid syntactic structures, repetitive patterns, and verbose constructs. This structural and lexical token inflation dilutes the cross-entropy mean, artificially lowering overall perplexity and making mechanical code appear more predictable to the model than human code.

Regarding the use of LLMs as qualitative judges, the two evaluated architectures showed distinct capabilities and limitations. \emph{Deepseek-r1} proved to be clearly superior in approximating human judgment, reaching an alignment rate of about 73.8\%. \emph{Qwen-3}, by contrast, showed significant weaknesses: a marked tendency toward verbosity that harms time and computational efficiency, and strong susceptibility to lexical bias, which improves only when code is abstracted into \ac{AST}. Moreover, both models showed the risk of analytical hallucinations. When faced with logically equivalent code variants, or variants differing only in irrelevant stylistic details (such as switch-case ordering), LLMs sometimes invented nonexistent technical justifications to support a preferred choice.

The human-developer experiment played a crucial role in validating our metrics and isolating LLM limitations. Humans proved much more resilient to lexical noise and better at ignoring non-functional stylistic differences. However, human consensus broadly confirmed the models macro-level verdicts, validating, for example, the preference for the readability of Binary Ninja and Hex-Rays outputs over Ghidras output.

Finally, it is important to emphasize that the models evaluated in this study are relatively small in terms of parameter count. This work provides a starting point and a baseline for developing future automated evaluation frameworks for reverse engineering. Although computational bottlenecks emerged and some architectural biases remain, the LLM-as-a-Judge approach is promising. Future developments based on larger foundation models, specialized fine-tuning, or more advanced abstraction pipelines may reduce hallucinations and make these tools essential standards for the continuous improvement of decompilers.
