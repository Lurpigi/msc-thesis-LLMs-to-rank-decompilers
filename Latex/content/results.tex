\chapter{Results}\label{ch:results}



\section{LLM performance}



\section{Perplexity as a Metric for ``Humanness''}

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{img/ppl_distribution.png}
    \caption{On the left perplexity values across original source code, base code, and pr code. On the right perplexity values across abstracted representations of the same functions.}\label{fig:ppl_distribution}
\end{figure}

In \Cref{fig:ppl_distribution} we can see the distribution of perplexity values for the original source code, the base code, and the pr code, as well as their abstracted representations.
We can observe that the original source code has generally higher perplexity values compared to the decompiled versions,
which is unexpected since the original source code should be more ``natural'' and predictable than the decompiled output.
This suggests that the decompilation process may introduce certain patterns or structures that are more familiar to the language model, leading to lower perplexity scores, while the original source code may contain more variability and less predictable constructs that result in higher perplexity.

Another observation is that the abstracted representations of the code (right side of the figure) tend to have higher perplexity values compared to their original counterparts (left side of the figure).
This is likely because the abstraction process removes specific identifiers and literals, which can make the code less predictable and more ``surprising'' to the language model, 
but even in this case, the original source code still has higher perplexity than the decompiled versions, reinforcing the idea that the decompilation process may be introducing more predictable patterns into the code.

... study for trying to know why this happens ...



\section{LLM-as-a-Judge Evaluation}

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{img/winner_distribution.png}
    \caption{Distribution of winners across all evaluated models.}\label{fig:winner_distribution}
\end{figure}

In \Cref{fig:winner_distribution} we can see the distribution of winners across all evaluated models, based on the qualitative judgments of the \ac{LLM} as a judge.
The two models, Qwen3 and DeepSeek, have a significant number of Ties, but they also show a balanced distribution of wins between the base code and the pr code.
We can see a bin for ``Error'' winners as well, this happens primarly when the model exceed the token limit of 4096 tokens (reasonable limit set by us), since they are reasoning models (they create a context with the generate tokens inside $</think>$ tags) sometimes the context becomes too large (often because they start to repeating thoughts) and they finish the limit without giving inside the response a clear winner (e.g., ``Winner'':``X'').

We previously said that a ``Tie'' is when the model judges always the same result regardless the switch of the base and pr code.
We have a significant number of Ties for all models, watching the result we can observe a ratio of $8.7$ ($618/71$) beetween the number of times that the \ac{LLM} prefers the \ac{PR} version regardless the content.
This suggests a strong bias towards the ``newer'' code, even without telling in the prompt that the Diff code is the newer one, this bias could be due to the fact that Diff code is often more recent and may contain improvements or bug fixes that make it more appealing to the model, or it could be a bias in the model itself towards preferring changes.
Unfortunatly for highlight changes through versions and save on the context window the Diff method is the most convenient, forcing us to Do not count tie-break results in our analysis, giving up almost half of the results.


\subsection{PR \#8628}

\subsection{PR \#8587}

\subsection{PR \#8161}

\subsection{PR \#7253}

\subsection{PR \#6722}



\section{Vs Human Evaluation}


\section{Discussion}

