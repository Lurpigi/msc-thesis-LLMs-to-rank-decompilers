\chapter{Results}\label{ch:results}



\section{LLM performance}

An empirical evaluation of the \emph{qwen-3} and \emph{deepseek-r1} models was conducted across two distinct tasks: evaluating the ``Humanity'' of decompiled code via generation (\ac{LLM} Judge) and calculating code perplexity (Score). The performance was measured in terms of execution time and peak \ac{VRAM} usage.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/llm_time.png}
    \caption{Execution time for each evaluated model.}\label{fig:llm_time}
\end{figure}

As shown in \Cref{fig:llm_time}, there is a stark dichotomy in execution times between the two operations. The \texttt{score} operation is inherently fast, with both models completing most passes in under 50 seconds. However, the \texttt{generate} operation exhibits significantly higher and more erratic execution times, particularly for \emph{qwen-3}, which frequently exceeds 200 seconds and reaches up to nearly 400 seconds. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/llm_vram.png}
    \caption{Peak VRAM for each evaluated model.}\label{fig:llm_vram}
\end{figure}

\Cref{fig:llm_vram} presents a counter-intuitive finding: calculating perplexity (\texttt{score}) requires a higher median and maximum Peak VRAM than generating text (\emph{generate}). While generation heavily utilizes the KV cache over time, perplexity calculations typically require processing the entire sequence in a single forward pass to compute logits, leading to massive activation memory spikes~\cite{atmer2025prefillvsdecodebottlenecks}.

\subsection{Generation}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/llm_gen.png}
    \caption{Generated tokens vs execution time and peak VRAM for each evaluated model.}\label{fig:llm_gen}
\end{figure}

A closer inspection of \Cref{fig:llm_gen} reveals the root cause of the discrepancy between the two models. The scatter plot for ``Generated Tokens vs Execution Time'' shows a near-perfect linear correlation, confirming that generation time is bottlenecked entirely by the length of the model's output rather than the input context. 
Critically, \emph{qwen-3} routinely generates between 2,000 and 4,000+ tokens during the evaluation task and often reaches our limit of 4096 new tokens. 

Given that the task is an \ac{LLM} Judge producing an evaluation of code ``humanity,'' a rationale exceeding 4,000 tokens is highly suspicious. This suggests that \emph{qwen-3} may be suffering from severe verbosity, repeating the input code, or failing to trigger stop tokens appropriately. 
In contrast, \emph{deepseek-r1} is significantly more concise, rarely exceeding 2,500 tokens; it can still reach the 4096-token limit, but much less frequently, which translates directly to more predictable and efficient execution times.


\begin{wrapfigure}{r}{0.55\textwidth}
    \centering
    \includegraphics[width=0.45\textwidth]{img/qwen_stuck.png}
    \caption{Example of repetitions in \emph{qwen-3} output}\label{fig:stuck}
\end{wrapfigure}

In \Cref{fig:stuck} we can see an example of the output of \emph{qwen-3} for a specific function, we can see that the model is repeating the same pattern of thoughts, which is a common symptom of verbosity and lack of proper stop token triggering. 
This behavior not only leads to unnecessarily long outputs but also significantly increases execution time and VRAM usage, as the model continues to generate tokens without producing meaningful content.
This verbosity issue is a critical flaw for the \ac{LLM} Judge task, as it undermines the model's ability to provide concise and relevant evaluations of code ``humanity,'' and it also creates significant computational inefficiencies that could be prohibitive in larger-scale evaluations or with longer input contexts, 
maybe could be mitigated by implementing stricter stop conditions or by fine-tuning the model to better understand the task requirements and avoid unnecessary verbosity but it will need further investigation.

Furthermore, analyzing the memory consumption in the bottom row of \Cref{fig:llm_gen} reveals a distinct behavior regarding how \ac{VRAM} scales during the generation process. 
The ``Input Tokens vs Peak VRAM'' subplot demonstrates a strange phenomenon: Under 2500 tokens we can observe a cloud of data points but exceeding that number of token we observe an uninterrupted linear relationship, this line is also the minimum boundary of the data. In the line, \emph{deepseek-r1} exhibits a slightly higher memory scaling compared to \emph{qwen-3}, consuming marginally more \ac{VRAM} for the same input length. 

\subsection{Score}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/llm_score.png}
    \caption{Score vs execution time and peak VRAM for each evaluated model.}\label{fig:llm_score}
\end{figure}

The linear scaling of VRAM during the \emph{score} operation (\Cref{fig:llm_score}, right panel) is a major scalability risk. At roughly 3,800 input tokens, both models push past 12 GB of \ac{VRAM}. 
If the dataset contains an enormous larger input (e.g., 8k to 32k tokens), the current perplexity scoring methodology will inevitably result in \ac{OOM} failures on standard consumer GPUs.

Fortunately, the \emph{generate} operation does not exhibit this issue, as it processes tokens sequentially and can leverage the KV cache to manage memory more efficiently.

The duration is mostly flat (0--20 seconds), but spikes occurfor both models. This implies either batch-processing artifacts or that the evaluation dataset contains many decompiled functions of the exact same token length that trigger specific internal computational bottlenecks.
A possible explanation is that runtime is influenced not only by input length, but also by the distribution of per-token loss values. This hypothesis requires additional targeted experiments, and further studies will be necessary to confirm it.


While both models are capable of performing the required tasks, \emph{deepseek-r1} is objectively better suited for the \ac{LLM} Judge (\emph{generate}) role due to its restraint and conciseness, avoiding the computationally expensive verbosity traps that plague \emph{qwen-3}. 
However, for the perplexity (\emph{score}) task, the architecture of the operation itself poses a severe hardware bottleneck that scales poorly with larger decompiler outputs.

\section{Perplexity as a Metric for ``Humanness''}\label{sec:perplexityres}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/ppl_distribution.png}
    \caption{On the left perplexity values across original source code, base code, and pr code. On the right perplexity values across abstracted representations of the same functions.}\label{fig:ppl_distribution}
\end{figure}

In \Cref{fig:ppl_distribution} we can see the distribution of perplexity values for the original source code, the base code, and the pr code, as well as their abstracted representations.
We can observe that the original source code has generally higher perplexity values compared to the decompiled versions,
which is unexpected since the original source code should be more ``natural'' and predictable than the decompiled output.
This suggests that the decompilation process may introduce certain patterns or structures that are more familiar to the language model, leading to lower perplexity scores, while the original source code may contain more variability and less predictable constructs that result in higher perplexity.
We can observe that the perplexity distribution calculated with \emph{deepseek-r1} is generally higher than the one calculated with \emph{qwen-3}, this is consistent with the previous observation that \emph{qwen-3}.

Another observation is that the abstracted representations of the code (right side of the figure) tend to have higher perplexity values compared to their original counterparts (left side of the figure).
This is likely because the abstraction process removes specific identifiers and literals, which can make the code less predictable and more ``surprising'' to the language model, 
but even in this case, the original source code still has higher perplexity than the decompiled versions, reinforcing the idea that the decompilation process may be introducing more predictable patterns into the code.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.99\textwidth]{img/global_loss.png}
    \caption{Density distribution of cross-entropy loss values for all tokens in the original source code and the decompiled versions.}\label{fig:global_loss}
\end{figure}

In \Cref{fig:global_loss} we can observe that the original code exhibits a wider, slightly flatter distribution with a higher mean loss compared to the decompiled output, which is characterized by a sharper peak shifted towards zero.
This visualization highlights a counter-intuitive phenomenon: despite the original source code being the ``human ground truth'', the language model finds the decompiled code significantly more predictable.

We attribute this behavior to Token Inflation and Loss Dilution:
As indicated by the token counts in the figure (e.g., $\sim$168k tokens for decompiled vs $\sim$120k for original source), the decompilation process introduces a substantial \emph{token inflation}.
Ghidra generate verbose, explicit code full of boilerplate structures (e.g., redundant casts, standard control flow patterns, explicit initializations, and restricted vocabulary). These pattern tokens are syntactically rigid and in a context where are used, they are easy to predict for the model, leading to a large number of tokens with very low loss values (close to zero). 
Their sheer volume effectively dilutes the mean loss, artificially lowering the overall perplexity score compared to the denser, more information-rich human code.
In contrast, the original source code reflects human authorship, which includes domain-specific naming conventions, creative syntactic choices, and stylistic variability. This ``human entropy'' flattens the density curve and shifts the mean loss to the right, as the model is more frequently ``surprised'' by the programmer's unique choices compared to the machine's standardized output.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.99\textwidth]{img/global_loss_ast.png}
    \caption{Density distribution of cross-entropy loss values for all tokens in the original source code and the decompiled versions.}\label{fig:global_loss_ast}
\end{figure}

In \Cref{fig:global_loss_ast} we can see the \ac{AST} version of the previous analysis, where we abstracted away variable names and literals to focus on the structural aspects of the code.
Comparing these results with the previous analysis on raw code tokens, we observe two critical phenomena:

\begin{enumerate}
    \item \textbf{Persistence of structural inflation:} Even in the anonymized form, the \textit{token inflation} remains significant. The decompiled AST contains $\sim$107k tokens compared to $\sim$70k for the original source ($\sim$+52\%). 
        This confirms that the verbosity of the decompiled code is not merely lexical (e.g., long variable names) but syntactical. The decompiler introduces explicit casts, redundant blocks, and verbose control flow structures that persist even after anonymization, continuing to dilute the mean loss with predictable tokens.
    \item \textbf{The closer entropy:}
    Unlike the raw code analysis, where the gap between the distributions was pronounced, the AST distributions for Source and Decompiled code are more similar in shape. The difference in Mean Loss has narrowed (e.g., for \emph{deepseek-r1}, the gap represents only $\sim 0.047$, compared to larger margins in the raw code).

    This convergence suggests that the \emph{lexical entropy} was a discriminator in the previous analysis.
    \begin{itemize}
        \item In the raw code, human-written names provided high variability (surprisal), while decompiled names were generic.
        \item In the AST version, the anonymization process effectively ``standardizes'' the two codes; Consequently, the source code becomes more predictable by the model.
    \end{itemize}
\end{enumerate}


\begin{wrapfigure}{r}{0.55\textwidth}
    \centering
    \includegraphics[width=0.45\textwidth]{img/example_loss_func.png}
    \caption{Loss values for \texttt{xls\_parseWorkBook} in the decompiled base code}\label{fig:example_loss_func}
\end{wrapfigure}

The token-level analisys allow us to verify the tokens that contribute the most to the loss, and consequently to the perplexity, in a specific function.
We can observe that the token itself is not the only factor that contributes to the loss, but especially the context in which it is used.
For example, in \Cref{fig:example_loss_func} we can see the loss values for the token ``\texttt{LAB}'', which is a common label used in the decompiled code to indicate jump targets.
This token has a low loss value (1.664) when it appears after the \texttt{do{}while loop}, but it has a much higher loss value (11.812) when it appears inside the \texttt{if} allowing a flow branch to ignore the condition and entering the scope without checks.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.99\textwidth]{img/example_loss.png}
    \caption{Example of loss values for a function in the decompiled code.}\label{fig:example_loss}
\end{figure}

Obviously every function has a different distribution of loss values, and some functions may be more ``natural'' than others.
In \Cref{fig:example_loss} we can see an example of the loss values for \texttt{xls\_parseWorkBook} function from the DeepSeek analisys, 
remembering that the perplexity is calculated as the exponential of the mean loss~\ref{sec:perplexity}, we can see that the original source code has a perplexity of $\sim 3.06$ (mean loss $\sim 1.12$), while the decompiled base version has a perplexity of $\sim 2.4$ (mean loss $\sim 0.89$).
We can see that the max values for the loss are higher for the decompiled version, in contrary to the global distribution where the decompiled code had a sharper peak towards zero, but then when we look at the anonymized version of the same function, the original source code became the one with a higher max loss value than the decompiled version.
However, the anonymization process manages to bring the two versions significantly closer, both in terms of loss values and, consequently, perplexity.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.99\textwidth]{img/std_dev_loss.png}
    \caption{Standard deviation of loss values across all functions in the normal version and anonymized version.}\label{fig:std_dev_loss}
\end{figure}

In \Cref{fig:std_dev_loss} we investigate the \emph{Entropy Variance} of the code by analyzing the standard deviation of the token loss.
While the mean loss indicates the average predictability, the standard deviation reveals the \emph{dynamic range} of the code complexity.

We can observe a clear trend across both models:
\begin{itemize}
    \item \textbf{Difference in Variance:} The original source code consistently exhibits higher standard deviation compared to the decompiled versions. This confirms that human-written code is characterized by \textit{burstiness}: it alternates between low-entropy boilerplate and high-entropy domain-specific logic. The language model struggles to predict this rhythm, leading to fluctuating loss values.
    The decompiled code shows significantly lower variance. This reflects the \textit{monotonicity} of machine-generated code. In our case Ghidra applies consistent transformation rules throughout the binary, resulting in more predictabile results.
    
    \item \textbf{The Lexical Factor:} Comparing the code panel with the \ac{AST} one, we notice that the gap between Source and Decompiled shrinks significantly in the AST representation. This implies that a substantial portion of the entropy variance in human code is driven by \emph{lexical choices} (variable naming and literals) as we predicted. Once these are removed, the structural variability of human code is only marginally higher than that of the decompiled code.
\end{itemize}

This result reinforces our conclusion: in our case (Ghidra vs Source) Human-Likeness is not defined by raw predictability (where the machine wins), but by the \textbf{variance of unpredictability}. A ``natural'' code signature is one that surprises the model in inconsistent, context-dependent bursts, rather than being uniformly predictable.
Meanwhile for the anonymized code, the gap in variance is smaller but it still exists, suggesting that even at the structural level, human code retains a degree of unpredictability that machine-generated code lacks.

\subsection{Other decompilers}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/ppl_distribution_dogbolt.png}
    \caption{Perplexity values for the original source code and the decompiled versions from other decompilers.}\label{fig:ppl_distribution_dogbolt}
\end{figure}

The dogbolt analysis allows us to compare the perplexity values of the decompiled versions from other decompilers (Hex-Rays and Binary Ninja) with the original source code and the Ghidra decompilation, with a subset of the original database (we cannot compare in an absolute way the plot in \Cref{fig:ppl_distribution} with these but only the relative ordering of the distributions).
We can see in \Cref{fig:ppl_distribution_dogbolt} that the perplexity values for the decompiled versions resulted by the dogbolt analysis, are generally in line with the Ghidra ones.
we can see that the original source code still has higher perplexity values compared to the decompiled versions, and the abstracted representations of the code still tend to have higher perplexity values compared to their original counterparts.
Another key observation is that the binary ninja has a perplexity distribution that is more similar to the original source code compared to the Ghidra decompilation, meanwhile Hex-Rays has a distribution in the middle between the other two decompilers, then we can observe that every distribution with \emph{deepseek-r1} has a higher mean perplexity than the same distribution with \emph{qwen-3} like we seen previously.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/dogbolt_loss.png}
    \caption{Loss values for the original source code and the decompiled versions from other decompilers. Code and Abstract}\label{fig:dogbolt_loss}
\end{figure}

The density plots in \Cref{fig:dogbolt_loss} confirm the phenomena observed in the previous sections across both LLM judges. Focusing on the raw \emph{Code Tokens} (top row), we can observe a distinct hierarchy in predictability:

\begin{itemize}
    \item \textbf{Ghidra} exhibits the sharpest peak near zero and the lowest mean loss ($\sim 1.09$ for DeepSeek-R1 and $\sim 0.90$ for Qwen-3). This reaffirms its tendency to generate highly rigid and predictable code.
    \item \textbf{Hex-Rays} follows closely, showing a similar sharp peak but with a slightly higher mean loss and a much larger token count (over 7,000 tokens in this sample), in particular with the \emph{deepseek-r1} while with \emph{qwen-3} we can see that the mean is the same as Ghidra but with a density at peak higher than the others (as the Max loss is much higher than Ghidra). The massive token inflation in Hex-Rays suggests extremely verbose syntactic and lexical choices that artificially dilute the cross-entropy loss.
    \item \textbf{Binary Ninja} stands out as the decompiler that most closely approximates the original source code. Its loss distribution is flatter, and its mean loss ($\sim 1.33$ for DeepSeek-R1) is significantly closer to the original source ($\sim 1.60$) than the other tools. It also exhibits less token inflation compared to Ghidra and Hex-Rays with a Max loss closer to the original source.
\end{itemize}

Meanwhile in the \emph{AST} plots, where identifiers and literals are abstracted, we observe the expected flattening of all distributions. By stripping away lexical entropy, the gap between human-written code and machine-generated code narrows. Notably, the massive token inflation seen in Hex-Rays raw code drops significantly during AST abstraction (from 7,034 down to 3,869 tokens), 
indicating that a vast majority of its predictability stems from highly repetitive lexical tokens, types, and literal declarations rather than purely structural blocks. Even in the abstracted form, Binary Ninja maintains the distribution shape most similar to the original source code and with Hex-Rays having less tokens, the density at peak now is less than the Ghidra one for \emph{qwen-3}. 
Finally, comparing the two models across all subplots, we note that while \emph{qwen-3} consistently produces lower absolute loss values (higher confidence/predictability) than \emph{deepseek-r1}, both models almost perfectly agree (except for the code tokens for \emph{qwen-3} where Hex-Rays have a higher peak density than Ghidra) on the relative ordering of the distributions. Original source code always retains the highest mean loss and widest variance, followed by Binary Ninja, Hex-Rays, and finally Ghidra. 

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/hex-ray_loss.png}
        \caption{Heatmap of loss values for Hex-Rays decompilation with model \emph{qwen-3}}\label{fig:hex-ray_loss}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/hex-ray_loss2.png}
        \caption{Heatmap of loss values for Hex-Rays decompilation with model \emph{deepseek-r1}}\label{fig:hex-ray_loss2}
    \end{minipage}
\end{figure}

A token-level analysis of the Hex-Rays decompilation (e.g., \Cref{fig:hex-ray_loss}) for the model \emph{qwen-3}, reveals that Hex-Rays generates comments and other repetitive tokens that are highly predictable. The most predictable tokens are often those associated with those artefacts of the decompilation process. 
These tokens create a dense cluster of low-loss values that significantly lower the overall perplexity score, despite the presence of more complex and less predictable tokens elsewhere in the code.

When we look at the same decompilation with the model \emph{deepseek-r1} (\Cref{fig:hex-ray_loss2}), we can see that the loss values are generally higher, but the distribution of low-loss tokens is still present, confirming that the anonimization process is a key factor in the analysis of the perplexity, as it standardizes the code through the removal of personalized identifiers, making the predictability of the code more dependent on its structural patterns rather than on specific lexical choices.

\section{LLM-as-a-Judge Evaluation}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/winner_distribution.png}
    \caption{Distribution of winners across all evaluated models.}\label{fig:winner_distribution}
\end{figure}

\md{Aggiornare valori}

In \Cref{fig:winner_distribution} we can see the distribution of winners across all evaluated models, based on the qualitative judgments of the \ac{LLM} as a judge.
The two models, Qwen3 and DeepSeek, have a significant number of Ties, but they also show a balanced distribution of wins between the base code and the pr code.
We can see a bin for ``Error'' winners as well, this happens primarly when the model exceed the token limit of 4096 tokens (reasonable limit set by us), since they are reasoning models (they create a context with the generate tokens inside $</think>$ tags) sometimes the context becomes too large (often because they start to repeating thoughts) and they finish the limit without giving inside the response a clear winner (e.g., ``Winner'':``X'').

We previously said that a ``Tie'' is when the model judges always the same result regardless the switch of the base and pr code.
We have a significant number of Ties for all models, watching the result we can observe a ratio of $8.7$ ($618/71$) beetween the number of times that the \ac{LLM} prefers the \ac{PR} version regardless the content.
This suggests a strong bias towards the ``newer'' code, even without telling in the prompt that the Diff code is the newer one, this bias could be due to the fact that Diff code is often more recent and may contain improvements or bug fixes that make it more appealing to the model, or it could be a bias in the model itself towards preferring changes.
Unfortunatly for highlight changes through versions and save on the context window the Diff method is the most convenient, forcing us to Do not count tie-break results in our analysis, giving up almost half of the results.


\subsection{PR \#8628}
This \ac{PR} improves handling of constant subtractions that are rewritten during decompilation as additions with negative immediates (e.g., \texttt{x + -0x1a} $\rightarrow$ \texttt{x - 0x1a}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8628/preference.png}
    \caption{Distribution of winners across the four prompts for PR \#8628}\label{fig:8628_winner_distribution}
\end{figure}

As shown in \Cref{fig:8628_winner_distribution}, the qualitative evaluation strongly favors the \ac{PR} branch. In the ``Code Blind (Human-Likeness)'' and ``AST Blind (Structural Humanity)'' assessments, the \ac{PR} version overwhelmingly wins against the \texttt{BASE} branch across both \emph{deepseek-r1} and \emph{qwen-3} models, often exceeding a 60\% win rate. This indicates that rewriting expressions to use explicit subtraction rather than adding a negative immediate is universally recognized by the models as a more natural, human-readable idiom.

However, when evaluating fidelity to the original source code (``Code Fidelity'' and ``AST Fidelity''), the margin of victory for the \ac{PR} branch shrinks, and the number of ``TIE'' outcomes increases significantly. This suggests that while the \ac{PR}'s output is structurally cleaner, the original source code may contain diverse expressions that do not perfectly align with either the \texttt{BASE} or the \ac{PR} decompilation, causing the models to frequently declare a tie.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8628/consistency.png}
    \caption{Consistency across the prompts for evaluating ``humanity'' for PR \#8628}\label{fig:8628_consistency}
\end{figure}

To further investigate the robustness of these judgments, we analyzed the consistency of the models' decisions between evaluating raw code and evaluating its abstracted \ac{AST} representation (\Cref{fig:8628_consistency}). \emph{Deepseek-r1} shows 100\% consistency, meaning it never changed its preference between the raw code and the \ac{AST}. In contrast, \emph{qwen-3} exhibits an inconsistency rate of approximately 14\% (two times).
These two inconsistencies occur in the files \texttt{task-readstat\_sav\_parse\_date} and \texttt{task-readstat\_readstat\_parse\_por}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8628/ex1.png}
    \caption{Inconsistency in file task-readstat\_sav\_parse\_date-O3}\label{fig:8628_ex1}
\end{figure}

In the first inconsistent case (\Cref{fig:8628_ex1}), the \emph{qwen-3} model shifts its preference based on the presence of parentheses. When evaluating the raw code, it praises the \ac{PR} for using parentheses to group arithmetic operations (``\texttt{((type)id * 2 - 2)}''), arguing it makes the intent clearer and aligns with human engineering practices, It does not talk about the sum of negative number but rely his judgment on parenthesis. However, when evaluating the \ac{AST}, the model flips its vote to the \texttt{BASE} version. It suddenly argues that the extra parentheses introduced in the \ac{PR} feel ``mechanically generated'' and that the \texttt{BASE} version is more idiomatic because it trusts operator precedence, againg ignoring the sum of negative number. 
We can see that in this case, the model put in priority the parentheses over the sum of negative number. But the fact that the model is not consistent in its judgment when the parentheses are removed in the \ac{AST} evaluation, suggests that it may be overfitting to superficial cues rather than truly understanding the underlying code structure and semantics.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8628/ex2.png}
    \caption{Inconsistency in file task-readstat\_readstat\_parse\_por-O0}\label{fig:8628_ex2}
\end{figure}

The second inconsistency (\Cref{fig:8628_ex2}) highlights a conflict regarding type casting and literal suffixes. In the raw code evaluation, \emph{qwen-3} prefers the \texttt{BASE} version because it uses a direct comparison with \texttt{-1}, calling the \ac{PR}'s use of an unsigned cast (\texttt{-1U}) an unnecessary complication that obscures intent. 
Yet, when evaluating the \ac{AST}, the model flips to favor the \ac{PR}. In this abstracted view, it argues that the explicit unsigned suffix (\texttt{U}) improves clarity and correctness, avoiding signed/unsigned ambiguity. 
This flip suggests that the model's judgment is heavily influenced by surface-level syntax rather than a deeper understanding of the code's semantics. The presence of the unsigned suffix in the \ac{PR} version may have triggered a bias in the model when evaluating the raw code, leading it to view the \texttt{BASE} version as more straightforward. However, when the syntax is abstracted away in the \ac{AST}, the model's preference shifts, indicating that it may be overfitting to specific tokens or patterns rather than consistently evaluating the underlying code quality and readability.

Despite these edge cases of inconsistency, the models generally demonstrate a strong ability to recognize the \ac{PR}'s structural improvements regardless of the lexical context. \Cref{fig:8628_ex3} provides a clear example of this robust evaluation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8628/ex3.png}
    \caption{Consistency in evaluating arithmetic expressions}\label{fig:8628_ex3}
\end{figure}

In this instance, \emph{qwen-3} correctly identifies the core structural improvement in both the raw code and the \ac{AST} representation. When evaluating the raw code, it notes that subtracting a positive integer (e.g., \texttt{iVar3 - 1}) is a standard decrement pattern and significantly more idiomatic than adding a negative value. When transitioning to the \ac{AST} evaluation, the model maintains this exact reasoning, explicitly favoring the abstracted \texttt{id - 1} over \texttt{id + -1}. 

This demonstrates that when a structural transformation is sufficiently distinct and maps clearly to standard human programming conventions (like direct mathematical notation), the model's judgment remains stable. The removal of specific variable names and literal values during the \ac{AST} anonymization does not disrupt its ability to identify the more human-like syntactic pattern.


\subsection{PR \#8587}

This \ac{PR} focuses on automatically detecting and correcting one-based indexing patterns for spacebase constants (e.g., \texttt{*(undefined *)((long)i * 0x30 + 0xaddr + (long)j * 4)} $\rightarrow$ \texttt{globalArray[(long)i + -1].field[j]}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8587/preference.png}
    \caption{Distribution of winners across the four prompts for PR \#8587}\label{fig:8587_winner_distribution}
\end{figure}

As seen in \Cref{fig:8587_winner_distribution}, the results for the raw code blind evaluation are exceptionally decisive: both \emph{deepseek-r1} and \emph{qwen-3} unanimously prefer the \ac{PR} output 100\% of the time. Transforming obscure, hardcoded pointer arithmetic into clean array indexing is universally recognized by the models as a massive improvement in human readability. 

However, a notable shift occurs when examining the \ac{AST} Fidelity evaluations (bottom right). When forced to compare the abstracted code against the abstracted original source, the ``TIE'' outcome becomes the dominant result for both models. This suggests that while the \ac{PR} produces much cleaner C code, the original source code's structure might still differ slightly from Ghidra's output, or the abstraction process obscures the specific array-indexing improvements, leading the models to find both versions equally distant from the original blueprint.

This \ac{PR} in particular only modifies 7 functions from the base branch.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8587/consistency.png}
    \caption{Consistency across the prompts for evaluating ``humanity'' for PR \#8587}\label{fig:8587_consistency}
\end{figure}

Because of this small sample size, \Cref{fig:8587_consistency} shows binary extremes. \emph{Deepseek-r1} is 100\% consistent in its evaluations across the 7 functions. \emph{Qwen-3}, however, shows an inconsistency rate of roughly 25\% (one function).
We encounter this inconsistency specifically in the \texttt{task-readstat\_sav\_parse\_date-O2} file.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8587/ex1.png}
    \caption{Inconsistency in file task-readstat\_sav\_parse\_date-O2}\label{fig:8587_ex1}
\end{figure}

In \Cref{fig:8587_ex1}, we can observe exactly why \emph{qwen-3} changes its verdict. During the raw code evaluation, it correctly praises the \ac{PR} for using a direct array lookup rather than the \texttt{BASE} branch's obscure pointer arithmetic with a hardcoded address (\texttt{0x102311}). It notes that this leverages named data structures, which is highly idiomatic.
However, when the code is stripped into an \ac{AST}, the array name is replaced by generic identifiers. In the anonimized version, \emph{qwen-3} suddenly perceives the \ac{PR}'s dynamic array indexing (e.g., \texttt{id + id + 1}) as ``unnecessary arithmetic complexity'' and reverts to favoring the \texttt{BASE} branch, arguing that a fixed offset (\texttt{0x102311 + id}) is simpler and more intentional. 
In this case the model's judgment hallucinates, since the \ac{PR} version is objectively more human-like in its use of array indexing, and the \texttt{BASE} version's pointer arithmetic is more convoluted.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8587/ex2.png}
    \caption{Example of consistency in evaluation by deepseek-r1}\label{fig:8587_ex2}
\end{figure}

Conversely, \Cref{fig:8587_ex2} shows that the majority of cases correctly identify the modifications of the \ac{PR}. \emph{Deepseek-r1} correctly rewards the \ac{PR} in both contexts. It identifies that the \ac{PR}'s output avoids raw pointer arithmetic in the raw code, and when evaluating the \ac{AST}, it continues to recognize the underlying structural pattern of ``direct array access'' as the superior, more human-like engineering choice, regardless of whether the specific variable names are visible or not.
\subsection{PR \#8161}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8161/preference.png}
    \caption{Distribution of winners across the four prompts for PR \#8161}\label{fig:8161_winner_distribution}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/8161/consistency.png}
    \caption{Consistency across the prompts for evaluating ``humanity'' for PR \#8161}\label{fig:8161_consistency}
\end{figure}


\subsection{PR \#7253}

\subsection{PR \#6722}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/6722/preference.png}
    \caption{Distribution of winners across the four prompts for PR \#6722}\label{fig:6722_winner_distribution}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/6722/consistency.png}
    \caption{Consistency across the prompts for evaluating ``humanity'' for PR \#6722}\label{fig:6722_consistency}
\end{figure}

\section{Dogbolt Evaluation}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/dogbolt_winners.png}
    \caption{Distribution of winners across the four prompts for the dogbolt evaluation}\label{fig:dogbolt_winner_distribution}
\end{figure}


Based on the data presented in \Cref{fig:dogbolt_winner_distribution} and detailed in \Cref{tab:dogbolt_winners_by_prompt}, several critical insights emerge regarding how the LLM judges perceive the outputs of the different decompilers.
There is a stark contrast between the ``Blind'' evaluations (where the LLM judges purely on abstract readability and perceived ``humanity'') and the ``+ Source'' evaluations (where the LLM judges fidelity to the original code) for \emph{binary-ninja}.
In the blind tests (\emph{Code Blind} and \emph{AST Blind}), \emph{binary-ninja} frequently emerges as winner, particularly according to \emph{deepseek-r1}, securing 16 and 21 wins respectively. The models evidently perceive its output as highly readable and natural. However, the moment the original source code is introduced as a baseline (\emph{Code + Source} and \emph{AST + Source}), its win rate collapses drastically. This heavily implies that while \emph{binary-ninja} applies aggressive transformations to make the code look clean and human-readable, these transformations structurally distance the output from the original human-written logic. 

Another key indicator of the massive differences between the versions is the exceptionally high number of ``Ties''. When the source code is provided, the tie rate spikes, peaking at 30 ties out of the dataset for \emph{deepseek-r1} in the \emph{Code + Source} test. 
This high rate of indecision suggests that none of the decompilers successfully reconstruct the original source in a recognizable way. When the LLM attempts to match the decompiler outputs to the human ground truth, it finds that all candidates have hallucinated distinct structural artifacts, applied different control-flow recovery heuristics, or inflated the code with boilerplate to such a degree that choosing a ``closest match'' becomes arbitrary. 
It could be position Bias, (since the ratio of wins after switch is 99/47 for Version B) but it could also be that the LLM judges are overwhelmed by the sheer unpredictability of two different decompiled code, leading to a default tie decision when no clear winner emerges.

\emph{Hex-Rays} manages to maintain the most stable performance across both blind and source-provided prompts, winning almost consistently across all prompts, suggesting a more balanced approach that preserves some of the original code's structure while still applying transformations for readability.

\emph{Ghidra}, on the other hand, consistently underperforms for \emph{deepseek-r1}, rarely winning and often being overshadowed by the other two decompilers, but it also perform slightly better when the raw code is provided. Furthermore the inconsistencies in the Ghidra results across the two models suggest that its output may be more sensitive to the specific evaluation criteria emphasized by each LLM, which further complicates the notion of a single ``best'' decompiler.

\begin{table}
\centering
\caption{Dogbolt evaluation winners by judge and prompt type.}\label{tab:dogbolt_winners_by_prompt}
\begin{tabular}{llrrrr}
\toprule
\textbf{Judge} & \textbf{Test Type} & \textbf{Tie} & \textbf{binary-ninja} & \textbf{ghidra} & \textbf{hex-rays} \\
\midrule
deepseek-r1 & AST + Source  & 25 & 11 &  6 & 14 \\
 & AST Blind     & 19 & 21 &  6 & 14 \\
 & Code + Source & 30 &  3 &  6 & 18 \\
 & Code Blind    & 18 & 16 &  9 & 17 \\
\midrule
qwen-3 & AST + Source  & 17 & 11 & 11 & 16 \\
 & AST Blind     & 14 & 18 & 11 & 17 \\
 & Code + Source & 23 &  5 & 13 & 15 \\
 & Code Blind    & 16 & 12 & 17 & 15 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Length Bias}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/dogbolt_length_bias.png}
    \caption{Distribution of winners across the four prompts for the dogbolt evaluation, showing length bias}\label{fig:dogbolt_length_bias}
\end{figure}

\Cref{fig:dogbolt_length_bias} further quantifies the drastic variations among the decompilers. The boxplots illustrate the delta length between the winning snippet and the losing alternatives. The presence of massive outliers—some exceeding a difference of $\pm 1000$ tokens—proves that the tools generate code of vastly different lengths for the exact same binary function. 

While the medians hover near zero, we can observe a slight negative bias in the blind tests (\emph{winner} and \emph{winner\_ast}), meaning the LLM judges generally exhibit a slight preference for more concise code when evaluating purely for readability. However, in the source-provided tests (\emph{winner\_s} and \emph{winner\_ast\_s}), this distribution flattens or shifts slightly positive. 
When forced to prioritize fidelity, the models occasionally reward verbosity if the longer decompiler output happens to capture explicit logic or variable declarations that were present in the original source but optimized away by the more concise decompilers. 


\section{Correlation Perplexity \& LLM judge}

\subsection{Ghidra}

\subsection{Dogbolt}

\section{Vs Human Evaluation}

\section{Discussion}

